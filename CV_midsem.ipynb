{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:01:26.526907Z",
     "iopub.status.busy": "2025-10-17T08:01:26.526386Z",
     "iopub.status.idle": "2025-10-17T08:01:26.627287Z",
     "shell.execute_reply": "2025-10-17T08:01:26.626501Z",
     "shell.execute_reply.started": "2025-10-17T08:01:26.526878Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:01:34.227603Z",
     "iopub.status.busy": "2025-10-17T08:01:34.226911Z",
     "iopub.status.idle": "2025-10-17T08:01:34.232614Z",
     "shell.execute_reply": "2025-10-17T08:01:34.231865Z",
     "shell.execute_reply.started": "2025-10-17T08:01:34.227573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "VIDEO_DIR = os.path.join(\"dataset/data/MSRVTT/MSRVTT/videos\")  \n",
    "CAPTIONS_FILE = os.path.join(\"dataset/data/MSRVTT/MSRVTT/annotation/MSR_VTT.json\") \n",
    "\n",
    "# Hyperparams (tweak)\n",
    "SAMPLE_FRAMES = 16       \n",
    "FEATURE_DIM = 2048       \n",
    "ENC_HIDDEN = 512\n",
    "DEC_HIDDEN = 512\n",
    "EMBED_SIZE = 512\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T18:17:42.991220Z",
     "iopub.status.busy": "2025-10-07T18:17:42.990881Z",
     "iopub.status.idle": "2025-10-07T18:17:43.005455Z",
     "shell.execute_reply": "2025-10-07T18:17:43.003947Z",
     "shell.execute_reply.started": "2025-10-07T18:17:42.991200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (28, 2048)\n",
      "Data type: float32\n",
      "\n",
      "Sample values:\n",
      " [[8.3046875e+00 4.7924805e-01 3.9154053e-02 ... 6.2500000e+00\n",
      "  4.6718750e+00 6.2250000e+01]\n",
      " [8.3593750e+00 4.7827148e-01 5.8197021e-02 ... 5.8476562e+00\n",
      "  4.5078125e+00 6.2531250e+01]\n",
      " [8.4218750e+00 3.7670898e-01 8.7280273e-02 ... 6.1367188e+00\n",
      "  4.3828125e+00 6.2468750e+01]\n",
      " [8.1640625e+00 3.0639648e-01 3.7322998e-02 ... 5.7851562e+00\n",
      "  4.2656250e+00 6.0968750e+01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_path = \"/kaggle/input/msr-vtt-global-motion-350/global_features/video4.npy\"   \n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "print(\"Type:\", type(data))\n",
    "print(\"Shape:\", data.shape)\n",
    "print(\"Data type:\", data.dtype)\n",
    "\n",
    "print(\"\\nSample values:\\n\", data[:4])   # print first 2 items or frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:39:41.271131Z",
     "iopub.status.busy": "2025-10-07T09:39:41.270868Z",
     "iopub.status.idle": "2025-10-07T09:39:42.477798Z",
     "shell.execute_reply": "2025-10-07T09:39:42.477140Z",
     "shell.execute_reply.started": "2025-10-07T09:39:41.271110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             caption  id   image_id\n",
      "0  a cartoon animals runs through an ice cave in ...   0  video2960\n",
      "1  a cartoon character runs around inside of a vi...   1  video2960\n",
      "2                 a character is running in the snow   2  video2960\n",
      "3  a person plays a video game centered around ic...   3  video2960\n",
      "4       a person plays online and records themselves   4  video2960\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your annotation file\n",
    "anno_file = \"/kaggle/input/msr-vtt/MSR_VTT.json\"\n",
    "\n",
    "# Load JSON\n",
    "with open(anno_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert annotations into DataFrame\n",
    "df = pd.DataFrame(data[\"annotations\"])\n",
    "\n",
    "# Show first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:01:42.084280Z",
     "iopub.status.busy": "2025-10-17T08:01:42.084022Z",
     "iopub.status.idle": "2025-10-17T08:01:53.792534Z",
     "shell.execute_reply": "2025-10-17T08:01:53.791807Z",
     "shell.execute_reply.started": "2025-10-17T08:01:42.084263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 16834\n"
     ]
    }
   ],
   "source": [
    "#  vocabulary from captions\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "CAPTIONS_FILE = os.path.join(\"/kaggle/input/msr-vtt/captions.json\") \n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, freq_threshold=1, max_size=None):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        for i,w in enumerate([self.pad_token, self.bos_token, self.eos_token, self.unk_token]):\n",
    "            self.word2idx[w] = i\n",
    "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
    "\n",
    "    def build_vocab(self, captions_dict):\n",
    "        counter = Counter()\n",
    "        for vid, caps in captions_dict.items():\n",
    "            for c in caps:\n",
    "                tokens = [t.lower() for t in word_tokenize(c)]\n",
    "                counter.update(tokens)\n",
    "        # filter\n",
    "        words = [w for w,c in counter.items() if c >= self.freq_threshold]\n",
    "        words = sorted(words, key=lambda w: (-counter[w], w))\n",
    "        if self.max_size:\n",
    "            words = words[:self.max_size - len(self.word2idx)]\n",
    "        idx = len(self.word2idx)\n",
    "        for w in words:\n",
    "            self.word2idx[w] = idx\n",
    "            self.idx2word[idx] = w\n",
    "            idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = [t.lower() for t in word_tokenize(text)]\n",
    "        nums = [self.word2idx.get(t, self.word2idx[self.unk_token]) for t in tokens]\n",
    "        return [self.word2idx[self.bos_token]] + nums + [self.word2idx[self.eos_token]]\n",
    "\n",
    "# Load captions.json\n",
    "with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "vocab = Vocab(freq_threshold=2, max_size=20000)\n",
    "vocab.build_vocab(captions)\n",
    "print(\"Vocab size:\", len(vocab.word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:02:00.411660Z",
     "iopub.status.busy": "2025-10-17T08:02:00.411430Z",
     "iopub.status.idle": "2025-10-17T08:02:00.502991Z",
     "shell.execute_reply": "2025-10-17T08:02:00.502241Z",
     "shell.execute_reply.started": "2025-10-17T08:02:00.411644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 159994 Val size: 20000 Test size: 20000\n"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "FEATURES_DIR = \"/kaggle/input/msr-vtt/features/features\"\n",
    "class MSRVTTDataset(Dataset):\n",
    "    def __init__(self, captions_dict, features_dir, vocab, sample_frames=SAMPLE_FRAMES, max_caption_len=30):\n",
    "        self.items = []  # (video_filename, caption_text)\n",
    "        for vid, caps in captions_dict.items():\n",
    "            for c in caps:\n",
    "                self.items.append((vid, c))\n",
    "        self.features_dir = features_dir\n",
    "        self.vocab = vocab\n",
    "        self.sample_frames = sample_frames\n",
    "        self.max_caption_len = max_caption_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, cap = self.items[idx]\n",
    "        feat_path = os.path.join(self.features_dir, vid.replace('.mp4', '.npy'))\n",
    "        feats = np.load(feat_path)  # (T, D)\n",
    "        # pad/truncate frames (should already be SAMPLE_FRAMES)\n",
    "        if feats.shape[0] < self.sample_frames:\n",
    "            pad = np.zeros((self.sample_frames - feats.shape[0], feats.shape[1]), dtype=np.float32)\n",
    "            feats = np.concatenate([feats, pad], axis=0)\n",
    "        else:\n",
    "            feats = feats[:self.sample_frames]\n",
    "        numer = self.vocab.numericalize(cap)\n",
    "        if len(numer) > self.max_caption_len:\n",
    "            numer = numer[:self.max_caption_len-1] + [self.vocab.word2idx[self.vocab.eos_token]]\n",
    "        cap_len = len(numer)\n",
    "        # pad caption\n",
    "        pad_len = self.max_caption_len - cap_len\n",
    "        if pad_len > 0:\n",
    "            numer = numer + [self.vocab.word2idx[self.vocab.pad_token]] * pad_len\n",
    "        return torch.FloatTensor(feats), torch.LongTensor(numer), cap_len\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats = torch.stack([b[0] for b in batch], dim=0)  # (B, T, D)\n",
    "    caps = torch.stack([b[1] for b in batch], dim=0)\n",
    "    cap_lens = torch.LongTensor([b[2] for b in batch])\n",
    "    return feats, caps, cap_lens\n",
    "\n",
    "# split dataset (simple random split)\n",
    "items = list(captions.items())\n",
    "random.seed(42)\n",
    "random.shuffle(items)\n",
    "n = len(items)\n",
    "train_items = dict(items[:int(0.8*n)])\n",
    "val_items = dict(items[int(0.8*n):int(0.9*n)])\n",
    "test_items = dict(items[int(0.9*n):])\n",
    "\n",
    "train_ds = MSRVTTDataset(train_items, FEATURES_DIR, vocab)\n",
    "val_ds   = MSRVTTDataset(val_items, FEATURES_DIR, vocab)\n",
    "test_ds  = MSRVTTDataset(test_items, FEATURES_DIR, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds), \"Test size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T05:03:30.602523Z",
     "iopub.status.busy": "2025-10-17T05:03:30.602236Z",
     "iopub.status.idle": "2025-10-17T05:03:30.616219Z",
     "shell.execute_reply": "2025-10-17T05:03:30.615373Z",
     "shell.execute_reply.started": "2025-10-17T05:03:30.602498Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Model-1)BiLSTM Encoder+LSTM Decoder\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, num_layers=1, bidirectional=True,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size=feat_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.output_size = hidden_size * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats: (B, T, D)\n",
    "        outputs, (h_n, c_n) = self.rnn(feats)  # outputs: (B, T, hidden*dir)\n",
    "        return outputs, (h_n, c_n)\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_dim, dec_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "     \n",
    "        proj = self.attn(encoder_outputs)  # (B, T, dec_dim)\n",
    "        # up: (B, T)\n",
    "        scores = torch.bmm(proj, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (B, T)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (B, enc_dim)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, embed_size, enc_dim, dec_hidden, vocab_size, num_layers=1,num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = LuongAttention(enc_dim, dec_hidden)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(embed_size + enc_dim, dec_hidden)\n",
    "        self.fc_out = nn.Linear(dec_hidden, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward_step(self, prev_word, last_hidden, last_cell, encoder_outputs):\n",
    "        # prev_word: (B,) token ids\n",
    "        emb = self.embedding(prev_word)  # (B, E)\n",
    "       \n",
    "        context, attn_weights = self.attention(last_hidden, encoder_outputs)  # (B, enc_dim), (B, T)\n",
    "\n",
    "        lstm_input = torch.cat([emb, context], dim=1)\n",
    "        h, c = self.lstm(lstm_input, (last_hidden, last_cell))\n",
    "        output = self.fc_out(self.dropout(h))\n",
    "        return output, h, c, attn_weights\n",
    "\n",
    "    def forward(self, encoder_outputs, captions, teacher_forcing_ratio=0.9):\n",
    "        # encoder_outputs: (B, T, enc_dim)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_len = captions.size(1)\n",
    "        vocab_size = self.fc_out.out_features\n",
    "\n",
    "        \n",
    "        hidden = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)\n",
    "        cell = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size, device=encoder_outputs.device)\n",
    "        attn_weights_all = []\n",
    "\n",
    "       \n",
    "        input_word = captions[:,0]  # (B,)\n",
    "        for t in range(1, max_len):\n",
    "            out, hidden, cell, attn_weights = self.forward_step(input_word, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = out\n",
    "            attn_weights_all.append(attn_weights)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            input_word = captions[:, t] if teacher_force else top1\n",
    "        # outputs: (B, max_len, vocab)\n",
    "        return outputs, attn_weights_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T05:03:36.537774Z",
     "iopub.status.busy": "2025-10-17T05:03:36.537497Z",
     "iopub.status.idle": "2025-10-17T05:03:36.546670Z",
     "shell.execute_reply": "2025-10-17T05:03:36.546035Z",
     "shell.execute_reply.started": "2025-10-17T05:03:36.537755Z"
    }
   },
   "outputs": [],
   "source": [
    "#Decoder model using BiLSTM\n",
    "\n",
    "class DecoderWithBiLSTM(nn.Module):\n",
    "    def __init__(self, embed_size, enc_dim, dec_hidden, vocab_size, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = LuongAttention(enc_dim, dec_hidden * 2)  # since decoder is bidirectional\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size=embed_size + enc_dim,\n",
    "                              hidden_size=dec_hidden,\n",
    "                              num_layers=num_layers,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(dec_hidden * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_outputs, captions, teacher_forcing_ratio=0.9):\n",
    "        # encoder_outputs: (B, T, enc_dim)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_len = captions.size(1)\n",
    "        vocab_size = self.fc_out.out_features\n",
    "\n",
    "        # Initialize hidden and cell states for both directions\n",
    "        h0 = torch.zeros(2, batch_size, self.bilstm.hidden_size, device=encoder_outputs.device)\n",
    "        c0 = torch.zeros(2, batch_size, self.bilstm.hidden_size, device=encoder_outputs.device)\n",
    "\n",
    "        # Embedding all captions\n",
    "        embeddings = self.embedding(captions)  # (B, L, E)\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size, device=encoder_outputs.device)\n",
    "        attn_weights_all = []\n",
    "\n",
    "        # Loop through each timestep (teacher forcing)\n",
    "        input_word = captions[:, 0]  # <BOS> token\n",
    "        hidden, cell = h0, c0\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            emb = self.embedding(input_word).unsqueeze(1)  # (B,1,E)\n",
    "            \n",
    "           \n",
    "            # Combine both directions' last layer hidden states\n",
    "            last_hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)  # (B, dec_hidden*2)\n",
    "            context, attn_weights = self.attention(last_hidden_cat, encoder_outputs)\n",
    "            attn_weights_all.append(attn_weights)\n",
    "\n",
    "            # Concatenate embedding and context\n",
    "            lstm_input = torch.cat([emb, context.unsqueeze(1)], dim=2)  # (B,1,E+enc_dim)\n",
    "\n",
    "            # Run one BiLSTM step (we process one token at a time)\n",
    "            output, (hidden, cell) = self.bilstm(lstm_input, (hidden, cell))  # output: (B,1,2*hidden)\n",
    "\n",
    "            # Generate next word prediction\n",
    "            out_vocab = self.fc_out(self.dropout(output.squeeze(1)))  # (B, vocab)\n",
    "            outputs[:, t, :] = out_vocab\n",
    "\n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out_vocab.argmax(1)\n",
    "            input_word = captions[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs, attn_weights_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilaization of Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T05:03:42.816737Z",
     "iopub.status.busy": "2025-10-17T05:03:42.816227Z",
     "iopub.status.idle": "2025-10-17T05:03:43.232728Z",
     "shell.execute_reply": "2025-10-17T05:03:43.231969Z",
     "shell.execute_reply.started": "2025-10-17T05:03:42.816716Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "enc = EncoderRNN(feat_size=FEATURE_DIM, hidden_size=ENC_HIDDEN, bidirectional=True).to(DEVICE)\n",
    "# dec = DecoderWithAttention(embed_size=EMBED_SIZE, enc_dim=enc.output_size, dec_hidden=DEC_HIDDEN, vocab_size=len(vocab.word2idx)).to(DEVICE)\n",
    "dec = DecoderWithBiLSTM(embed_size=EMBED_SIZE, enc_dim=enc.output_size, dec_hidden=DEC_HIDDEN, vocab_size=len(vocab.word2idx)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n",
    "\n",
    "def train_one_epoch(train_loader, enc, dec, optimizer, criterion, device, clip=5.0):\n",
    "    enc.train(); dec.train()\n",
    "    running_loss = 0.0\n",
    "    for feats, caps, cap_lens in tqdm(train_loader):\n",
    "        feats = feats.to(device)            # (B, T, D)\n",
    "        caps = caps.to(device)              # (B, L)\n",
    "        optimizer.zero_grad()\n",
    "        encoder_outs, _ = enc(feats)       # (B, T, enc_dim)\n",
    "        outputs, _ = dec(encoder_outs, caps, teacher_forcing_ratio=0.75)  # (B, L, V)\n",
    "        # shift outputs and targets: ignore the first token (<BOS>)\n",
    "        outputs = outputs[:,1:,:].contiguous()\n",
    "        targets = caps[:,1:].contiguous()\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate(loader, enc, dec, device):\n",
    "    enc.eval(); dec.eval()\n",
    "    all_refs = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, cap_lens in tqdm(loader):\n",
    "            feats = feats.to(device)\n",
    "            caps = caps.to(device)\n",
    "            encoder_outs, _ = enc(feats)\n",
    "            # greedy decode\n",
    "            batch_size = feats.size(0)\n",
    "            hidden = torch.zeros(batch_size, dec.lstm.hidden_size, device=device)\n",
    "            cell   = torch.zeros(batch_size, dec.lstm.hidden_size, device=device)\n",
    "            input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]] * batch_size).to(device)\n",
    "            preds = [[] for _ in range(batch_size)]\n",
    "            for t in range(1, caps.size(1)):\n",
    "                out, hidden, cell, attn = dec.forward_step(input_word, hidden, cell, encoder_outs)\n",
    "                top1 = out.argmax(1)  # (B,)\n",
    "                input_word = top1\n",
    "                for i in range(batch_size):\n",
    "                    preds[i].append(decoder_token := top1[i].item())\n",
    "            # convert preds and refs to token lists\n",
    "            for i in range(batch_size):\n",
    "                # predicted sentence until EOS or max len\n",
    "                p = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok == vocab.word2idx[vocab.pad_token] or tok == vocab.word2idx[vocab.bos_token]:\n",
    "                        continue\n",
    "                    p.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(p)\n",
    "              \n",
    "                ref_tokens = []\n",
    "                target_seq = caps[i].cpu().numpy()\n",
    "                for tok in target_seq:\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.bos_token], vocab.word2idx[vocab.pad_token]):\n",
    "                        continue\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "    # BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)\n",
    "    return bleu4, all_refs, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:34:45.613620Z",
     "iopub.status.busy": "2025-10-16T06:34:45.612939Z",
     "iopub.status.idle": "2025-10-16T06:34:45.630267Z",
     "shell.execute_reply": "2025-10-16T06:34:45.629661Z",
     "shell.execute_reply.started": "2025-10-16T06:34:45.613594Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def evaluate_with_metrics(loader, enc, dec, vocab, device, DEC_HIDDEN=512):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    all_refs, all_preds = [], []\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    cider_scorer = Cider()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, cap_lens in tqdm(loader, desc=\"Evaluating\"):\n",
    "            feats, caps = feats.to(device), caps.to(device)\n",
    "            B = feats.size(0)\n",
    "            \n",
    "            # Encoder forward\n",
    "            encoder_outs, _ = enc(feats)\n",
    "\n",
    "            # Initialize hidden and cell\n",
    "            hidden = torch.zeros(B, DEC_HIDDEN, device=device)\n",
    "            cell = torch.zeros(B, DEC_HIDDEN, device=device)\n",
    "            \n",
    "            # Start with <BOS> token\n",
    "            input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]] * B).to(device)\n",
    "\n",
    "            preds = [[] for _ in range(B)]\n",
    "\n",
    "            # Greedy decoding\n",
    "            max_len = caps.size(1)\n",
    "            for t in range(1, max_len):\n",
    "                out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                    input_word, hidden, cell, encoder_outs\n",
    "                )\n",
    "                top1 = out.argmax(1)\n",
    "                input_word = top1\n",
    "                for i in range(B):\n",
    "                    preds[i].append(top1[i].item())\n",
    "\n",
    "            # Convert predicted tokens â†’ words\n",
    "            for i in range(B):\n",
    "                pred_tokens = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    pred_tokens.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(pred_tokens)\n",
    "\n",
    "                # Reference captions\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "\n",
    "   \n",
    "    # Compute Metrics\n",
    " \n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "    refs_str = [' '.join(ref[0]) for ref in all_refs]\n",
    "    preds_str = [' '.join(pred) for pred in all_preds]\n",
    "\n",
    "    rouge_scores = rouge.get_scores(preds_str, refs_str, avg=True)\n",
    "    cider_score, _ = cider_scorer.compute_score(\n",
    "        {i: [refs_str[i]] for i in range(len(refs_str))},\n",
    "        {i: [preds_str[i]] for i in range(len(preds_str))}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBLEU-4: {bleu4:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "    print(f\"CIDEr: {cider_score:.4f}\")\n",
    "\n",
    "    return bleu4, rouge_scores, cider_score, all_refs, all_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing 1st Model BiLSTM Enc & LSTM Dec +Luong +label smoothing+dropout+all 3 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T08:48:44.086536Z",
     "iopub.status.busy": "2025-10-11T08:48:44.085733Z",
     "iopub.status.idle": "2025-10-11T10:46:27.443730Z",
     "shell.execute_reply": "2025-10-11T10:46:27.442834Z",
     "shell.execute_reply.started": "2025-10-11T08:48:44.086509Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:12<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.6179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0518\n",
      "ROUGE-L: 0.2815\n",
      "CIDEr: 0.4819\n",
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0518\n",
      "  ROUGE-L (F1) = 0.2815\n",
      "  CIDEr = 0.4819\n",
      "âœ… Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.5586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0508\n",
      "ROUGE-L: 0.2812\n",
      "CIDEr: 0.4880\n",
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0508\n",
      "  ROUGE-L (F1) = 0.2812\n",
      "  CIDEr = 0.4880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 4.5077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0514\n",
      "ROUGE-L: 0.2843\n",
      "CIDEr: 0.4896\n",
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0514\n",
      "  ROUGE-L (F1) = 0.2843\n",
      "  CIDEr = 0.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 4.4580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:27<00:00, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0522\n",
      "ROUGE-L: 0.2825\n",
      "CIDEr: 0.4988\n",
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0522\n",
      "  ROUGE-L (F1) = 0.2825\n",
      "  CIDEr = 0.4988\n",
      "âœ… Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:13<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 4.4123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0520\n",
      "ROUGE-L: 0.2801\n",
      "CIDEr: 0.4963\n",
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0520\n",
      "  ROUGE-L (F1) = 0.2801\n",
      "  CIDEr = 0.4963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:12<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 4.3873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0527\n",
      "ROUGE-L: 0.2824\n",
      "CIDEr: 0.5100\n",
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0527\n",
      "  ROUGE-L (F1) = 0.2824\n",
      "  CIDEr = 0.5100\n",
      "âœ… Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:13<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 4.3469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0520\n",
      "ROUGE-L: 0.2805\n",
      "CIDEr: 0.4878\n",
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0520\n",
      "  ROUGE-L (F1) = 0.2805\n",
      "  CIDEr = 0.4878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:13<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 4.3162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0524\n",
      "ROUGE-L: 0.2831\n",
      "CIDEr: 0.4952\n",
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0524\n",
      "  ROUGE-L (F1) = 0.2831\n",
      "  CIDEr = 0.4952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:13<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 4.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:27<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0523\n",
      "ROUGE-L: 0.2834\n",
      "CIDEr: 0.5080\n",
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0523\n",
      "  ROUGE-L (F1) = 0.2834\n",
      "  CIDEr = 0.5080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [11:13<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 4.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0535\n",
      "ROUGE-L: 0.2810\n",
      "CIDEr: 0.4992\n",
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0535\n",
      "  ROUGE-L (F1) = 0.2810\n",
      "  CIDEr = 0.4992\n",
      "âœ… Saved new best checkpoint.\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0535\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    #Training \n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation \n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE, DEC_HIDDEN)\n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']  \n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    " \n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint.pth\")\n",
    "        print(\"âœ… Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T11:25:17.256042Z",
     "iopub.status.busy": "2025-10-11T11:25:17.255371Z",
     "iopub.status.idle": "2025-10-11T11:25:50.718965Z",
     "shell.execute_reply": "2025-10-11T11:25:50.717803Z",
     "shell.execute_reply.started": "2025-10-11T11:25:17.256018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0523\n",
      "ROUGE-L: 0.2775\n",
      "CIDEr: 0.5009\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "BLEU-4  = 0.0523\n",
      "ROUGE-L = 0.2775\n",
      "CIDEr   = 0.5009\n",
      "\n",
      " --- Random Sample Predictions ---\n",
      "\n",
      " Video 1: video6643.mp4\n",
      "Generated Caption: a man is talking to a man\n",
      "Reference Captions:\n",
      "  Ref 1: a clip showing a man giving instructions to others\n",
      "  Ref 2: a guy is giving a speech in front of an audience\n",
      "  Ref 3: a man giving a motavational speech\n",
      "\n",
      " Video 2: video1850.mp4\n",
      "Generated Caption: a man in a suit is talking to a man in a suit\n",
      "Reference Captions:\n",
      "  Ref 1: a gray haired news anchor with a purple tie discusses an election\n",
      "  Ref 2: there is a suit man is talking about election\n",
      "  Ref 3: there is a suit man is talking about politics\n",
      "\n",
      " Video 3: video4829.mp4\n",
      "Generated Caption: a woman is talking to a woman\n",
      "Reference Captions:\n",
      "  Ref 1: a couple is singing different parts of the same song\n",
      "  Ref 2: a couple is watching a movie\n",
      "  Ref 3: a couple singing a song\n",
      "\n",
      " Video 4: video8925.mp4\n",
      "Generated Caption: a band is performing a song\n",
      "Reference Captions:\n",
      "  Ref 1: a band is performing a song\n",
      "  Ref 2: a band is performing on stage\n",
      "  Ref 3: a band is playing a song on stage\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the best saved checkpoint\n",
    "\n",
    "ckpt = torch.load(\"best_checkpoint.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE,DEC_HIDDEN)\n",
    "# Extract the ROUGE-L F1 score\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    feat_tensor = torch.tensor(video_feat).unsqueeze(0).float().to(device)  # (1, T, D)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        encoder_outs, _ = enc(feat_tensor)\n",
    "\n",
    "        hidden = torch.zeros(1, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word, hidden, cell, encoder_outs\n",
    "            )\n",
    "            next_word = out.argmax(1).item()\n",
    "\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "\n",
    "# few random test samples\n",
    "\n",
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "\n",
    "print(\"\\n --- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid]  \n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):  \n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-06T02:29:53.780Z",
     "iopub.execute_input": "2025-10-05T18:48:54.915305Z",
     "iopub.status.busy": "2025-10-05T18:48:54.914748Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:54<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 5.5310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 24.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0371\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:53<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 5.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0438\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:55<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 4.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:25<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0483\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:55<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 4.7271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0499\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:55<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 4.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:25<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:55<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 4.5579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:56<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 4.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:25<00:00, 24.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0522\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:54<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 4.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:55<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 4.4010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:54<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss: 4.3578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:53<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train loss: 4.3314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:26<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU-4: 0.0520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 865/5000 [01:43<08:20,  8.27it/s]"
     ]
    }
   ],
   "source": [
    "#BiLSTM Encoder(+dropout) decoder+ label smoothening\n",
    "#  Training loop\n",
    "EPOCHS=15\n",
    "best_bleu = 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} train loss: {train_loss:.4f}\")\n",
    "    bleu, _, _ = evaluate(val_loader, enc, dec, DEVICE)\n",
    "    print(f\"Validation BLEU-4: {bleu:.4f}\")\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint.pth\")\n",
    "        print(\"Saved best checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T06:44:27.322472Z",
     "iopub.status.busy": "2025-10-04T06:44:27.321737Z",
     "iopub.status.idle": "2025-10-04T06:44:53.764198Z",
     "shell.execute_reply": "2025-10-04T06:44:53.763331Z",
     "shell.execute_reply.started": "2025-10-04T06:44:27.322446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint. Running test evaluation (BLEU-4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:25<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-4: 0.05059154183767522\n",
      "Generated: a man is talking about a\n",
      "Reference(s): ['a late night talk show host speaking', 'a man is asking people to make predictions', 'a man is giving speech']\n"
     ]
    }
   ],
   "source": [
    "#  Greedy inference for a single video, and test BLEU\n",
    "def generate_caption_for_video(feat_np, enc, dec, vocab, device, max_len=30):\n",
    "    enc.eval(); dec.eval()\n",
    "    with torch.no_grad():\n",
    "        feats = torch.FloatTensor(feat_np).unsqueeze(0).to(device)  # (1, T, D)\n",
    "        encoder_outs, _ = enc(feats)\n",
    "        hidden = torch.zeros(1, dec.lstm.hidden_size, device=device)\n",
    "        cell   = torch.zeros(1, dec.lstm.hidden_size, device=device)\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        pred_tokens = []\n",
    "        for t in range(max_len):\n",
    "            out, hidden, cell, attn = dec.forward_step(input_word, hidden, cell, encoder_outs)\n",
    "            top1 = out.argmax(1).item()\n",
    "            if top1 == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "            if top1 not in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                pred_tokens.append(vocab.idx2word.get(top1, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([top1]).to(device)\n",
    "    return \" \".join(pred_tokens)\n",
    "\n",
    "# Load checkpoint and test\n",
    "ckpt = torch.load(\"best_checkpoint.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\"Loaded checkpoint. Running test evaluation (BLEU-4)...\")\n",
    "test_bleu, refs, preds = evaluate(test_loader, enc, dec, DEVICE)\n",
    "print(\"Test BLEU-4:\", test_bleu)\n",
    "\n",
    "\n",
    "sample_feat_path = os.path.join(FEATURES_DIR, list(test_items.keys())[0].replace('.mp4', '.npy'))\n",
    "sample_feat = np.load(sample_feat_path)\n",
    "print(\"Generated:\", generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE))\n",
    "print(\"Reference(s):\", test_items[list(test_items.keys())[0]][:3])  # first few refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T06:51:46.643315Z",
     "iopub.status.busy": "2025-10-04T06:51:46.642642Z",
     "iopub.status.idle": "2025-10-04T06:52:13.582386Z",
     "shell.execute_reply": "2025-10-04T06:52:13.581596Z",
     "shell.execute_reply.started": "2025-10-04T06:51:46.643293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint. Running test evaluation (BLEU-4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:25<00:00, 24.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-4: 0.05059154183767522\n",
      "\n",
      "--- Sample Predictions ---\n",
      "\n",
      "Video 1: video2972.mp4\n",
      "Generated: a man is talking about a\n",
      "Reference(s): ['a late night talk show host speaking', 'a man is asking people to make predictions', 'a man is giving speech']\n",
      "\n",
      "Video 2: video7381.mp4\n",
      "Generated: a girl is singing on stage\n",
      "Reference(s): ['a boy singing in the stage', 'a couple preforms a song on stage', 'a group is singing a song']\n",
      "\n",
      "Video 3: video6018.mp4\n",
      "Generated: a cartoon is playing with a\n",
      "Reference(s): ['a group of people near a pool', 'a man is clapping', 'a puppet is climbing a diving board ladder']\n",
      "\n",
      "Video 4: video7663.mp4\n",
      "Generated: a man and woman are dancing\n",
      "Reference(s): ['a woman sings a country song about her shoes and love', 'performers are dancing on the stage while blue spotlight scans the audience', 'people dance around on a stage in front of an audience']\n"
     ]
    }
   ],
   "source": [
    "# lstm(+dropout)\n",
    "ckpt = torch.load(\"best_checkpoint.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\"Loaded checkpoint. Running test evaluation (BLEU-4)...\")\n",
    "test_bleu, refs, preds = evaluate(test_loader, enc, dec, DEVICE)\n",
    "print(\"Test BLEU-4:\", test_bleu)\n",
    "\n",
    "# Show results for 3â€“4 videos\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "for i, vid in enumerate(list(test_items.keys())[:4]):  # first 4 test videos\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    # Generate caption\n",
    "    generated = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\nVideo {i+1}: {vid}\")\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Reference(s):\", test_items[vid][:3])  # show first 3 references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T06:56:22.465040Z",
     "iopub.status.busy": "2025-10-04T06:56:22.464756Z",
     "iopub.status.idle": "2025-10-04T06:56:48.676645Z",
     "shell.execute_reply": "2025-10-04T06:56:48.675692Z",
     "shell.execute_reply.started": "2025-10-04T06:56:22.465018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint. Running test evaluation (BLEU-4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:25<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-4: 0.05059154183767522\n",
      "\n",
      "--- Random Sample Predictions ---\n",
      "\n",
      "Video 1: video5438.mp4\n",
      "Generated Caption: a cartoon is showing a cartoon\n",
      "Original Captions:\n",
      "  Ref 1: there is a boy with cap is dancing on the floor\n",
      "  Ref 2: long time no see the trailer of the smurfs-2 in 3d\n",
      "  Ref 3: the cartoon character smurfs is blue in colour and wearing white pant and shoes\n",
      "\n",
      "Video 2: video5879.mp4\n",
      "Generated Caption: a person is using a toy\n",
      "Original Captions:\n",
      "  Ref 1: a girl is being cut out of paper\n",
      "  Ref 2: a man cuts material with scissors\n",
      "  Ref 3: a man showing his craft skills\n",
      "\n",
      "Video 3: video7557.mp4\n",
      "Generated Caption: a girl is playing guitar and singing\n",
      "Original Captions:\n",
      "  Ref 1: a girl singing a song with an acoustic guitar outside\n",
      "  Ref 2: this asian lady with a guitar is singing a song on the side of the street\n",
      "  Ref 3: a blue shirt lady is playing guitar on the road\n",
      "\n",
      "Video 4: video520.mp4\n",
      "Generated Caption: a cartoon character is talking to a cartoon\n",
      "Original Captions:\n",
      "  Ref 1: a cartoon of pigs surfing and pigs at the beach\n",
      "  Ref 2: a cartoon pig is standing on surfboard on top of pretend waves\n",
      "  Ref 3: a pig is riding a wave\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Load checkpoint and test\n",
    "ckpt = torch.load(\"best_checkpoint.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\"Loaded checkpoint. Running test evaluation (BLEU-4)...\")\n",
    "test_bleu, refs, preds = evaluate(test_loader, enc, dec, DEVICE)\n",
    "print(\"Test BLEU-4:\", test_bleu)\n",
    "\n",
    "# Pick 4 random test videos\n",
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "\n",
    "print(\"\\n--- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    # Generate caption\n",
    "    generated = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "\n",
    "    # References (original captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\nVideo {i+1}: {vid}\")\n",
    "    print(\"Generated Caption:\", generated)\n",
    "    print(\"Original Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):  # show first 3 refs\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:02:19.248500Z",
     "iopub.status.busy": "2025-10-17T08:02:19.248215Z",
     "iopub.status.idle": "2025-10-17T08:02:19.262188Z",
     "shell.execute_reply": "2025-10-17T08:02:19.261388Z",
     "shell.execute_reply.started": "2025-10-17T08:02:19.248480Z"
    }
   },
   "outputs": [],
   "source": [
    "#BiLSTM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def evaluate_with_metrics(loader, enc, dec, vocab, device):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    all_refs, all_preds = [], []\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    cider_scorer = Cider()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, cap_lens in tqdm(loader, desc=\"Evaluating\"):\n",
    "            feats, caps = feats.to(device), caps.to(device)\n",
    "            B = feats.size(0)\n",
    "\n",
    "            # Encoder Forward\n",
    "            encoder_outs, _ = enc(feats)  # (B, T, enc_dim)\n",
    "\n",
    "            # Initialize hidden states for BiLSTM\n",
    "            num_directions = 2\n",
    "            hidden_size = dec.bilstm.hidden_size\n",
    "            num_layers = dec.bilstm.num_layers\n",
    "\n",
    "            hidden = torch.zeros(num_layers * num_directions, B, hidden_size, device=device)\n",
    "            cell = torch.zeros(num_layers * num_directions, B, hidden_size, device=device)\n",
    "\n",
    "    \n",
    "            #  Start decoding\n",
    "            input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]] * B).to(device)\n",
    "            preds = [[] for _ in range(B)]\n",
    "\n",
    "            max_len = caps.size(1)\n",
    "\n",
    "            for t in range(1, max_len):\n",
    "                emb = dec.embedding(input_word).unsqueeze(1)  # (B,1,E)\n",
    "\n",
    "                # Combine forward and backward hidden states from the last layer\n",
    "                last_hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)  # (B, dec_hidden*2)\n",
    "\n",
    "                # Attention over encoder outputs\n",
    "                context, attn_weights = dec.attention(last_hidden_cat, encoder_outs)\n",
    "\n",
    "              \n",
    "                lstm_input = torch.cat([emb, context.unsqueeze(1)], dim=2)\n",
    "\n",
    "                # One decoding step\n",
    "                output, (hidden, cell) = dec.bilstm(lstm_input, (hidden, cell))\n",
    "                out_vocab = dec.fc_out(dec.dropout(output.squeeze(1)))  # (B, vocab_size)\n",
    "\n",
    "                # Greedy decoding\n",
    "                top1 = out_vocab.argmax(1)\n",
    "                input_word = top1\n",
    "\n",
    "                for i in range(B):\n",
    "                    preds[i].append(top1[i].item())\n",
    "\n",
    "       \n",
    "            #  Convert Predictions & References to Tokens\n",
    "            for i in range(B):\n",
    "                pred_tokens = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    pred_tokens.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(pred_tokens)\n",
    "\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "\n",
    "  \n",
    "    #  Compute Evaluation Metrics\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "    refs_str = [' '.join(ref[0]) for ref in all_refs]\n",
    "    preds_str = [' '.join(pred) for pred in all_preds]\n",
    "\n",
    "    rouge_scores = rouge.get_scores(preds_str, refs_str, avg=True)\n",
    "    cider_score, _ = cider_scorer.compute_score(\n",
    "        {i: [refs_str[i]] for i in range(len(refs_str))},\n",
    "        {i: [preds_str[i]] for i in range(len(preds_str))}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBLEU-4: {bleu4:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "    print(f\"CIDEr: {cider_score:.4f}\")\n",
    "\n",
    "    return bleu4, rouge_scores, cider_score, all_refs, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:02:30.271598Z",
     "iopub.status.busy": "2025-10-17T08:02:30.271074Z",
     "iopub.status.idle": "2025-10-17T08:02:30.277424Z",
     "shell.execute_reply": "2025-10-17T08:02:30.276631Z",
     "shell.execute_reply.started": "2025-10-17T08:02:30.271571Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, enc, dec, optimizer, criterion, device, clip=5.0):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for feats, caps, cap_lens in tqdm(train_loader, desc=\"Training\"):\n",
    "        feats = feats.to(device)          # (B, T, D)\n",
    "        caps = caps.to(device)            # (B, L)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "        # Encoder forward\n",
    "        encoder_outs, _ = enc(feats)      # (B, T, enc_dim)\n",
    "\n",
    "\n",
    "        # Decoder forward (BiLSTM)\n",
    "        outputs, _ = dec(encoder_outs, caps, teacher_forcing_ratio=0.75)  # (B, L, V)\n",
    "\n",
    "\n",
    "        # Ignore the <BOS> token\n",
    "        outputs = outputs[:, 1:, :].contiguous()\n",
    "        targets = caps[:, 1:].contiguous()\n",
    "\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(list(enc.parameters()) + list(dec.parameters()), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T05:04:11.021338Z",
     "iopub.status.busy": "2025-10-17T05:04:11.021080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:53<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.4230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:37<00:00, 16.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0379\n",
      "ROUGE-L: 0.2682\n",
      "CIDEr: 0.3723\n",
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0379\n",
      "  ROUGE-L (F1) = 0.2682\n",
      "  CIDEr = 0.3723\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:54<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.9368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:37<00:00, 16.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0461\n",
      "ROUGE-L: 0.2745\n",
      "CIDEr: 0.4435\n",
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0461\n",
      "  ROUGE-L (F1) = 0.2745\n",
      "  CIDEr = 0.4435\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:53<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 4.7452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:36<00:00, 17.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0497\n",
      "ROUGE-L: 0.2758\n",
      "CIDEr: 0.4599\n",
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0497\n",
      "  ROUGE-L (F1) = 0.2758\n",
      "  CIDEr = 0.4599\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:53<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 4.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:37<00:00, 16.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0501\n",
      "ROUGE-L: 0.2795\n",
      "CIDEr: 0.4809\n",
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0501\n",
      "  ROUGE-L (F1) = 0.2795\n",
      "  CIDEr = 0.4809\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:53<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 4.5161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:37<00:00, 16.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0479\n",
      "ROUGE-L: 0.2798\n",
      "CIDEr: 0.4802\n",
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0479\n",
      "  ROUGE-L (F1) = 0.2798\n",
      "  CIDEr = 0.4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:53<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 4.4323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:37<00:00, 16.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0527\n",
      "ROUGE-L: 0.2819\n",
      "CIDEr: 0.4931\n",
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0527\n",
      "  ROUGE-L (F1) = 0.2819\n",
      "  CIDEr = 0.4931\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:53<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 4.3661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:36<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0521\n",
      "ROUGE-L: 0.2823\n",
      "CIDEr: 0.5015\n",
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0521\n",
      "  ROUGE-L (F1) = 0.2823\n",
      "  CIDEr = 0.5015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [16:52<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 4.3097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:36<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0499\n",
      "ROUGE-L: 0.2795\n",
      "CIDEr: 0.4848\n",
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0499\n",
      "  ROUGE-L (F1) = 0.2795\n",
      "  CIDEr = 0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4764/5000 [16:04<00:47,  4.94it/s]"
     ]
    }
   ],
   "source": [
    "#Decder-BiLSTM \n",
    "#Training Loop\n",
    "\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Training \n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE)\n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "  \n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:22:02.641027Z",
     "iopub.status.busy": "2025-10-17T08:22:02.640728Z",
     "iopub.status.idle": "2025-10-17T08:22:02.648866Z",
     "shell.execute_reply": "2025-10-17T08:22:02.648136Z",
     "shell.execute_reply.started": "2025-10-17T08:22:02.641005Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=30):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        feat_tensor = torch.tensor(video_feat, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        encoder_outs, _ = enc(feat_tensor)  # (1, T, enc_dim)\n",
    "\n",
    "        batch_size = 1\n",
    "        dec_hidden = dec.bilstm.hidden_size\n",
    "        num_directions = 2  # BiLSTM\n",
    "        enc_dim = encoder_outs.size(2)\n",
    "\n",
    "        # Initialize hidden and cell states for BiLSTM\n",
    "        hidden = torch.zeros(num_directions, batch_size, dec_hidden, device=device)\n",
    "        cell = torch.zeros(num_directions, batch_size, dec_hidden, device=device)\n",
    "\n",
    "\n",
    "        input_word = torch.tensor([vocab.word2idx[vocab.bos_token]], device=device)\n",
    "        generated_words = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "          \n",
    "            emb = dec.embedding(input_word).unsqueeze(1)  # (1,1,E)\n",
    "\n",
    "            # Combine last forward & backward hidden states\n",
    "            last_hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)  # (1, 2*dec_hidden)\n",
    "\n",
    "            # Attention over encoder outputs\n",
    "            context, _ = dec.attention(last_hidden_cat, encoder_outs)  # (1, enc_dim)\n",
    "\n",
    "            # Prepare LSTM input (concat embedding + context)\n",
    "            lstm_input = torch.cat([emb, context.unsqueeze(1)], dim=2)  # (1,1,E+enc_dim)\n",
    "\n",
    "            # Run one BiLSTM step\n",
    "            output, (hidden, cell) = dec.bilstm(lstm_input, (hidden, cell))  # output: (1,1,2*dec_hidden)\n",
    "\n",
    "           \n",
    "            out_vocab = dec.fc_out(dec.dropout(output.squeeze(1)))  # (1, vocab_size)\n",
    "\n",
    "         \n",
    "            top1 = out_vocab.argmax(1)\n",
    "            word = vocab.idx2word.get(top1.item(), vocab.unk_token)\n",
    "\n",
    "            # Stop if <EOS> token\n",
    "            if word == vocab.eos_token:\n",
    "                break\n",
    "\n",
    "            generated_words.append(word)\n",
    "            input_word = top1\n",
    "\n",
    "        return ' '.join(generated_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:22:10.641776Z",
     "iopub.status.busy": "2025-10-17T08:22:10.641507Z",
     "iopub.status.idle": "2025-10-17T08:22:53.643892Z",
     "shell.execute_reply": "2025-10-17T08:22:53.643064Z",
     "shell.execute_reply.started": "2025-10-17T08:22:10.641757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:36<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-4: 0.0500\n",
      "ROUGE-L: 0.2772\n",
      "CIDEr: 0.4896\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "BLEU-4  = 0.0500\n",
      "ROUGE-L = 0.2772\n",
      "CIDEr   = 0.4896\n",
      "\n",
      " --- Random Sample Predictions ---\n",
      "\n",
      " Video 1: video5619.mp4\n",
      "Generated Caption: a beautiful view of a mountain and a mountain\n",
      "Reference Captions:\n",
      "  Ref 1: a clip of a forested area\n",
      "  Ref 2: a group of people wait at a toll\n",
      "  Ref 3: a man drives a car\n",
      "\n",
      " Video 2: video3818.mp4\n",
      "Generated Caption: a man is playing a game of ping pong\n",
      "Reference Captions:\n",
      "  Ref 1: a child preforms a gymnastics preformance\n",
      "  Ref 2: a female gymnast is jumping\n",
      "  Ref 3: a girl is doing gymnastics on the mat\n",
      "\n",
      " Video 3: video350.mp4\n",
      "Generated Caption: a man is playing minecraft\n",
      "Reference Captions:\n",
      "  Ref 1: a game is being played\n",
      "  Ref 2: a gamer is playing minecraft\n",
      "  Ref 3: a man and a woman playing a video  game\n",
      "\n",
      " Video 4: video2520.mp4\n",
      "Generated Caption: a man is a a boat\n",
      "Reference Captions:\n",
      "  Ref 1: a cartoon about a boy\n",
      "  Ref 2: a cartoon character swims in rough waters\n",
      "  Ref 3: a cartoon of a boy in the ocean\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Load the best saved checkpoint\n",
    "ckpt = torch.load(\"/kaggle/input/bilstm-bestpath/best_checkpoint (5).pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "\n",
    "#  Evaluate on test set\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "#  few random test samples\n",
    "\n",
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "\n",
    "print(\"\\n --- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid] \n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]): \n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T08:41:30.289709Z",
     "iopub.status.busy": "2025-10-17T08:41:30.289056Z",
     "iopub.status.idle": "2025-10-17T08:41:30.379305Z",
     "shell.execute_reply": "2025-10-17T08:41:30.378639Z",
     "shell.execute_reply.started": "2025-10-17T08:41:30.289684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Random Sample Predictions ---\n",
      "\n",
      " Video 1: video6643.mp4\n",
      "Generated Caption: a man is talking about something\n",
      "Reference Captions:\n",
      "  Ref 1: a clip showing a man giving instructions to others\n",
      "  Ref 2: a guy is giving a speech in front of an audience\n",
      "  Ref 3: a man giving a motavational speech\n",
      "\n",
      " Video 2: video1850.mp4\n",
      "Generated Caption: a man in a suit is talking about the news\n",
      "Reference Captions:\n",
      "  Ref 1: a gray haired news anchor with a purple tie discusses an election\n",
      "  Ref 2: there is a suit man is talking about election\n",
      "  Ref 3: there is a suit man is talking about politics\n",
      "\n",
      " Video 3: video4829.mp4\n",
      "Generated Caption: a woman is talking to a man\n",
      "Reference Captions:\n",
      "  Ref 1: a couple is singing different parts of the same song\n",
      "  Ref 2: a couple is watching a movie\n",
      "  Ref 3: a couple singing a song\n",
      "\n",
      " Video 4: video8925.mp4\n",
      "Generated Caption: a man is playing a guitar\n",
      "Reference Captions:\n",
      "  Ref 1: a band is performing a song\n",
      "  Ref 2: a band is performing on stage\n",
      "  Ref 3: a band is playing a song on stage\n",
      "\n",
      " Video 1: video7419.mp4\n",
      "Generated Caption: a man is playing guitar\n",
      "Reference Captions:\n",
      "  Ref 1: a man in a red ball cap holds and plays a guitar while singing into a microphone\n",
      "  Ref 2: there is a man  singing a song  and playing his guitar\n",
      "  Ref 3: there is a man with red cap singing a song\n",
      "\n",
      " Video 2: video7981.mp4\n",
      "Generated Caption: a man is driving a car\n",
      "Reference Captions:\n",
      "  Ref 1: a car is moving down a narrow alleyway\n",
      "  Ref 2: a man drives a car in the city\n",
      "  Ref 3: a man drives a car through a city street\n",
      "\n",
      " Video 3: video8047.mp4\n",
      "Generated Caption: a man is talking about a news\n",
      "Reference Captions:\n",
      "  Ref 1: female reporter discussing the immigrant migration crisis in england specifically regarding almost 200000 people signing a petition to push this issue to parliament\n",
      "  Ref 2: more information on the bombings in brussels courtesy of skynews\n",
      "  Ref 3: a british news reporter discusses the country s viewpoint on allowing migrants into the country\n",
      "\n",
      " Video 4: video7612.mp4\n",
      "Generated Caption: a person is folding a piece of paper\n",
      "Reference Captions:\n",
      "  Ref 1: a man folding up and making a paper airplane\n",
      "  Ref 2: a man is folding a piece of paper\n",
      "  Ref 3: a man is making an aeroplane out of some yellow paper\n"
     ]
    }
   ],
   "source": [
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "selected_vids = [\n",
    "    \"video6643.mp4\",\n",
    "    \"video1850.mp4\",\n",
    "    \"video4829.mp4\",\n",
    "    \"video8925.mp4\"\n",
    "]\n",
    "\n",
    "print(\"\\n --- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(selected_vids):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid]  \n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]): \n",
    "        print(f\"  Ref {j+1}: {ref}\")\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid]  # list of ground-truth captions\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]): \n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM-Encoder Tranformer-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T12:56:13.490109Z",
     "iopub.status.busy": "2025-10-12T12:56:13.489406Z",
     "iopub.status.idle": "2025-10-12T12:56:13.500532Z",
     "shell.execute_reply": "2025-10-12T12:56:13.499721Z",
     "shell.execute_reply.started": "2025-10-12T12:56:13.490083Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "# LSTM Encoder\n",
    "\n",
    "#no of layers=2 #bidirectional=true(1st)\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, hidden_dim=512, num_layers=1, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(feat_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        self.output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        \"\"\"\n",
    "        feats: (B, T, feat_dim)\n",
    "        Returns: (B, T, hidden_dim)\n",
    "        \"\"\"\n",
    "        outputs, _ = self.lstm(feats)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Decoder (No Cross Attention)\n",
    "\n",
    "class TransformerDecoderNoAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, enc_dim, num_layers=3, ff_dim=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.enc_proj = nn.Linear(enc_dim, embed_dim)  \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "\n",
    "    def forward(self, encoder_outs, captions):\n",
    "      \n",
    "        B, T_dec = captions.size()\n",
    "        tgt = self.embedding(captions) * math.sqrt(self.embed_dim)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(T_dec).to(captions.device)\n",
    "        memory = self.enc_proj(encoder_outs)\n",
    "\n",
    "        out = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T12:56:25.143291Z",
     "iopub.status.busy": "2025-10-12T12:56:25.143027Z",
     "iopub.status.idle": "2025-10-12T12:56:25.775304Z",
     "shell.execute_reply": "2025-10-12T12:56:25.774567Z",
     "shell.execute_reply.started": "2025-10-12T12:56:25.143265Z"
    }
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "FEATURE_DIM = 2048\n",
    "ENC_HIDDEN = 512\n",
    "EMBED_SIZE = 512\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = EncoderLSTM(feat_dim=FEATURE_DIM, hidden_dim=ENC_HIDDEN).to(DEVICE)\n",
    "dec = TransformerDecoderNoAttn(\n",
    "    vocab_size=len(vocab.word2idx),\n",
    "    embed_dim=EMBED_SIZE,\n",
    "    enc_dim=enc.output_dim,\n",
    "    num_layers=3\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_one_epoch(train_loader, enc, dec, optimizer, criterion, device, clip=5.0):\n",
    "    enc.train(); dec.train()\n",
    "    running_loss = 0.0\n",
    "    for feats, caps, cap_lens in tqdm(train_loader):\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_outs = enc(feats)\n",
    "        outputs = dec(encoder_outs, caps[:, :-1])\n",
    "        targets = caps[:, 1:]\n",
    "\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Evaluation (greedy decoding)\n",
    "def evaluate(loader, enc, dec, device):\n",
    "    enc.eval(); dec.eval()\n",
    "    all_refs, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, cap_lens in tqdm(loader):\n",
    "            feats, caps = feats.to(device), caps.to(device)\n",
    "            encoder_outs = enc(feats)\n",
    "\n",
    "            batch_size = feats.size(0)\n",
    "            input_seq = torch.full((batch_size, 1),\n",
    "                                   vocab.word2idx[vocab.bos_token],\n",
    "                                   dtype=torch.long, device=device)\n",
    "            preds = [[] for _ in range(batch_size)]\n",
    "\n",
    "            for _ in range(caps.size(1)):\n",
    "                outputs = dec(encoder_outs, input_seq)\n",
    "                next_word = outputs[:, -1, :].argmax(1)\n",
    "                input_seq = torch.cat([input_seq, next_word.unsqueeze(1)], dim=1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    preds[i].append(next_word[i].item())\n",
    "\n",
    "            # Convert tokens to words\n",
    "            for i in range(batch_size):\n",
    "                pred_tokens = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.bos_token], vocab.word2idx[vocab.pad_token]):\n",
    "                        continue\n",
    "                    pred_tokens.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(pred_tokens)\n",
    "\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.bos_token], vocab.word2idx[vocab.pad_token]):\n",
    "                        continue\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)\n",
    "    return bleu4, all_refs, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:05:11.453901Z",
     "iopub.status.busy": "2025-10-12T13:05:11.453602Z",
     "iopub.status.idle": "2025-10-12T13:05:11.465704Z",
     "shell.execute_reply": "2025-10-12T13:05:11.464973Z",
     "shell.execute_reply.started": "2025-10-12T13:05:11.453878Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_with_metrics(loader, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    all_refs = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, caps,cap_lens in tqdm(loader):\n",
    "            feats = feats.to(device)\n",
    "            caps = caps.to(device)\n",
    "\n",
    "            # Encoder forward\n",
    "            encoder_outs = enc(feats)  # (B, T_enc, enc_dim)\n",
    "\n",
    "            # Greedy decoding\n",
    "            batch_size = feats.size(0)\n",
    "            input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]] * batch_size).to(device)\n",
    "            preds = torch.zeros(batch_size, max_len).long().to(device)\n",
    "\n",
    "            for t in range(max_len):\n",
    "                out = dec(encoder_outs, input_word.unsqueeze(1))  # (B, 1, vocab_size)\n",
    "                next_word = out[:, -1, :].argmax(-1)\n",
    "                preds[:, t] = next_word\n",
    "                input_word = next_word  \n",
    "\n",
    "            # Convert preds and refs to token lists\n",
    "            for i in range(batch_size):\n",
    "              \n",
    "                p = []\n",
    "                for tok in preds[i].cpu().numpy():\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    p.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(p)\n",
    "\n",
    "                # Reference tokens\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens]) \n",
    "\n",
    "    # Compute metrics\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)\n",
    "\n",
    "   \n",
    "    # ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l_f1s = [scorer.score(\" \".join(ref[0]), \" \".join(pred))['rougeL'].fmeasure for ref, pred in zip(all_refs, all_preds)]\n",
    "    rouge_l = np.mean(rouge_l_f1s)\n",
    "\n",
    "    # CIDEr\n",
    "   \n",
    "    cider_scorer = Cider()\n",
    "    refs_for_cider = {i: [\" \".join(r[0]) for r in all_refs[i:i+1]] for i in range(len(all_refs))}\n",
    "    preds_for_cider = {i: [\" \".join(all_preds[i])] for i in range(len(all_preds))}  \n",
    "    cider_score, _ = cider_scorer.compute_score(refs_for_cider, preds_for_cider)\n",
    "\n",
    "\n",
    "    return bleu4 ,rouge_l, cider_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T06:31:47.211403Z",
     "iopub.status.busy": "2025-10-05T06:31:47.211096Z",
     "iopub.status.idle": "2025-10-05T07:32:28.635804Z",
     "shell.execute_reply": "2025-10-05T07:32:28.634910Z",
     "shell.execute_reply.started": "2025-10-05T06:31:47.211353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:06<00:00, 16.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 2.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics: BLEU-4=0.0126, ROUGE-L=0.1896, CIDEr=0.1648\n",
      "âœ… Saved new best model at epoch 1 with BLEU=0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:04<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 2.5282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics: BLEU-4=0.0130, ROUGE-L=0.1861, CIDEr=0.1573\n",
      "âœ… Saved new best model at epoch 2 with BLEU=0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:04<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 2.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics: BLEU-4=0.0115, ROUGE-L=0.1767, CIDEr=0.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:04<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 2.3434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics: BLEU-4=0.0114, ROUGE-L=0.1821, CIDEr=0.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:05<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 2.2649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics: BLEU-4=0.0136, ROUGE-L=0.1868, CIDEr=0.1644\n",
      "âœ… Saved new best model at epoch 5 with BLEU=0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:04<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 2.1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:50<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics: BLEU-4=0.0126, ROUGE-L=0.1799, CIDEr=0.1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:04<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 2.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics: BLEU-4=0.0123, ROUGE-L=0.1830, CIDEr=0.1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:04<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 2.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:50<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics: BLEU-4=0.0102, ROUGE-L=0.1753, CIDEr=0.1198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:04<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 2.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics: BLEU-4=0.0094, ROUGE-L=0.1675, CIDEr=0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:05<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train loss: 1.9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:50<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics: BLEU-4=0.0104, ROUGE-L=0.1747, CIDEr=0.1219\n"
     ]
    }
   ],
   "source": [
    "best_bleu = 0.0\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validate with metrics\n",
    "    bleu, rouge_l, cider,_,_ = evaluate_with_metrics(val_loader, enc, dec,vocab, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics: BLEU-4={bleu:.4f}, ROUGE-L={rouge_l:.4f}, CIDEr={cider:.4f}\")\n",
    "\n",
    "    # Save best BLEU model\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_metrics.pth\")\n",
    "        print(f\"âœ… Saved new best model at epoch {epoch} with BLEU={bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T17:11:20.458786Z",
     "iopub.status.busy": "2025-10-11T17:11:20.458480Z",
     "iopub.status.idle": "2025-10-11T18:10:32.167191Z",
     "shell.execute_reply": "2025-10-11T18:10:32.166272Z",
     "shell.execute_reply.started": "2025-10-11T17:11:20.458763Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:52<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 4.0825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:54<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics: BLEU-4=0.0106, ROUGE-L=0.1709, CIDEr=0.1154\n",
      "âœ… Saved new best model at epoch 1 with BLEU=0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:51<00:00, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 3.3970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:54<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics: BLEU-4=0.0128, ROUGE-L=0.2035, CIDEr=0.1869\n",
      "âœ… Saved new best model at epoch 2 with BLEU=0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:52<00:00, 17.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 3.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:54<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics: BLEU-4=0.0131, ROUGE-L=0.2072, CIDEr=0.2056\n",
      "âœ… Saved new best model at epoch 3 with BLEU=0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 2.9193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:54<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics: BLEU-4=0.0151, ROUGE-L=0.2060, CIDEr=0.2021\n",
      "âœ… Saved new best model at epoch 4 with BLEU=0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 2.7653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:54<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics: BLEU-4=0.0138, ROUGE-L=0.1976, CIDEr=0.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:52<00:00, 17.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 2.6369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:54<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics: BLEU-4=0.0138, ROUGE-L=0.1977, CIDEr=0.1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 2.5277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:53<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics: BLEU-4=0.0118, ROUGE-L=0.1817, CIDEr=0.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 2.4315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:53<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics: BLEU-4=0.0118, ROUGE-L=0.1856, CIDEr=0.1493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 2.3466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:54<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics: BLEU-4=0.0127, ROUGE-L=0.1884, CIDEr=0.1487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:52<00:00, 17.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train loss: 2.2680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:55<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics: BLEU-4=0.0124, ROUGE-L=0.1933, CIDEr=0.1611\n"
     ]
    }
   ],
   "source": [
    "#biLSTMEnc(1L)+Tranformer\n",
    "best_bleu = 0.0\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validate with metrics\n",
    "    bleu, rouge_l, cider = evaluate_with_metrics(val_loader, enc, dec,vocab, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics: BLEU-4={bleu:.4f}, ROUGE-L={rouge_l:.4f}, CIDEr={cider:.4f}\")\n",
    "\n",
    "    # Save best BLEU model\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_metrics.pth\")\n",
    "        print(f\"âœ… Saved new best model at epoch {epoch} with BLEU={bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:05:44.760019Z",
     "iopub.status.busy": "2025-10-12T13:05:44.759742Z",
     "iopub.status.idle": "2025-10-12T14:07:32.351566Z",
     "shell.execute_reply": "2025-10-12T14:07:32.350906Z",
     "shell.execute_reply.started": "2025-10-12T13:05:44.760000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:09<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 3.3972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics: BLEU-4=0.0111, ROUGE-L=0.1727, CIDEr=0.1153\n",
      "âœ… Saved new best model at epoch 1 with BLEU=0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:09<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 3.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics: BLEU-4=0.0122, ROUGE-L=0.1936, CIDEr=0.1608\n",
      "âœ… Saved new best model at epoch 2 with BLEU=0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:10<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 2.9168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics: BLEU-4=0.0120, ROUGE-L=0.1891, CIDEr=0.1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:10<00:00, 16.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 2.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics: BLEU-4=0.0141, ROUGE-L=0.1929, CIDEr=0.1598\n",
      "âœ… Saved new best model at epoch 4 with BLEU=0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:10<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 2.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics: BLEU-4=0.0133, ROUGE-L=0.1883, CIDEr=0.1525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:10<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 2.5259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:53<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics: BLEU-4=0.0114, ROUGE-L=0.1809, CIDEr=0.1336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:10<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 2.4299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics: BLEU-4=0.0120, ROUGE-L=0.1830, CIDEr=0.1446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:11<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 2.3455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics: BLEU-4=0.0111, ROUGE-L=0.1900, CIDEr=0.1491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:11<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 2.2668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics: BLEU-4=0.0124, ROUGE-L=0.1954, CIDEr=0.1615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:11<00:00, 16.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train loss: 2.1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics: BLEU-4=0.0113, ROUGE-L=0.1883, CIDEr=0.1427\n"
     ]
    }
   ],
   "source": [
    "#bidrectional LSTM +Transformer=true\n",
    "best_bleu = 0.0\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validate with metrics\n",
    "    bleu, rouge_l, cider = evaluate_with_metrics(val_loader, enc, dec,vocab, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics: BLEU-4={bleu:.4f}, ROUGE-L={rouge_l:.4f}, CIDEr={cider:.4f}\")\n",
    "\n",
    "    # Save best BLEU model\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_metrics.pth\")\n",
    "        print(f\"âœ… Saved new best model at epoch {epoch} with BLEU={bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T08:43:32.915800Z",
     "iopub.status.busy": "2025-10-05T08:43:32.915525Z",
     "iopub.status.idle": "2025-10-05T08:44:32.233657Z",
     "shell.execute_reply": "2025-10-05T08:44:32.232782Z",
     "shell.execute_reply.started": "2025-10-05T08:43:32.915778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded best model checkpoint successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Test Results:\n",
      "BLEU-4  : 0.0117\n",
      "ROUGE-L : 0.1828\n",
      "CIDEr   : 0.1546\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"best_checkpoint_metrics.pth\", map_location=DEVICE)\n",
    "\n",
    "enc.load_state_dict(checkpoint['enc_state'])\n",
    "dec.load_state_dict(checkpoint['dec_state'])\n",
    "print(\" Loaded best model checkpoint successfully.\")\n",
    "\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "\n",
    "bleu, rouge_l, cider = evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "\n",
    "print(\"\\nðŸ“Š Final Test Results:\")\n",
    "print(f\"BLEU-4  : {bleu:.4f}\")\n",
    "print(f\"ROUGE-L : {rouge_l:.4f}\")\n",
    "print(f\"CIDEr   : {cider:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T10:04:50.884343Z",
     "iopub.status.busy": "2025-10-07T10:04:50.884081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 3.4068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:53<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics: BLEU-4=0.0109, ROUGE-L=0.1723, CIDEr=0.1352\n",
      " Saved new best model at epoch 1 with BLEU=0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 3.1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics: BLEU-4=0.0104, ROUGE-L=0.1716, CIDEr=0.1345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:54<00:00, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 2.9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics: BLEU-4=0.0125, ROUGE-L=0.1822, CIDEr=0.1555\n",
      " Saved new best model at epoch 3 with BLEU=0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 2.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:53<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics: BLEU-4=0.0136, ROUGE-L=0.1879, CIDEr=0.1667\n",
      " Saved new best model at epoch 4 with BLEU=0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 2.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics: BLEU-4=0.0128, ROUGE-L=0.1962, CIDEr=0.1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 2.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics: BLEU-4=0.0119, ROUGE-L=0.1857, CIDEr=0.1589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [04:53<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 2.4203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics: BLEU-4=0.0118, ROUGE-L=0.1816, CIDEr=0.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4667/5000 [04:33<00:19, 17.10it/s]"
     ]
    }
   ],
   "source": [
    "#With 2 layer in encoder bilstm \n",
    "best_bleu = 0.0\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validate with metrics\n",
    "    bleu, rouge_l, cider = evaluate_with_metrics(val_loader, enc, dec,vocab, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics: BLEU-4={bleu:.4f}, ROUGE-L={rouge_l:.4f}, CIDEr={cider:.4f}\")\n",
    "\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_metrics.pth\")\n",
    "        print(f\" Saved new best model at epoch {epoch} with BLEU={bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T11:03:14.636536Z",
     "iopub.status.busy": "2025-10-07T11:03:14.635991Z",
     "iopub.status.idle": "2025-10-07T11:04:16.087217Z",
     "shell.execute_reply": "2025-10-07T11:04:16.086437Z",
     "shell.execute_reply.started": "2025-10-07T11:03:14.636512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:53<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "BLEU-4 = 0.0115\n",
      "ROUGE-L = 0.1838\n",
      "CIDEr = 0.1604\n",
      "\n",
      " --- Random Sample Predictions ---\n",
      "\n",
      " Video 1: video7879.mp4\n",
      "Generated Caption: a group of soldiers are fighting\n",
      "Reference Captions:\n",
      "  Ref 1: a big war with swords fighting\n",
      "  Ref 2: a clip from a movie\n",
      "  Ref 3: a clip of a battle with swords\n",
      "\n",
      " Video 2: video6612.mp4\n",
      "Generated Caption: a red color car moving on road and a man standing displaying on screen\n",
      "Reference Captions:\n",
      "  Ref 1: an animated white station wagon rolls down a hill and runs into a frozen banana stand\n",
      "  Ref 2: a cartoon depicting the differences between using their brakes or not using their brakes while coming down a hill towards a frozen banana stand\n",
      "  Ref 3: a cartoon explaining the concept of speed and force into a cartoon frozen banana stand\n",
      "\n",
      " Video 3: video4461.mp4\n",
      "Generated Caption: a group of girls dancing in a room\n",
      "Reference Captions:\n",
      "  Ref 1: a few girls dance together\n",
      "  Ref 2: a girl does a dance in front of a wall and looks silly\n",
      "  Ref 3: a girl is dancing\n",
      "\n",
      " Video 4: video6067.mp4\n",
      "Generated Caption: a woman is standing\n",
      "Reference Captions:\n",
      "  Ref 1: a clip from a tv show called an unlikely encounter\n",
      "  Ref 2: a couple talking at a restaurant\n",
      "  Ref 3: a hindi movie clip\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Load the best saved checkpoint\n",
    "ckpt = torch.load(\"best_checkpoint_metrics.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "#  Evaluate on test set\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider = evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "print(f\"\\nðŸ“Š Test Metrics:\\nBLEU-4 = {test_bleu:.4f}\\nROUGE-L = {test_rouge:.4f}\\nCIDEr = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    feat_tensor = torch.tensor(video_feat).unsqueeze(0).float().to(device)  # (1, T, D)\n",
    "    with torch.no_grad():\n",
    "        enc_outs = enc(feat_tensor)\n",
    "     \n",
    "\n",
    "        input_seq = torch.LongTensor([[vocab.word2idx[vocab.bos_token]]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            out = dec(enc_outs, input_seq)\n",
    "            next_word = out[:, -1, :].argmax(-1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_seq = torch.cat([input_seq, torch.LongTensor([[next_word]]).to(device)], dim=1)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "\n",
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "\n",
    "print(\"\\n --- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid] \n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):  \n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T18:11:43.782539Z",
     "iopub.status.busy": "2025-10-11T18:11:43.781816Z",
     "iopub.status.idle": "2025-10-11T18:12:50.912299Z",
     "shell.execute_reply": "2025-10-11T18:12:50.911322Z",
     "shell.execute_reply.started": "2025-10-11T18:11:43.782510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:57<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "BLEU-4 = 0.0142\n",
      "ROUGE-L = 0.2028\n",
      "CIDEr = 0.2119\n",
      "\n",
      "--- Selected Video Predictions ---\n",
      "\n",
      "Video 1: video6643.mp4\n",
      "Generated Caption: a man is talking about something\n",
      "Reference Captions:\n",
      "  Ref 1: a clip showing a man giving instructions to others\n",
      "  Ref 2: a guy is giving a speech in front of an audience\n",
      "  Ref 3: a man giving a motavational speech\n",
      "\n",
      "Video 2: video1850.mp4\n",
      "Generated Caption: a man in a suit is talking about something\n",
      "Reference Captions:\n",
      "  Ref 1: a gray haired news anchor with a purple tie discusses an election\n",
      "  Ref 2: there is a suit man is talking about election\n",
      "  Ref 3: there is a suit man is talking about politics\n",
      "\n",
      "Video 3: video4829.mp4\n",
      "Generated Caption: a man and woman are kissing each other\n",
      "Reference Captions:\n",
      "  Ref 1: a couple is singing different parts of the same song\n",
      "  Ref 2: a couple is watching a movie\n",
      "  Ref 3: a couple singing a song\n",
      "\n",
      "Video 4: video8925.mp4\n",
      "Generated Caption: a band is performing a song\n",
      "Reference Captions:\n",
      "  Ref 1: a band is performing a song\n",
      "  Ref 2: a band is performing on stage\n",
      "  Ref 3: a band is playing a song on stage\n",
      "\n",
      " --- Random Sample Predictions ---\n",
      "\n",
      " Video 1: video7879.mp4\n",
      "Generated Caption: a group of soldiers are running around in a field\n",
      "Reference Captions:\n",
      "  Ref 1: a big war with swords fighting\n",
      "  Ref 2: a clip from a movie\n",
      "  Ref 3: a clip of a battle with swords\n",
      "\n",
      " Video 2: video6612.mp4\n",
      "Generated Caption: a man is talking about a car\n",
      "Reference Captions:\n",
      "  Ref 1: an animated white station wagon rolls down a hill and runs into a frozen banana stand\n",
      "  Ref 2: a cartoon depicting the differences between using their brakes or not using their brakes while coming down a hill towards a frozen banana stand\n",
      "  Ref 3: a cartoon explaining the concept of speed and force into a cartoon frozen banana stand\n",
      "\n",
      " Video 3: video4461.mp4\n",
      "Generated Caption: a group of girls are dancing\n",
      "Reference Captions:\n",
      "  Ref 1: a few girls dance together\n",
      "  Ref 2: a girl does a dance in front of a wall and looks silly\n",
      "  Ref 3: a girl is dancing\n",
      "\n",
      " Video 4: video6067.mp4\n",
      "Generated Caption: a woman is talking about a movie\n",
      "Reference Captions:\n",
      "  Ref 1: a clip from a tv show called an unlikely encounter\n",
      "  Ref 2: a couple talking at a restaurant\n",
      "  Ref 3: a hindi movie clip\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Load the best saved checkpoint\n",
    "ckpt = torch.load(\"best_checkpoint_metrics.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "#  Evaluate on test set\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider = evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "print(f\"\\nðŸ“Š Test Metrics:\\nBLEU-4 = {test_bleu:.4f}\\nROUGE-L = {test_rouge:.4f}\\nCIDEr = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    feat_tensor = torch.tensor(video_feat).unsqueeze(0).float().to(device)  # (1, T, D)\n",
    "    with torch.no_grad():\n",
    "        enc_outs = enc(feat_tensor)\n",
    "\n",
    "\n",
    "        input_seq = torch.LongTensor([[vocab.word2idx[vocab.bos_token]]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            out = dec(enc_outs, input_seq)\n",
    "            next_word = out[:, -1, :].argmax(-1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_seq = torch.cat([input_seq, torch.LongTensor([[next_word]]).to(device)], dim=1)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "    \n",
    "def predict_for_selected_videos(selected_vids, features_dir, test_items, enc, dec, vocab, device, max_len=20):\n",
    "  \n",
    "    print(\"\\n--- Selected Video Predictions ---\")\n",
    "    for i, vid in enumerate(selected_vids):\n",
    "        feat_path = os.path.join(features_dir, vid.replace('.mp4', '.npy'))\n",
    "        if not os.path.exists(feat_path):\n",
    "            print(f\"âš ï¸ Feature file missing for {vid}\")\n",
    "            continue\n",
    "\n",
    "        video_feat = np.load(feat_path)\n",
    "        generated_caption = generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len)\n",
    "        references = test_items.get(vid, [])\n",
    "\n",
    "        print(f\"\\nVideo {i+1}: {vid}\")\n",
    "        print(f\"Generated Caption: {generated_caption}\")\n",
    "        print(\"Reference Captions:\")\n",
    "        for j, ref in enumerate(references[:3]):  # show up to 3 refs\n",
    "            print(f\"  Ref {j+1}: {ref}\")\n",
    "\n",
    "# Example: Choose specific video IDs from your test set\n",
    "selected_vids = [\n",
    "    \"video6643.mp4\",\n",
    "    \"video1850.mp4\",\n",
    "    \"video4829.mp4\",\n",
    "    \"video8925.mp4\"\n",
    "]\n",
    "\n",
    "predict_for_selected_videos(\n",
    "    selected_vids,\n",
    "    FEATURES_DIR,\n",
    "    test_items,\n",
    "    enc,\n",
    "    dec,\n",
    "    vocab,\n",
    "    DEVICE,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "# few random test samples\n",
    "\n",
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "\n",
    "print(\"\\n --- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):  \n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between two Decoders used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T18:52:44.784912Z",
     "iopub.status.busy": "2025-10-11T18:52:44.784612Z",
     "iopub.status.idle": "2025-10-11T18:52:44.978077Z",
     "shell.execute_reply": "2025-10-11T18:52:44.977054Z",
     "shell.execute_reply.started": "2025-10-11T18:52:44.784891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['LSTM+Luong', 'LSTM+Transformer']\n",
    "bleu = [0.0535, 0.0142]\n",
    "rouge = [0.281, 0.2028]\n",
    "cider = [0.5009, 0.2119]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(models, bleu, marker='o', label='BLEU-4')\n",
    "plt.plot(models, rouge, marker='s', label='ROUGE-L')\n",
    "plt.plot(models, cider, marker='^', label='CIDEr')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Video Caption Generation Metric Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:07:47.520152Z",
     "iopub.status.busy": "2025-10-12T14:07:47.519895Z",
     "iopub.status.idle": "2025-10-12T14:08:47.233240Z",
     "shell.execute_reply": "2025-10-12T14:08:47.232462Z",
     "shell.execute_reply.started": "2025-10-12T14:07:47.520128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "BLEU-4 = 0.0132\n",
      "ROUGE-L = 0.1949\n",
      "CIDEr = 0.1782\n",
      "\n",
      "--- Selected Video Predictions ---\n",
      "\n",
      "Video 1: video6643.mp4\n",
      "Generated Caption: a man is talking about something\n",
      "Reference Captions:\n",
      "  Ref 1: a clip showing a man giving instructions to others\n",
      "  Ref 2: a guy is giving a speech in front of an audience\n",
      "  Ref 3: a man giving a motavational speech\n",
      "\n",
      "Video 2: video1850.mp4\n",
      "Generated Caption: a man in a suit is talking about the latest news\n",
      "Reference Captions:\n",
      "  Ref 1: a gray haired news anchor with a purple tie discusses an election\n",
      "  Ref 2: there is a suit man is talking about election\n",
      "  Ref 3: there is a suit man is talking about politics\n",
      "\n",
      "Video 3: video4829.mp4\n",
      "Generated Caption: a woman is talking to another woman\n",
      "Reference Captions:\n",
      "  Ref 1: a couple is singing different parts of the same song\n",
      "  Ref 2: a couple is watching a movie\n",
      "  Ref 3: a couple singing a song\n",
      "\n",
      "Video 4: video8925.mp4\n",
      "Generated Caption: a band is playing on stage\n",
      "Reference Captions:\n",
      "  Ref 1: a band is performing a song\n",
      "  Ref 2: a band is performing on stage\n",
      "  Ref 3: a band is playing a song on stage\n",
      "\n",
      " --- Random Sample Predictions ---\n",
      "\n",
      " Video 1: video7879.mp4\n",
      "Generated Caption: a group of soldiers are fighting\n",
      "Reference Captions:\n",
      "  Ref 1: a big war with swords fighting\n",
      "  Ref 2: a clip from a movie\n",
      "  Ref 3: a clip of a battle with swords\n",
      "\n",
      " Video 2: video6612.mp4\n",
      "Generated Caption: a man is talking about a car\n",
      "Reference Captions:\n",
      "  Ref 1: an animated white station wagon rolls down a hill and runs into a frozen banana stand\n",
      "  Ref 2: a cartoon depicting the differences between using their brakes or not using their brakes while coming down a hill towards a frozen banana stand\n",
      "  Ref 3: a cartoon explaining the concept of speed and force into a cartoon frozen banana stand\n",
      "\n",
      " Video 3: video4461.mp4\n",
      "Generated Caption: a group of girls are dancing to music\n",
      "Reference Captions:\n",
      "  Ref 1: a few girls dance together\n",
      "  Ref 2: a girl does a dance in front of a wall and looks silly\n",
      "  Ref 3: a girl is dancing\n",
      "\n",
      " Video 4: video6067.mp4\n",
      "Generated Caption: a woman is sitting in a room and talking\n",
      "Reference Captions:\n",
      "  Ref 1: a clip from a tv show called an unlikely encounter\n",
      "  Ref 2: a couple talking at a restaurant\n",
      "  Ref 3: a hindi movie clip\n"
     ]
    }
   ],
   "source": [
    "#bidirectional=True +transformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Load the best saved checkpoint\n",
    "\n",
    "ckpt = torch.load(\"best_checkpoint_metrics.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "\n",
    "#Evaluate on test set\n",
    "\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider = evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "print(f\"\\nðŸ“Š Test Metrics:\\nBLEU-4 = {test_bleu:.4f}\\nROUGE-L = {test_rouge:.4f}\\nCIDEr = {test_cider:.4f}\")\n",
    "\n",
    "#  Function to generate caption for a single video\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    feat_tensor = torch.tensor(video_feat).unsqueeze(0).float().to(device)  # (1, T, D)\n",
    "    with torch.no_grad():\n",
    "        enc_outs = enc(feat_tensor)\n",
    "     \n",
    "\n",
    "        input_seq = torch.LongTensor([[vocab.word2idx[vocab.bos_token]]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            out = dec(enc_outs, input_seq)\n",
    "            next_word = out[:, -1, :].argmax(-1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_seq = torch.cat([input_seq, torch.LongTensor([[next_word]]).to(device)], dim=1)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "    \n",
    "def predict_for_selected_videos(selected_vids, features_dir, test_items, enc, dec, vocab, device, max_len=20):\n",
    "   \n",
    "    print(\"\\n--- Selected Video Predictions ---\")\n",
    "    for i, vid in enumerate(selected_vids):\n",
    "        feat_path = os.path.join(features_dir, vid.replace('.mp4', '.npy'))\n",
    "        if not os.path.exists(feat_path):\n",
    "            print(f\" Feature file missing for {vid}\")\n",
    "            continue\n",
    "\n",
    "        video_feat = np.load(feat_path)\n",
    "        generated_caption = generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len)\n",
    "        references = test_items.get(vid, [])\n",
    "\n",
    "        print(f\"\\nVideo {i+1}: {vid}\")\n",
    "        print(f\"Generated Caption: {generated_caption}\")\n",
    "        print(\"Reference Captions:\")\n",
    "        for j, ref in enumerate(references[:3]):  # show up to 3 refs\n",
    "            print(f\"  Ref {j+1}: {ref}\")\n",
    "\n",
    "# Example: Choose specific video IDs from your test set\n",
    "selected_vids = [\n",
    "    \"video6643.mp4\",\n",
    "    \"video1850.mp4\",\n",
    "    \"video4829.mp4\",\n",
    "    \"video8925.mp4\"\n",
    "]\n",
    "\n",
    "predict_for_selected_videos(\n",
    "    selected_vids,\n",
    "    FEATURES_DIR,\n",
    "    test_items,\n",
    "    enc,\n",
    "    dec,\n",
    "    vocab,\n",
    "    DEVICE,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# ðŸ”¹ Pick a few random test samples\n",
    "# =====================================\n",
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "\n",
    "print(\"\\n --- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid]  # list of ground-truth captions\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):  # print up to 3 refs\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other experiments done: Spatial+temporal attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T11:06:45.936804Z",
     "iopub.status.busy": "2025-10-07T11:06:45.936524Z",
     "iopub.status.idle": "2025-10-07T11:06:45.944074Z",
     "shell.execute_reply": "2025-10-07T11:06:45.943323Z",
     "shell.execute_reply.started": "2025-10-07T11:06:45.936783Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpatialTemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Add lightweight spatial + temporal attention between encoder and decoder.\n",
    "    Works with (B, T, D_enc) encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_dim, use_spatial=False):\n",
    "        super().__init__()\n",
    "        self.enc_dim = enc_dim\n",
    "        self.use_spatial = use_spatial\n",
    "\n",
    "        # Temporal attention: learns to weight frames\n",
    "        self.temporal_attn = nn.Sequential(\n",
    "            nn.Linear(enc_dim, enc_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(enc_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "       \n",
    "        if use_spatial:\n",
    "            self.spatial_attn = nn.Sequential(\n",
    "                nn.Linear(enc_dim, enc_dim // 2),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(enc_dim // 2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, enc_outs, spatial_feats=None):\n",
    "        \"\"\"\n",
    "        enc_outs: (B, T, D_enc)\n",
    "        spatial_feats (optional): (B, T, R, D_enc)\n",
    "        \"\"\"\n",
    "        # 1ï¸ Spatial attention (optional)\n",
    "        if self.use_spatial and spatial_feats is not None:\n",
    "            attn_weights = F.softmax(self.spatial_attn(spatial_feats).squeeze(-1), dim=-1)  # (B,T,R)\n",
    "            spatial_context = (attn_weights.unsqueeze(-1) * spatial_feats).sum(dim=2)      # (B,T,D)\n",
    "            enc_outs = enc_outs + spatial_context  # residual fusion\n",
    "\n",
    "        # 2ï¸ Temporal attention\n",
    "        energy = self.temporal_attn(enc_outs).squeeze(-1)      # (B,T)\n",
    "        weights = F.softmax(energy, dim=-1).unsqueeze(-1)      # (B,T,1)\n",
    "        attended = (weights * enc_outs).sum(dim=1, keepdim=True)  # (B,1,D)\n",
    "\n",
    "        # Optionally repeat context for decoder\n",
    "        refined_outs = attended.repeat(1, enc_outs.size(1), 1)\n",
    "        return refined_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T11:06:58.255090Z",
     "iopub.status.busy": "2025-10-07T11:06:58.254806Z",
     "iopub.status.idle": "2025-10-07T11:06:58.540938Z",
     "shell.execute_reply": "2025-10-07T11:06:58.540330Z",
     "shell.execute_reply.started": "2025-10-07T11:06:58.255071Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_DIM = 2048\n",
    "ENC_HIDDEN = 512\n",
    "EMBED_SIZE = 512\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = EncoderLSTM(feat_dim=FEATURE_DIM, hidden_dim=ENC_HIDDEN).to(DEVICE)\n",
    "dec = TransformerDecoderNoAttn(\n",
    "    vocab_size=len(vocab.word2idx),\n",
    "    embed_dim=EMBED_SIZE,\n",
    "    enc_dim=enc.output_dim,\n",
    "    num_layers=3\n",
    ").to(DEVICE)\n",
    "\n",
    "# âœ… Add the attention bridge\n",
    "attn_refiner = SpatialTemporalAttention(enc_dim=enc.output_dim).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "params = list(enc.parameters()) + list(dec.parameters()) + list(attn_refiner.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T11:07:12.754566Z",
     "iopub.status.busy": "2025-10-07T11:07:12.753746Z",
     "iopub.status.idle": "2025-10-07T11:07:12.760395Z",
     "shell.execute_reply": "2025-10-07T11:07:12.759856Z",
     "shell.execute_reply.started": "2025-10-07T11:07:12.754532Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, enc, dec, attn_refiner, optimizer, criterion, device, clip=5.0):\n",
    "    enc.train(); dec.train(); attn_refiner.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for feats, caps, cap_lens in tqdm(train_loader):\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_outs = enc(feats)                        # (B, T, D_enc)\n",
    "        refined_enc_outs = attn_refiner(encoder_outs)    # (B, T, D_enc) â€” attended features\n",
    "\n",
    "        outputs = dec(refined_enc_outs, caps[:, :-1])\n",
    "        targets = caps[:, 1:]\n",
    "\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:10:48.991709Z",
     "iopub.status.busy": "2025-10-05T13:10:48.991194Z",
     "iopub.status.idle": "2025-10-05T14:11:47.712126Z",
     "shell.execute_reply": "2025-10-05T14:11:47.711248Z",
     "shell.execute_reply.started": "2025-10-05T13:10:48.991687Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:08<00:00, 16.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 4.1122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics: BLEU-4=0.0110, ROUGE-L=0.1772, CIDEr=0.1015\n",
      " Saved new best model at epoch 1 with BLEU=0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:05<00:00, 16.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 3.4253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics: BLEU-4=0.0110, ROUGE-L=0.1765, CIDEr=0.1349\n",
      " Saved new best model at epoch 2 with BLEU=0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:06<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 3.1480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics: BLEU-4=0.0104, ROUGE-L=0.1979, CIDEr=0.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:05<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 2.9532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics: BLEU-4=0.0114, ROUGE-L=0.1960, CIDEr=0.1723\n",
      " Saved new best model at epoch 4 with BLEU=0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:06<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 2.7999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics: BLEU-4=0.0126, ROUGE-L=0.1890, CIDEr=0.1669\n",
      " Saved new best model at epoch 5 with BLEU=0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:06<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 2.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics: BLEU-4=0.0106, ROUGE-L=0.1837, CIDEr=0.1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:05<00:00, 16.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 2.5680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics: BLEU-4=0.0117, ROUGE-L=0.1879, CIDEr=0.1537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:06<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 2.4735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics: BLEU-4=0.0102, ROUGE-L=0.1757, CIDEr=0.1229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:05<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 2.3901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics: BLEU-4=0.0106, ROUGE-L=0.1929, CIDEr=0.1593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [05:06<00:00, 16.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train loss: 2.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:51<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics: BLEU-4=0.0104, ROUGE-L=0.1873, CIDEr=0.1519\n"
     ]
    }
   ],
   "source": [
    "best_bleu = 0.0\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, attn_refiner, optimizer, criterion, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validate with metrics\n",
    "    bleu, rouge_l, cider = evaluate_with_metrics(val_loader, enc, dec,attn_refiner, vocab, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics: BLEU-4={bleu:.4f}, ROUGE-L={rouge_l:.4f}, CIDEr={cider:.4f}\")\n",
    "\n",
    "    # Save best BLEU model\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_metrics.pth\")\n",
    "        print(f\" Saved new best model at epoch {epoch} with BLEU={bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T11:14:42.996605Z",
     "iopub.status.busy": "2025-10-07T11:14:42.996252Z",
     "iopub.status.idle": "2025-10-07T11:14:43.009204Z",
     "shell.execute_reply": "2025-10-07T11:14:43.008421Z",
     "shell.execute_reply.started": "2025-10-07T11:14:42.996577Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_with_metrics(loader, enc, dec, attn_refiner,vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    all_refs = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, caps,cap_lens in tqdm(loader):\n",
    "            feats = feats.to(device)\n",
    "            caps = caps.to(device)\n",
    "\n",
    "            # Encoder forward\n",
    "            encoder_outs = enc(feats)  # (B, T_enc, enc_dim)\n",
    "\n",
    "            encoder_outs = attn_refiner(encoder_outs)\n",
    "\n",
    "            # Greedy decoding\n",
    "            batch_size = feats.size(0)\n",
    "            input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]] * batch_size).to(device)\n",
    "            preds = torch.zeros(batch_size, max_len).long().to(device)\n",
    "\n",
    "            for t in range(max_len):\n",
    "                out = dec(encoder_outs, input_word.unsqueeze(1))  # (B, 1, vocab_size)\n",
    "                next_word = out[:, -1, :].argmax(-1)\n",
    "                preds[:, t] = next_word\n",
    "                input_word = next_word \n",
    "\n",
    "            # Convert preds and refs to token lists\n",
    "            for i in range(batch_size):\n",
    "              \n",
    "                p = []\n",
    "                for tok in preds[i].cpu().numpy():\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    p.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(p)\n",
    "\n",
    "                # Reference tokens\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])  \n",
    "\n",
    "  \n",
    "    # Compute metrics\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)\n",
    "\n",
    "  \n",
    "\n",
    "    # ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l_f1s = [scorer.score(\" \".join(ref[0]), \" \".join(pred))['rougeL'].fmeasure for ref, pred in zip(all_refs, all_preds)]\n",
    "    rouge_l = np.mean(rouge_l_f1s)\n",
    "\n",
    "  #CidER\n",
    "    cider_scorer = Cider()\n",
    "    refs_for_cider = {i: [\" \".join(r[0]) for r in all_refs[i:i+1]] for i in range(len(all_refs))}\n",
    "    preds_for_cider = {i: [\" \".join(all_preds[i])] for i in range(len(all_preds))} \n",
    "    cider_score, _ = cider_scorer.compute_score(refs_for_cider, preds_for_cider)\n",
    "\n",
    "\n",
    "    return bleu4 ,rouge_l, cider_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T10:44:01.916983Z",
     "iopub.status.busy": "2025-10-05T10:44:01.916673Z",
     "iopub.status.idle": "2025-10-05T10:45:00.994960Z",
     "shell.execute_reply": "2025-10-05T10:45:00.994147Z",
     "shell.execute_reply.started": "2025-10-05T10:44:01.916956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:50<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "BLEU-4 = 0.0107\n",
      "ROUGE-L = 0.1760\n",
      "CIDEr = 0.1489\n",
      "\n",
      " --- Random Sample Predictions ---\n",
      "\n",
      " Video 1: video3818.mp4\n",
      "Generated Caption: a man is playing a game of ping pong\n",
      "Reference Captions:\n",
      "  Ref 1: a child preforms a gymnastics preformance\n",
      "  Ref 2: a female gymnast is jumping\n",
      "  Ref 3: a girl is doing gymnastics on the mat\n",
      "\n",
      " Video 2: video350.mp4\n",
      "Generated Caption: a man is playing a video game\n",
      "Reference Captions:\n",
      "  Ref 1: a game is being played\n",
      "  Ref 2: a gamer is playing minecraft\n",
      "  Ref 3: a man and a woman playing a video  game\n",
      "\n",
      " Video 3: video2520.mp4\n",
      "Generated Caption: a man is looking at a car\n",
      "Reference Captions:\n",
      "  Ref 1: a cartoon about a boy\n",
      "  Ref 2: a cartoon character swims in rough waters\n",
      "  Ref 3: a cartoon of a boy in the ocean\n",
      "\n",
      " Video 4: video4377.mp4\n",
      "Generated Caption: a woman is singing a song in a music video\n",
      "Reference Captions:\n",
      "  Ref 1: rihanna wears camouflage pants a white top and sings on top of a car\n",
      "  Ref 2: rihanna wears large hoop earrings and sings while climbing on a car\n",
      "  Ref 3: a woman is dancing on top of a car and singing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#  Load the best saved checkpoint\n",
    "ckpt = torch.load(\"best_checkpoint_metrics.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "#  Evaluate on test set\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider = evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "print(f\"\\nðŸ“Š Test Metrics:\\nBLEU-4 = {test_bleu:.4f}\\nROUGE-L = {test_rouge:.4f}\\nCIDEr = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    feat_tensor = torch.tensor(video_feat).unsqueeze(0).float().to(device)  # (1, T, D)\n",
    "    with torch.no_grad():\n",
    "        enc_outs = enc(feat_tensor)\n",
    "\n",
    "\n",
    "        input_seq = torch.LongTensor([[vocab.word2idx[vocab.bos_token]]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            out = dec(enc_outs, input_seq)\n",
    "            next_word = out[:, -1, :].argmax(-1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_seq = torch.cat([input_seq, torch.LongTensor([[next_word]]).to(device)], dim=1)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "# few random test samples\n",
    "sample_videos = random.sample(list(test_items.keys()), 4)\n",
    "\n",
    "print(\"\\n --- Random Sample Predictions ---\")\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    sample_feat_path = os.path.join(FEATURES_DIR, vid.replace('.mp4', '.npy'))\n",
    "    sample_feat = np.load(sample_feat_path)\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid] \n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\"Generated Caption: {generated_caption}\")\n",
    "    print(\"Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]): \n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8207060,
     "sourceId": 12967447,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8237579,
     "sourceId": 13011387,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8398319,
     "sourceId": 13253411,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8425256,
     "sourceId": 13293134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8483908,
     "sourceId": 13372589,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8505390,
     "sourceId": 13402486,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8512190,
     "sourceId": 13412337,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
