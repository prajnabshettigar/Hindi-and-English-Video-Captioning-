{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T12:32:59.292020Z",
     "iopub.status.busy": "2025-11-13T12:32:59.291738Z",
     "iopub.status.idle": "2025-11-13T12:32:59.383381Z",
     "shell.execute_reply": "2025-11-13T12:32:59.382401Z",
     "shell.execute_reply.started": "2025-11-13T12:32:59.291989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dimensions of: /kaggle/input/msr-vtt-1289-hindi-english/features_global/features_global/video209.npy\n",
      "Shape: (16, 2048)\n",
      "Data type: float32\n"
     ]
    }
   ],
   "source": [
    "# Load and check dimensions of Global Features\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "folder_path = \"/kaggle/input/msr-vtt-1289-hindi-english/features_global/features_global\"\n",
    "\n",
    "\n",
    "npy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "\n",
    "\n",
    "if not npy_files:\n",
    "    print(\"No .npy files found in the folder.\")\n",
    "else:\n",
    "  \n",
    "    file_path = os.path.join(folder_path, npy_files[0])\n",
    "    print(f\"Checking dimensions of: {file_path}\")\n",
    "\n",
    "   \n",
    "    data = np.load(file_path)\n",
    "    print(\"Shape:\", data.shape)\n",
    "    print(\"Data type:\", data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T12:33:14.979028Z",
     "iopub.status.busy": "2025-11-13T12:33:14.978193Z",
     "iopub.status.idle": "2025-11-13T12:33:15.108962Z",
     "shell.execute_reply": "2025-11-13T12:33:15.107801Z",
     "shell.execute_reply.started": "2025-11-13T12:33:14.978990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dimensions of: /kaggle/input/msr-vtt-1289-hindi-english/features_local/features_local/video209.npy\n",
      "Shape: (16, 49, 2048)\n",
      "Data type: float32\n"
     ]
    }
   ],
   "source": [
    "# Load and check dimensions of local Features\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "folder_path = \"/kaggle/input/msr-vtt-1289-hindi-english/features_local/features_local\"\n",
    "\n",
    "npy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "\n",
    "\n",
    "if not npy_files:\n",
    "    print(\"No .npy files found in the folder.\")\n",
    "else:\n",
    "  \n",
    "    file_path = os.path.join(folder_path, npy_files[0])\n",
    "    print(f\"Checking dimensions of: {file_path}\")\n",
    "\n",
    "  \n",
    "    data = np.load(file_path)\n",
    "    print(\"Shape:\", data.shape)\n",
    "    print(\"Data type:\", data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T12:33:41.046365Z",
     "iopub.status.busy": "2025-11-13T12:33:41.046030Z",
     "iopub.status.idle": "2025-11-13T12:33:41.055277Z",
     "shell.execute_reply": "2025-11-13T12:33:41.054269Z",
     "shell.execute_reply.started": "2025-11-13T12:33:41.046337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dimensions of: /kaggle/input/msr-vtt-1289-hindi-english/features_motion/features_motion/video209.npy\n",
      "Shape: (512,)\n",
      "Data type: float32\n"
     ]
    }
   ],
   "source": [
    "# Load and check dimensions of motion Features\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "folder_path = \"/kaggle/input/msr-vtt-1289-hindi-english/features_motion/features_motion\"\n",
    "\n",
    "\n",
    "npy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "\n",
    "\n",
    "if not npy_files:\n",
    "    print(\"No .npy files found in the folder.\")\n",
    "else:\n",
    " \n",
    "    file_path = os.path.join(folder_path, npy_files[0])\n",
    "    print(f\"Checking dimensions of: {file_path}\")\n",
    "\n",
    "  \n",
    "    data = np.load(file_path)\n",
    "    print(\"Shape:\", data.shape)\n",
    "    print(\"Data type:\", data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:28:23.285378Z",
     "iopub.status.busy": "2025-11-24T17:28:23.285148Z",
     "iopub.status.idle": "2025-11-24T17:28:35.236913Z",
     "shell.execute_reply": "2025-11-24T17:28:35.236099Z",
     "shell.execute_reply.started": "2025-11-24T17:28:23.285353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:42:04.477562Z",
     "iopub.status.busy": "2025-11-23T17:42:04.477229Z",
     "iopub.status.idle": "2025-11-23T17:42:04.483526Z",
     "shell.execute_reply": "2025-11-23T17:42:04.482575Z",
     "shell.execute_reply.started": "2025-11-23T17:42:04.477533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Parameters for BiLSTM Encoder-LSTM Decoder model\n",
    "SAMPLE_FRAMES = 16\n",
    "FEATURE_DIM   = 4608   # ‚Üê 2048 + 2048 + 512\n",
    "ENC_HIDDEN    = 512\n",
    "DEC_HIDDEN    = 512\n",
    "EMBED_SIZE    = 512\n",
    "BATCH_SIZE    = 32\n",
    "LR            = 1e-4\n",
    "EPOCHS        = 15\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T13:12:33.971105Z",
     "iopub.status.busy": "2025-11-13T13:12:33.970704Z",
     "iopub.status.idle": "2025-11-13T13:12:34.057885Z",
     "shell.execute_reply": "2025-11-13T13:12:34.056983Z",
     "shell.execute_reply.started": "2025-11-13T13:12:33.971074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Total entries in JSON: 10000\n",
      "üîπ Unique video IDs: 10000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#  captions.json file with english captions for 10000 videos(20 for each)\n",
    "CAPTIONS_JSON = \"/kaggle/input/msr-vtt-1289-hindi-english/captions.json\"\n",
    "\n",
    "# Load JSON file\n",
    "with open(CAPTIONS_JSON, \"r\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "\n",
    "video_ids = list(captions.keys())\n",
    "\n",
    "\n",
    "unique_videos = len(set(video_ids))\n",
    "\n",
    "print(f\"üîπ Total entries in JSON: {len(video_ids)}\")\n",
    "print(f\"üîπ Unique video IDs: {unique_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T06:45:31.511214Z",
     "iopub.status.busy": "2025-11-15T06:45:31.510496Z",
     "iopub.status.idle": "2025-11-15T06:45:43.168271Z",
     "shell.execute_reply": "2025-11-15T06:45:43.167490Z",
     "shell.execute_reply.started": "2025-11-15T06:45:31.511186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 16834\n"
     ]
    }
   ],
   "source": [
    "#  vocabulary from captions for English Caption Generation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "CAPTIONS_FILE = os.path.join(\"/kaggle/input/msr-vtt-1289-hindi-english/captions.json\") \n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, freq_threshold=1, max_size=None):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        for i,w in enumerate([self.pad_token, self.bos_token, self.eos_token, self.unk_token]):\n",
    "            self.word2idx[w] = i\n",
    "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
    "\n",
    "    def build_vocab(self, captions_dict):\n",
    "        counter = Counter()\n",
    "        for vid, caps in captions_dict.items():\n",
    "            for c in caps:\n",
    "                tokens = [t.lower() for t in word_tokenize(c)]\n",
    "                counter.update(tokens)\n",
    "        # filter\n",
    "        words = [w for w,c in counter.items() if c >= self.freq_threshold]\n",
    "        words = sorted(words, key=lambda w: (-counter[w], w))\n",
    "        if self.max_size:\n",
    "            words = words[:self.max_size - len(self.word2idx)]\n",
    "        idx = len(self.word2idx)\n",
    "        for w in words:\n",
    "            self.word2idx[w] = idx\n",
    "            self.idx2word[idx] = w\n",
    "            idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = [t.lower() for t in word_tokenize(text)]\n",
    "        nums = [self.word2idx.get(t, self.word2idx[self.unk_token]) for t in tokens]\n",
    "        return [self.word2idx[self.bos_token]] + nums + [self.word2idx[self.eos_token]]\n",
    "\n",
    "# Load captions.json\n",
    "with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "vocab = Vocab(freq_threshold=2, max_size=20000)\n",
    "vocab.build_vocab(captions)\n",
    "print(\"Vocab size:\", len(vocab.word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T15:55:48.382822Z",
     "iopub.status.busy": "2025-11-20T15:55:48.382527Z",
     "iopub.status.idle": "2025-11-20T15:55:48.657085Z",
     "shell.execute_reply": "2025-11-20T15:55:48.656412Z",
     "shell.execute_reply.started": "2025-11-20T15:55:48.382800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos in captions.json: 10000\n",
      "Videos after filtering: 1290\n",
      "First few video IDs: ['video0.mp4', 'video1.mp4', 'video2.mp4', 'video3.mp4', 'video4.mp4', 'video5.mp4', 'video6.mp4', 'video7.mp4', 'video8.mp4', 'video9.mp4']\n",
      "Last few video IDs: ['video1280.mp4', 'video1281.mp4', 'video1282.mp4', 'video1283.mp4', 'video1284.mp4', 'video1285.mp4', 'video1286.mp4', 'video1287.mp4', 'video1288.mp4', 'video1289.mp4']\n"
     ]
    }
   ],
   "source": [
    "#Selecting first 1290 videos in order\n",
    "import json\n",
    "\n",
    "CAPTIONS_JSON = \"/kaggle/input/msr-vtt-1289-hindi-english/captions.json\"\n",
    "\n",
    "# Load the captions\n",
    "with open(CAPTIONS_JSON, \"r\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "print(\"Total videos in captions.json:\", len(captions))\n",
    "\n",
    "# Function to extract number from 'video1234.mp4'\n",
    "def get_video_number(vid_name):\n",
    "    return int(''.join(ch for ch in vid_name if ch.isdigit()))\n",
    "\n",
    "# Sort videos numerically by number\n",
    "sorted_videos = sorted(captions.keys(), key=get_video_number)\n",
    "\n",
    "# Keep first 1289 videos\n",
    "captions_filtered = {vid: captions[vid] for vid in sorted_videos[:1290]}\n",
    "\n",
    "print(\"Videos after filtering:\", len(captions_filtered))\n",
    "print(\"First few video IDs:\", list(captions_filtered.keys())[:10])\n",
    "print(\"Last few video IDs:\", list(captions_filtered.keys())[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset into Dataloager and forming Train,validation and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:14:20.699302Z",
     "iopub.status.busy": "2025-11-24T18:14:20.699023Z",
     "iopub.status.idle": "2025-11-24T18:14:20.737051Z",
     "shell.execute_reply": "2025-11-24T18:14:20.736232Z",
     "shell.execute_reply.started": "2025-11-24T18:14:20.699280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20640 Val size: 2580 Test size: 2580\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "FEATURES_GLOBAL_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_global/features_global\"\n",
    "FEATURES_LOCAL_DIR  = \"/kaggle/input/msr-vtt-1289-hindi-english/features_local/features_local\"\n",
    "FEATURES_MOTION_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_motion/features_motion\"\n",
    "\n",
    "class MSRVTTMultiFeatureDataset(Dataset):\n",
    "    def __init__(self, captions_dict, vocab, sample_frames=16, max_caption_len=30):\n",
    "        self.items = [(vid, c) for vid, caps in captions_dict.items() for c in caps]\n",
    "        self.vocab = vocab\n",
    "        self.sample_frames = sample_frames\n",
    "        self.max_caption_len = max_caption_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, cap = self.items[idx]\n",
    "        vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "\n",
    "       \n",
    "        global_path = os.path.join(FEATURES_GLOBAL_DIR, vid_id)\n",
    "        local_path  = os.path.join(FEATURES_LOCAL_DIR, vid_id)\n",
    "        motion_path = os.path.join(FEATURES_MOTION_DIR, vid_id)\n",
    "\n",
    "        global_feat = np.load(global_path)   # (T, 2048)\n",
    "        local_feat  = np.load(local_path)    # (T, 49, 2048)\n",
    "        motion_feat = np.load(motion_path)   # (512,)\n",
    "\n",
    "\n",
    "        \n",
    "        #  Fix unexpected shapes \n",
    "        if global_feat.ndim == 1:\n",
    "            global_feat = np.expand_dims(global_feat, axis=0)\n",
    "        \n",
    "        if local_feat.ndim == 3:\n",
    "            local_mean = local_feat.mean(axis=1)  # (T, 2048)\n",
    "        elif local_feat.ndim == 2:\n",
    "            local_mean = local_feat               # already (T, 2048)\n",
    "        elif local_feat.ndim == 1:\n",
    "            local_mean = np.expand_dims(local_feat, axis=0)  # (1, D)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected local_feat shape: {local_feat.shape}\")\n",
    "        \n",
    "        if motion_feat.ndim == 1:\n",
    "            motion_repeat = np.repeat(motion_feat[np.newaxis, :], global_feat.shape[0], axis=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected motion_feat shape: {motion_feat.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "        #Concatenate along feature dimension\n",
    "        feats = np.concatenate([global_feat, local_mean, motion_repeat], axis=1)  # (T, 4608)\n",
    "\n",
    "        #Frame padding/truncation\n",
    "        if feats.shape[0] < self.sample_frames:\n",
    "            pad = np.zeros((self.sample_frames - feats.shape[0], feats.shape[1]), dtype=np.float32)\n",
    "            feats = np.concatenate([feats, pad], axis=0)\n",
    "        else:\n",
    "            feats = feats[:self.sample_frames]\n",
    "\n",
    "        # Caption numericalization + padding\n",
    "        numer = self.vocab.numericalize(cap)\n",
    "        if len(numer) > self.max_caption_len:\n",
    "            numer = numer[:self.max_caption_len-1] + [self.vocab.word2idx[self.vocab.eos_token]]\n",
    "\n",
    "        cap_len = len(numer)\n",
    "        pad_len = self.max_caption_len - cap_len\n",
    "        if pad_len > 0:\n",
    "            numer = numer + [self.vocab.word2idx[self.vocab.pad_token]] * pad_len\n",
    "\n",
    "        return torch.FloatTensor(feats), torch.LongTensor(numer), cap_len\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats = torch.stack([b[0] for b in batch], dim=0)  # (B, T, D)\n",
    "    caps = torch.stack([b[1] for b in batch], dim=0)\n",
    "    cap_lens = torch.LongTensor([b[2] for b in batch])\n",
    "    return feats, caps, cap_lens\n",
    "    \n",
    "items = list(captions_hindi.items())\n",
    "random.seed(42)\n",
    "random.shuffle(items)\n",
    "n = len(items)\n",
    "train_items = dict(items[:int(0.8*n)])\n",
    "val_items = dict(items[int(0.8*n):int(0.9*n)])\n",
    "test_items = dict(items[int(0.9*n):])\n",
    "\n",
    "train_ds = MSRVTTMultiFeatureDataset(train_items, vocab)\n",
    "val_ds   = MSRVTTMultiFeatureDataset(val_items, vocab)\n",
    "test_ds  = MSRVTTMultiFeatureDataset(test_items, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds), \"Test size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BiLSTM Encoder+ LSTM Decoder with Luong attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:42:22.314975Z",
     "iopub.status.busy": "2025-11-23T17:42:22.314249Z",
     "iopub.status.idle": "2025-11-23T17:42:22.328677Z",
     "shell.execute_reply": "2025-11-23T17:42:22.327963Z",
     "shell.execute_reply.started": "2025-11-23T17:42:22.314952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model-1: BiLSTM Encoder+ Decoder with Luong attention\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, num_layers=1, bidirectional=True,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size=feat_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.output_size = hidden_size * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats: (B, T, D)\n",
    "        outputs, (h_n, c_n) = self.rnn(feats)  # outputs: (B, T, hidden*dir)\n",
    "        return outputs, (h_n, c_n)\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_dim, dec_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "    \n",
    "        # project encoder outputs to decoder dim\n",
    "        proj = self.attn(encoder_outputs)  # (B, T, dec_dim)\n",
    "        # up: (B, T)\n",
    "        scores = torch.bmm(proj, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (B, T)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (B, enc_dim)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, embed_size, enc_dim, dec_hidden, vocab_size, num_layers=1,num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = LuongAttention(enc_dim, dec_hidden)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(embed_size + enc_dim, dec_hidden)\n",
    "        self.fc_out = nn.Linear(dec_hidden, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward_step(self, prev_word, last_hidden, last_cell, encoder_outputs):\n",
    "        # prev_word: (B,) token ids\n",
    "        emb = self.embedding(prev_word)  # (B, E)\n",
    "        # use last_hidden as query\n",
    "        context, attn_weights = self.attention(last_hidden, encoder_outputs)  # (B, enc_dim), (B, T)\n",
    "\n",
    "        lstm_input = torch.cat([emb, context], dim=1)\n",
    "        h, c = self.lstm(lstm_input, (last_hidden, last_cell))\n",
    "        output = self.fc_out(self.dropout(h))\n",
    "        return output, h, c, attn_weights\n",
    "\n",
    "    def forward(self, encoder_outputs, captions, teacher_forcing_ratio=0.9):\n",
    "        # encoder_outputs: (B, T, enc_dim)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_len = captions.size(1)\n",
    "        vocab_size = self.fc_out.out_features\n",
    "\n",
    "        # Initialize hidden state and cell to zeros\n",
    "        hidden = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)\n",
    "        cell = torch.zeros(batch_size, self.lstm.hidden_size, device=encoder_outputs.device)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size, device=encoder_outputs.device)\n",
    "        attn_weights_all = []\n",
    "\n",
    "        # first input is <BOS>\n",
    "        input_word = captions[:,0]  # (B,)\n",
    "        for t in range(1, max_len):\n",
    "            out, hidden, cell, attn_weights = self.forward_step(input_word, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = out\n",
    "            attn_weights_all.append(attn_weights)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            input_word = captions[:, t] if teacher_force else top1\n",
    "        # outputs: (B, max_len, vocab)\n",
    "        return outputs, attn_weights_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BiLSTM encoder+Transforme decoder with only self attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T17:26:16.639017Z",
     "iopub.status.busy": "2025-11-21T17:26:16.638698Z",
     "iopub.status.idle": "2025-11-21T17:26:16.651910Z",
     "shell.execute_reply": "2025-11-21T17:26:16.651172Z",
     "shell.execute_reply.started": "2025-11-21T17:26:16.638997Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# ----------------------------\n",
    "# LSTM Encoder\n",
    "# ----------------------------\n",
    "#no of layers=2 #bidirectional=true(1st)- tried with layers 1,2,3,5,4 etc.\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, num_layers=1, bidirectional=True,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size=feat_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.output_size = hidden_size * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats: (B, T, D)\n",
    "        outputs, (h_n, c_n) = self.rnn(feats)  # outputs: (B, T, hidden*dir)\n",
    "        return outputs, (h_n, c_n)\n",
    "\n",
    "# ----------------------------\n",
    "# Positional Encoding\n",
    "# ----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Decoder (No Cross Attention)\n",
    "# ----------------------------\n",
    "class TransformerDecoderNoAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, enc_dim, num_layers=3, ff_dim=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.enc_proj = nn.Linear(enc_dim, embed_dim)  # project LSTM outputs\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "\n",
    "    def forward(self, encoder_outs, captions):\n",
    "        # encoder_outs: (B, T_enc, enc_dim)\n",
    "        # captions: (B, T_dec)\n",
    "        B, T_dec = captions.size()\n",
    "        tgt = self.embedding(captions) * math.sqrt(self.embed_dim)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(T_dec).to(captions.device)\n",
    "        memory = self.enc_proj(encoder_outs)\n",
    "\n",
    "        out = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BiLSTM Encoder+TRANSFORMER( WITH CROSS ATTENTION)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:02:24.759698Z",
     "iopub.status.busy": "2025-11-24T18:02:24.759369Z",
     "iopub.status.idle": "2025-11-24T18:02:24.772054Z",
     "shell.execute_reply": "2025-11-24T18:02:24.771371Z",
     "shell.execute_reply.started": "2025-11-24T18:02:24.759673Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, num_layers=1, bidirectional=True,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size=feat_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.output_size = hidden_size * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats: (B, T, D)\n",
    "        outputs, (h_n, c_n) = self.rnn(feats)  # outputs: (B, T, hidden*dir)\n",
    "        return outputs, (h_n, c_n)\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "class TransformerDecoderWithCrossAttn(nn.Module):\n",
    " \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        enc_dim: int,\n",
    "        num_layers: int = 3,\n",
    "        nhead: int = 8,\n",
    "        ff_dim: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 500\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert embed_dim % nhead == 0, \"embed_dim must be divisible by nhead\"\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # token embedding + positional encoding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout, max_len=max_len)\n",
    "\n",
    "        # project encoder outputs to decoder d_model\n",
    "        self.enc_proj = nn.Linear(enc_dim, embed_dim)\n",
    "\n",
    "        # transformer decoder (uses cross-attention internally)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        \n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # final linear -> vocab\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        # boolean mask where True means masked (no attention)\n",
    "        return torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "\n",
    "    def forward(self, encoder_outs, captions, tgt_mask=None, memory_key_padding_mask=None):\n",
    "       \n",
    "        device = captions.device\n",
    "        B, T_dec = captions.size()\n",
    "\n",
    "        # embed tokens and add positional encodings\n",
    "        tgt = self.embedding(captions) * math.sqrt(self.embed_dim)  # (B, T_dec, embed_dim)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        # project encoder outputs to embed_dim\n",
    "        memory = self.enc_proj(encoder_outs)  # (B, T_enc, embed_dim)\n",
    "\n",
    "        # causal mask if not provided so that mode wont look ahead\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self._generate_square_subsequent_mask(T_dec, device=device)  # bool mask\n",
    "\n",
    "        # Pass memory_key_padding_mask to ignore padded encoder positions (True means pad).\n",
    "        out = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        logits = self.fc_out(out)  # (B, T_dec, vocab_size)\n",
    "        return logits\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1)BILSTM(1 layer+BiLSTM)+decoder(LSTM)\n",
    "enc = EncoderRNN(feat_size=FEATURE_DIM, hidden_size=ENC_HIDDEN, bidirectional=True).to(DEVICE)\n",
    "dec = DecoderWithAttention(embed_size=EMBED_SIZE, enc_dim=enc.output_size, dec_hidden=DEC_HIDDEN, vocab_size=len(vocab.word2idx)).to(DEVICE)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n",
    "\n",
    "def train_one_epoch(train_loader, enc, dec, optimizer, criterion, device, clip=5.0):\n",
    "    enc.train(); dec.train()\n",
    "    running_loss = 0.0\n",
    "    for feats, caps, cap_lens in tqdm(train_loader):\n",
    "        feats = feats.to(device)            # (B, T, D)\n",
    "        caps = caps.to(device)              # (B, L)\n",
    "        optimizer.zero_grad()\n",
    "        encoder_outs, _ = enc(feats)       # (B, T, enc_dim)\n",
    "        outputs, _ = dec(encoder_outs, caps, teacher_forcing_ratio=0.75)  # (B, L, V)\n",
    "        # shift outputs and targets: ignore the first token (<BOS>)\n",
    "        outputs = outputs[:,1:,:].contiguous()\n",
    "        targets = caps[:,1:].contiguous()\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM= 2layers+no bi\n",
    "enc = EncoderRNN(feat_size=FEATURE_DIM, hidden_size=ENC_HIDDEN,num_layers=2, bidirectional=False).to(DEVICE)\n",
    "dec = DecoderWithAttention(embed_size=EMBED_SIZE, enc_dim=enc.output_size, dec_hidden=DEC_HIDDEN, vocab_size=len(vocab.word2idx)).to(DEVICE)\n",
    "#dec = DecoderWithBiLSTM(embed_size=EMBED_SIZE, enc_dim=enc.output_size, dec_hidden=DEC_HIDDEN, vocab_size=len(vocab.word2idx)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n",
    "\n",
    "def train_one_epoch(train_loader, enc, dec, optimizer, criterion, device, clip=5.0):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for (global_feats, motion_feats, caps, cap_lens) in tqdm(train_loader):\n",
    "        # Move tensors to device\n",
    "        global_feats = global_feats.to(device)      # (B, 28, 2048)\n",
    "        motion_feats = motion_feats.to(device)      # (B, 28, 64)\n",
    "        caps = caps.to(device)                      # (B, max_caption_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass both global and motion features to encoder\n",
    "        encoder_outs = enc(global_feats, motion_feats)  # (B, 28, hidden_dim)\n",
    "\n",
    "        # Decoder input: all tokens except last\n",
    "        outputs = dec(encoder_outs, caps[:, :-1])       # (B, L-1, vocab_size)\n",
    "        targets = caps[:, 1:]                           # (B, L-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(optimizer.param_groups[0]['params'], clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM= 2layers+Transformer without cross attention\n",
    "enc = EncoderRNN(feat_size=FEATURE_DIM, hidden_size=ENC_HIDDEN, bidirectional=True).to(DEVICE)\n",
    "\n",
    "dec = TransformerDecoderNoAttn(\n",
    "    vocab_size=len(vocab.word2idx),\n",
    "    embed_dim=512,\n",
    "    enc_dim=enc.output_size, \n",
    "    num_layers=3 # 3,5,4\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n",
    "\n",
    "\n",
    "def train_one_epoch(train_loader, encoder, decoder, optimizer, criterion, device, clip=5.0):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for feats, caps, cap_lens in tqdm(train_loader):\n",
    "        feats = feats.to(device)          # (B, T, D)\n",
    "        caps = caps.to(device)            # (B, L)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ------------------------\n",
    "        # 1. ENCODER FORWARD PASS\n",
    "        # ------------------------\n",
    "        enc_out, _ = encoder(feats)       # (B, T, enc_dim)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 2. TRANSFORMER DECODER ‚Äî NO TEACHER FORCING\n",
    "        # -----------------------------------------------------------\n",
    "        outputs = decoder(enc_out, caps)  # (B, L, V)\n",
    "        \n",
    "        # -----------------------------------------------------------\n",
    "        # 3. SHIFT OUTPUTS/TARGETS FOR LOSS\n",
    "        # -----------------------------------------------------------\n",
    "        outputs = outputs[:, :-1, :].contiguous() \n",
    "        targets = caps[:, 1:].contiguous()      \n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.size(-1)),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate transformer+cross attention-3L\n",
    "enc = EncoderRNN(feat_size=FEATURE_DIM, hidden_size=ENC_HIDDEN,num_layers=2, bidirectional=True).to(DEVICE)\n",
    "dec = TransformerDecoderWithCrossAttn(\n",
    "    vocab_size=len(vocab.word2idx),\n",
    "    embed_dim=512,               \n",
    "    enc_dim=enc.output_size,       \n",
    "    num_layers=3,\n",
    "    nhead=8,\n",
    "    ff_dim=2048,\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:02:50.934410Z",
     "iopub.status.busy": "2025-11-24T18:02:50.933832Z",
     "iopub.status.idle": "2025-11-24T18:02:55.795446Z",
     "shell.execute_reply": "2025-11-24T18:02:55.794521Z",
     "shell.execute_reply.started": "2025-11-24T18:02:50.934385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:02:58.611759Z",
     "iopub.status.busy": "2025-11-24T18:02:58.610935Z",
     "iopub.status.idle": "2025-11-24T18:03:03.784068Z",
     "shell.execute_reply": "2025-11-24T18:03:03.783259Z",
     "shell.execute_reply.started": "2025-11-24T18:02:58.611724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocoevalcap\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools>=2.0.2->pycocoevalcap) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (2024.2.0)\n",
      "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
      "Successfully installed pycocoevalcap-1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T10:09:15.114148Z",
     "iopub.status.busy": "2025-11-21T10:09:15.113451Z",
     "iopub.status.idle": "2025-11-21T10:09:15.124890Z",
     "shell.execute_reply": "2025-11-21T10:09:15.124183Z",
     "shell.execute_reply.started": "2025-11-21T10:09:15.114123Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def evaluate_with_metrics(loader, enc, dec, vocab, device, DEC_HIDDEN=512):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    all_refs, all_preds = [], []\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    cider_scorer = Cider()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, cap_lens in tqdm(loader, desc=\"Evaluating\"):\n",
    "            feats, caps = feats.to(device), caps.to(device)\n",
    "            B = feats.size(0)\n",
    "            \n",
    "            # Encoder forward\n",
    "            encoder_outs, _ = enc(feats)\n",
    "\n",
    "            # Initialize hidden and cell\n",
    "            hidden = torch.zeros(B, DEC_HIDDEN, device=device)\n",
    "            cell = torch.zeros(B, DEC_HIDDEN, device=device)\n",
    "            \n",
    "            # Start with <BOS> token\n",
    "            input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]] * B).to(device)\n",
    "\n",
    "            preds = [[] for _ in range(B)]\n",
    "\n",
    "            # Greedy decoding\n",
    "            max_len = caps.size(1)\n",
    "            for t in range(1, max_len):\n",
    "                out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                    input_word, hidden, cell, encoder_outs\n",
    "                )\n",
    "                top1 = out.argmax(1)\n",
    "                input_word = top1\n",
    "                for i in range(B):\n",
    "                    preds[i].append(top1[i].item())\n",
    "\n",
    "            # Convert predicted tokens ‚Üí words\n",
    "            for i in range(B):\n",
    "                pred_tokens = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    pred_tokens.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(pred_tokens)\n",
    "\n",
    "                # Reference captions\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "\n",
    "    # -----------------------\n",
    "    # Compute Metrics\n",
    "    # -----------------------\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "    refs_str = [' '.join(ref[0]) for ref in all_refs]\n",
    "    preds_str = [' '.join(pred) for pred in all_preds]\n",
    "\n",
    "    rouge_scores = rouge.get_scores(preds_str, refs_str, avg=True)\n",
    "    cider_score, _ = cider_scorer.compute_score(\n",
    "        {i: [refs_str[i]] for i in range(len(refs_str))},\n",
    "        {i: [preds_str[i]] for i in range(len(preds_str))}\n",
    "    )\n",
    "\n",
    "    return bleu4, rouge_scores, cider_score, all_refs, all_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Evaluation of the models for English Caption genearttion-3 features,BiLSTM Encoder+diffrent Decoders (1290 videos)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T14:32:59.602658Z",
     "iopub.status.busy": "2025-11-13T14:32:59.602035Z",
     "iopub.status.idle": "2025-11-13T15:06:38.070566Z",
     "shell.execute_reply": "2025-11-13T15:06:38.069710Z",
     "shell.execute_reply.started": "2025-11-13T14:32:59.602635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 6.2559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0083\n",
      "  ROUGE-L (F1) = 0.2546\n",
      "  CIDEr = 0.1327\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:33<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 5.7394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0372\n",
      "  ROUGE-L (F1) = 0.2660\n",
      "  CIDEr = 0.3092\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 5.5613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0388\n",
      "  ROUGE-L (F1) = 0.2734\n",
      "  CIDEr = 0.3083\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 5.2977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0490\n",
      "  ROUGE-L (F1) = 0.2930\n",
      "  CIDEr = 0.4000\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 5.1862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0507\n",
      "  ROUGE-L (F1) = 0.2934\n",
      "  CIDEr = 0.4452\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 5.0884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0489\n",
      "  ROUGE-L (F1) = 0.2898\n",
      "  CIDEr = 0.4250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 5.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0474\n",
      "  ROUGE-L (F1) = 0.2882\n",
      "  CIDEr = 0.4158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 4.9314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0474\n",
      "  ROUGE-L (F1) = 0.2923\n",
      "  CIDEr = 0.4471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 4.8549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0505\n",
      "  ROUGE-L (F1) = 0.2932\n",
      "  CIDEr = 0.4795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 4.7734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Metrics:\n",
      "  BLEU-4 = 0.0517\n",
      "  ROUGE-L (F1) = 0.2945\n",
      "  CIDEr = 0.4853\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 4.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Metrics:\n",
      "  BLEU-4 = 0.0503\n",
      "  ROUGE-L (F1) = 0.3000\n",
      "  CIDEr = 0.4954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 4.6672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Validation Metrics:\n",
      "  BLEU-4 = 0.0530\n",
      "  ROUGE-L (F1) = 0.2990\n",
      "  CIDEr = 0.5095\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 4.6145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Validation Metrics:\n",
      "  BLEU-4 = 0.0497\n",
      "  ROUGE-L (F1) = 0.2983\n",
      "  CIDEr = 0.4878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 4.5430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Validation Metrics:\n",
      "  BLEU-4 = 0.0501\n",
      "  ROUGE-L (F1) = 0.2949\n",
      "  CIDEr = 0.4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:33<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 4.4977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Validation Metrics:\n",
      "  BLEU-4 = 0.0514\n",
      "  ROUGE-L (F1) = 0.2981\n",
      "  CIDEr = 0.5013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 4.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Validation Metrics:\n",
      "  BLEU-4 = 0.0470\n",
      "  ROUGE-L (F1) = 0.2928\n",
      "  CIDEr = 0.4526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 4.4114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Validation Metrics:\n",
      "  BLEU-4 = 0.0499\n",
      "  ROUGE-L (F1) = 0.2939\n",
      "  CIDEr = 0.4650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:33<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 4.3572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Validation Metrics:\n",
      "  BLEU-4 = 0.0524\n",
      "  ROUGE-L (F1) = 0.2956\n",
      "  CIDEr = 0.4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:34<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 4.3321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Validation Metrics:\n",
      "  BLEU-4 = 0.0510\n",
      "  ROUGE-L (F1) = 0.2976\n",
      "  CIDEr = 0.4795\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0530\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    " #Training Loop- BiLSTM ENC+LSTM DEC+LOUNG ATTENTION\n",
    "# -------------------------\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE, DEC_HIDDEN)\n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']  # Extract F1-score for ROUGE-L\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    # ---- Save best model ----\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_1290_3f_eng.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:19:32.669743Z",
     "iopub.status.busy": "2025-11-13T15:19:32.669048Z",
     "iopub.status.idle": "2025-11-13T15:19:39.820141Z",
     "shell.execute_reply": "2025-11-13T15:19:39.819345Z",
     "shell.execute_reply.started": "2025-11-13T15:19:32.669710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0515\n",
      "ROUGE-L = 0.2851\n",
      "CIDEr   = 0.4840\n",
      "\n",
      "üé¨ Video 1: video864.mp4\n",
      "üìù Generated Caption: a car is being shown\n",
      "üìñ Reference Captions:\n",
      "  Ref 1: a 1997 acura integra for sale\n",
      "  Ref 2: a 1997 acura is displayed for sale\n",
      "  Ref 3: a 1997 acura is parked and for sale at midtown motors\n",
      "\n",
      "üé¨ Video 2: video440.mp4\n",
      "üìù Generated Caption: a woman is a a bag\n",
      "üìñ Reference Captions:\n",
      "  Ref 1: a brown haired girl is twisting her hair and talking about keeping it tight\n",
      "  Ref 2: a woman with black hair is combing her hair\n",
      "  Ref 3: there is a woman is combing her hair beautifully\n",
      "\n",
      "üé¨ Video 3: video206.mp4\n",
      "üìù Generated Caption: a man is talking about a\n",
      "üìñ Reference Captions:\n",
      "  Ref 1: a guy playing a guitar\n",
      "  Ref 2: a guy singing a song in a airport setting\n",
      "  Ref 3: a male vocalist performs a ballad\n",
      "\n",
      "üé¨ Video 4: video810.mp4\n",
      "üìù Generated Caption: a man is swimming in a blue\n",
      "üìñ Reference Captions:\n",
      "  Ref 1: a diver swims in the ocean with fish and a manta ray while filming the ray\n",
      "  Ref 2: a manta ray is filmed by a photographer underwater as other fish swim by\n",
      "  Ref 3: a person is scuba diving with different species of sharks and fish\n",
      "\n",
      "üé¨ Video 5: video114.mp4\n",
      "üìù Generated Caption: a group of people are dancing\n",
      "üìñ Reference Captions:\n",
      "  Ref 1: a girl is demonstrating a dance move with a group of people on a beach\n",
      "  Ref 2: a girl teaches people how to dance on a beach\n",
      "  Ref 3: a group of men and women sings and dances at sea shore\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#Load the best saved checkpoint\n",
    "\n",
    "ckpt = torch.load(\"/kaggle/working/best_checkpoint_1290_3f_eng.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "#Evaluate on test set\n",
    "\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # Avoid warning about tensor creation\n",
    "    if isinstance(video_feat, torch.Tensor):\n",
    "        feat_tensor = video_feat.clone().detach().float().to(device)\n",
    "    else:\n",
    "        feat_tensor = torch.tensor(video_feat, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Ensure correct shape (B, T, D)\n",
    "    if feat_tensor.dim() == 2:\n",
    "        feat_tensor = feat_tensor.unsqueeze(0)   # (1, T, D)\n",
    "    elif feat_tensor.dim() == 4:\n",
    "        feat_tensor = feat_tensor.squeeze(0)     # (B, T, D)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outs, _ = enc(feat_tensor)\n",
    "\n",
    "        # hidden, cell initialized same as training\n",
    "        hidden = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "           \n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word,\n",
    "                hidden.squeeze(0),  # (B, hidden_size)\n",
    "                cell.squeeze(0),    # (B, hidden_size)\n",
    "                encoder_outs\n",
    "            )\n",
    "\n",
    "            # restore shape for next timestep\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            cell = cell.unsqueeze(0)\n",
    "\n",
    "            next_word = out.argmax(1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "\n",
    "#  Pick a few random test samples\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(9)\n",
    "\n",
    "#  random 5 videos test_items\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "\n",
    " \n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)  # (1, T, D)\n",
    "\n",
    "\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    references = test_items[vid]  # list of ground-truth captions\n",
    "\n",
    "    print(f\"\\nüé¨ Video {i+1}: {vid}\")\n",
    "    print(f\"üìù Generated Caption: {generated_caption}\")\n",
    "    print(\"üìñ Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T09:55:00.526650Z",
     "iopub.status.busy": "2025-11-22T09:55:00.526364Z",
     "iopub.status.idle": "2025-11-22T09:55:00.533076Z",
     "shell.execute_reply": "2025-11-22T09:55:00.532319Z",
     "shell.execute_reply.started": "2025-11-22T09:55:00.526629Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define feature folders\n",
    "FEATURES_GLOBAL_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_global/features_global\"\n",
    "FEATURES_LOCAL_DIR  = \"/kaggle/input/msr-vtt-1289-hindi-english/features_local/features_local\"\n",
    "FEATURES_MOTION_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_motion/features_motion\"\n",
    "\n",
    "def load_combined_features(vid, sample_frames=16):\n",
    "    \"\"\"\n",
    "    Load and combine global, local, and motion features for a given video.\n",
    "    Returns (T, 4608)\n",
    "    \"\"\"\n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "\n",
    "    global_path = os.path.join(FEATURES_GLOBAL_DIR, vid_id)\n",
    "    local_path  = os.path.join(FEATURES_LOCAL_DIR, vid_id)\n",
    "    motion_path = os.path.join(FEATURES_MOTION_DIR, vid_id)\n",
    "\n",
    "    # Load features\n",
    "    global_feat = np.load(global_path)    # (T, 2048)\n",
    "    local_feat  = np.load(local_path)     # (T, 49, 2048)\n",
    "    motion_feat = np.load(motion_path)    # (512,)\n",
    "\n",
    "    # Mean-pool local features\n",
    "    local_mean = local_feat.mean(axis=1)  # (T, 2048)\n",
    "\n",
    "    # Repeat motion features per frame\n",
    "    motion_repeat = np.repeat(motion_feat[np.newaxis, :], sample_frames, axis=0)  # (T, 512)\n",
    "\n",
    "    # Concatenate all\n",
    "    feats = np.concatenate([global_feat, local_mean, motion_repeat], axis=1)  # (T, 4608)\n",
    "\n",
    "    # Pad/truncate frames\n",
    "    if feats.shape[0] < sample_frames:\n",
    "        pad = np.zeros((sample_frames - feats.shape[0], feats.shape[1]), dtype=np.float32)\n",
    "        feats = np.concatenate([feats, pad], axis=0)\n",
    "    else:\n",
    "        feats = feats[:sample_frames]\n",
    "\n",
    "    return feats.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention based Fusion of features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to load the dataset for attention based fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T06:46:58.056501Z",
     "iopub.status.busy": "2025-11-15T06:46:58.056222Z",
     "iopub.status.idle": "2025-11-15T06:46:58.078125Z",
     "shell.execute_reply": "2025-11-15T06:46:58.077359Z",
     "shell.execute_reply.started": "2025-11-15T06:46:58.056479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20640 Val size: 2580 Test size: 2580\n"
     ]
    }
   ],
   "source": [
    "#Code to load the dataset for attention based fusion\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "FEATURES_GLOBAL_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_global/features_global\"\n",
    "FEATURES_LOCAL_DIR  = \"/kaggle/input/msr-vtt-1289-hindi-english/features_local/features_local\"\n",
    "FEATURES_MOTION_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_motion/features_motion\"\n",
    "\n",
    "class MSRVTTMultiFeatureDataset(Dataset):\n",
    "    def __init__(self, captions_dict, vocab, sample_frames=16, max_caption_len=30):\n",
    "        self.items = [(vid, c) for vid, caps in captions_dict.items() for c in caps]\n",
    "        self.vocab = vocab\n",
    "        self.sample_frames = sample_frames\n",
    "        self.max_caption_len = max_caption_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, cap = self.items[idx]\n",
    "        vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "\n",
    "        # Load features\n",
    "        global_feat = np.load(os.path.join(FEATURES_GLOBAL_DIR, vid_id))  # (T,2048)\n",
    "        local_feat  = np.load(os.path.join(FEATURES_LOCAL_DIR,  vid_id))  # (T,49,2048)\n",
    "        motion_feat = np.load(os.path.join(FEATURES_MOTION_DIR, vid_id))  # (512,)\n",
    "\n",
    "        # ---------- Fix shapes ----------\n",
    "        if global_feat.ndim == 1:\n",
    "            global_feat = global_feat[None, :]\n",
    "\n",
    "        if local_feat.ndim == 3:\n",
    "            local_mean = local_feat.mean(axis=1)              # (T,2048)\n",
    "        elif local_feat.ndim == 2:\n",
    "            local_mean = local_feat                            # (T,2048)\n",
    "        elif local_feat.ndim == 1:\n",
    "            local_mean = local_feat[None, :]                   # (1,2048)\n",
    "        else:\n",
    "            raise ValueError(f\"Bad local_feat shape: {local_feat.shape}\")\n",
    "\n",
    "        if motion_feat.ndim == 1:\n",
    "            motion_repeat = np.repeat(motion_feat[None, :], global_feat.shape[0], axis=0)  # (T,512)\n",
    "        else:\n",
    "            raise ValueError(f\"Bad motion_feat shape: {motion_feat.shape}\")\n",
    "\n",
    "        # ---------- Pad or truncate all three ----------\n",
    "        T = self.sample_frames\n",
    "\n",
    "        def pad_to_T(x):\n",
    "            if x.shape[0] < T:\n",
    "                pad = np.zeros((T - x.shape[0], x.shape[1]), dtype=np.float32)\n",
    "                return np.concatenate([x, pad], axis=0)\n",
    "            else:\n",
    "                return x[:T]\n",
    "\n",
    "        global_feat = pad_to_T(global_feat).astype(np.float32)      # (T,2048)\n",
    "        local_mean  = pad_to_T(local_mean).astype(np.float32)       # (T,2048)\n",
    "        motion_repeat = pad_to_T(motion_repeat).astype(np.float32)  # (T,512)\n",
    "\n",
    "        # ---------- Caption numericalization ----------\n",
    "        numer = self.vocab.numericalize(cap)\n",
    "        if len(numer) > self.max_caption_len:\n",
    "            numer = numer[:self.max_caption_len-1] + \\\n",
    "                    [self.vocab.word2idx[self.vocab.eos_token]]\n",
    "\n",
    "        cap_len = len(numer)\n",
    "        pad_len = self.max_caption_len - cap_len\n",
    "        if pad_len > 0:\n",
    "            numer = numer + [self.vocab.word2idx[self.vocab.pad_token]] * pad_len\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(global_feat),     # (T,2048)\n",
    "            torch.FloatTensor(local_mean),      # (T,2048)\n",
    "            torch.FloatTensor(motion_repeat),   # (T,512)\n",
    "            torch.LongTensor(numer),            # (L)\n",
    "            cap_len\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    global_f = torch.stack([b[0] for b in batch], dim=0)\n",
    "    local_f  = torch.stack([b[1] for b in batch], dim=0)\n",
    "    motion_f = torch.stack([b[2] for b in batch], dim=0)\n",
    "    caps     = torch.stack([b[3] for b in batch], dim=0)\n",
    "    cap_lens = torch.LongTensor([b[4] for b in batch])\n",
    "    return global_f, local_f, motion_f, caps, cap_lens\n",
    "    \n",
    "items = list(captions_filtered.items())\n",
    "random.seed(42)\n",
    "random.shuffle(items)\n",
    "n = len(items)\n",
    "\n",
    "train_items = dict(items[:int(0.8*n)])\n",
    "val_items   = dict(items[int(0.8*n):int(0.9*n)])\n",
    "test_items  = dict(items[int(0.9*n):])\n",
    "\n",
    "train_ds = MSRVTTMultiFeatureDataset(train_items, vocab)\n",
    "val_ds   = MSRVTTMultiFeatureDataset(val_items, vocab)\n",
    "test_ds  = MSRVTTMultiFeatureDataset(test_items, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds), \"Test size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:16:46.526265Z",
     "iopub.status.busy": "2025-11-23T17:16:46.525575Z",
     "iopub.status.idle": "2025-11-23T17:16:46.549994Z",
     "shell.execute_reply": "2025-11-23T17:16:46.549112Z",
     "shell.execute_reply.started": "2025-11-23T17:16:46.526233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20640 Val size: 2580 Test size: 2580\n"
     ]
    }
   ],
   "source": [
    "#for hindi\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "FEATURES_GLOBAL_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_global/features_global\"\n",
    "FEATURES_LOCAL_DIR  = \"/kaggle/input/msr-vtt-1289-hindi-english/features_local/features_local\"\n",
    "FEATURES_MOTION_DIR = \"/kaggle/input/msr-vtt-1289-hindi-english/features_motion/features_motion\"\n",
    "\n",
    "class MSRVTTMultiFeatureDataset(Dataset):\n",
    "    def __init__(self, captions_dict, vocab, sample_frames=16, max_caption_len=30):\n",
    "        self.items = [(vid, c) for vid, caps in captions_dict.items() for c in caps]\n",
    "        self.vocab = vocab\n",
    "        self.sample_frames = sample_frames\n",
    "        self.max_caption_len = max_caption_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, cap = self.items[idx]\n",
    "        vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "\n",
    "        # Load features\n",
    "        global_feat = np.load(os.path.join(FEATURES_GLOBAL_DIR, vid_id))  # (T,2048)\n",
    "        local_feat  = np.load(os.path.join(FEATURES_LOCAL_DIR,  vid_id))  # (T,49,2048)\n",
    "        motion_feat = np.load(os.path.join(FEATURES_MOTION_DIR, vid_id))  # (512,)\n",
    "\n",
    "        # ---------- Fix shapes ----------\n",
    "        if global_feat.ndim == 1:\n",
    "            global_feat = global_feat[None, :]\n",
    "\n",
    "        if local_feat.ndim == 3:\n",
    "            local_mean = local_feat.mean(axis=1)              # (T,2048)\n",
    "        elif local_feat.ndim == 2:\n",
    "            local_mean = local_feat                            # (T,2048)\n",
    "        elif local_feat.ndim == 1:\n",
    "            local_mean = local_feat[None, :]                   # (1,2048)\n",
    "        else:\n",
    "            raise ValueError(f\"Bad local_feat shape: {local_feat.shape}\")\n",
    "\n",
    "        if motion_feat.ndim == 1:\n",
    "            motion_repeat = np.repeat(motion_feat[None, :], global_feat.shape[0], axis=0)  # (T,512)\n",
    "        else:\n",
    "            raise ValueError(f\"Bad motion_feat shape: {motion_feat.shape}\")\n",
    "\n",
    "        # ---------- Pad or truncate all three ----------\n",
    "        T = self.sample_frames\n",
    "\n",
    "        def pad_to_T(x):\n",
    "            if x.shape[0] < T:\n",
    "                pad = np.zeros((T - x.shape[0], x.shape[1]), dtype=np.float32)\n",
    "                return np.concatenate([x, pad], axis=0)\n",
    "            else:\n",
    "                return x[:T]\n",
    "\n",
    "        global_feat = pad_to_T(global_feat).astype(np.float32)      # (T,2048)\n",
    "        local_mean  = pad_to_T(local_mean).astype(np.float32)       # (T,2048)\n",
    "        motion_repeat = pad_to_T(motion_repeat).astype(np.float32)  # (T,512)\n",
    "\n",
    "        # ---------- Caption numericalization ----------\n",
    "        numer = self.vocab.numericalize(cap)\n",
    "        if len(numer) > self.max_caption_len:\n",
    "            numer = numer[:self.max_caption_len-1] + \\\n",
    "                    [self.vocab.word2idx[self.vocab.eos_token]]\n",
    "\n",
    "        cap_len = len(numer)\n",
    "        pad_len = self.max_caption_len - cap_len\n",
    "        if pad_len > 0:\n",
    "            numer = numer + [self.vocab.word2idx[self.vocab.pad_token]] * pad_len\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(global_feat),     # (T,2048)\n",
    "            torch.FloatTensor(local_mean),      # (T,2048)\n",
    "            torch.FloatTensor(motion_repeat),   # (T,512)\n",
    "            torch.LongTensor(numer),            # (L)\n",
    "            cap_len\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    global_f = torch.stack([b[0] for b in batch], dim=0)\n",
    "    local_f  = torch.stack([b[1] for b in batch], dim=0)\n",
    "    motion_f = torch.stack([b[2] for b in batch], dim=0)\n",
    "    caps     = torch.stack([b[3] for b in batch], dim=0)\n",
    "    cap_lens = torch.LongTensor([b[4] for b in batch])\n",
    "    return global_f, local_f, motion_f, caps, cap_lens\n",
    "    \n",
    "items = list(captions_hindi.items())\n",
    "random.seed(42)\n",
    "random.shuffle(items)\n",
    "n = len(items)\n",
    "\n",
    "train_items = dict(items[:int(0.8*n)])\n",
    "val_items   = dict(items[int(0.8*n):int(0.9*n)])\n",
    "test_items  = dict(items[int(0.9*n):])\n",
    "\n",
    "train_ds = MSRVTTMultiFeatureDataset(train_items, vocab)\n",
    "val_ds   = MSRVTTMultiFeatureDataset(val_items, vocab)\n",
    "test_ds  = MSRVTTMultiFeatureDataset(test_items, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds), \"Test size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:54:15.259142Z",
     "iopub.status.busy": "2025-11-22T12:54:15.258579Z",
     "iopub.status.idle": "2025-11-22T12:54:15.265180Z",
     "shell.execute_reply": "2025-11-22T12:54:15.264332Z",
     "shell.execute_reply.started": "2025-11-22T12:54:15.259120Z"
    }
   },
   "outputs": [],
   "source": [
    "#Additive (Bahdanau) attention for multimodal fusion\n",
    "class FusionAttention(nn.Module):\n",
    "    def __init__(self, dim_g, dim_l, dim_m, fused_dim):\n",
    "        super().__init__()\n",
    "        self.Wg = nn.Linear(dim_g, fused_dim)\n",
    "        self.Wl = nn.Linear(dim_l, fused_dim)\n",
    "        self.Wm = nn.Linear(dim_m, fused_dim)\n",
    "        self.v  = nn.Linear(fused_dim, 1)\n",
    "\n",
    "    def forward(self, g, l, m):\n",
    "        # g, l, m = (B, T, D?)\n",
    "\n",
    "        # Project inputs to fused_dim\n",
    "        g_proj = self.Wg(g)  # (B, T, fused_dim)\n",
    "        l_proj = self.Wl(l)  # (B, T, fused_dim)\n",
    "        m_proj = self.Wm(m)  # (B, T, fused_dim)\n",
    "\n",
    "        # Compute attention scores on projected inputs\n",
    "        score_g = self.v(torch.tanh(g_proj))  # (B, T, 1)\n",
    "        score_l = self.v(torch.tanh(l_proj))  # (B, T, 1)\n",
    "        score_m = self.v(torch.tanh(m_proj))  # (B, T, 1)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        Œ±g = torch.softmax(score_g, dim=1)  # (B, T, 1)\n",
    "        Œ±l = torch.softmax(score_l, dim=1)\n",
    "        Œ±m = torch.softmax(score_m, dim=1)\n",
    "\n",
    "        # Fuse features with attention weights applied to projected features\n",
    "        fused = Œ±g * g_proj + Œ±l * l_proj + Œ±m * m_proj  # (B, T, fused_dim)\n",
    "\n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:02:05.472864Z",
     "iopub.status.busy": "2025-11-24T18:02:05.472314Z",
     "iopub.status.idle": "2025-11-24T18:02:05.567351Z",
     "shell.execute_reply": "2025-11-24T18:02:05.566506Z",
     "shell.execute_reply.started": "2025-11-24T18:02:05.472835Z"
    }
   },
   "outputs": [],
   "source": [
    "#Parameters used for Transformer based models+ multimodal attention fusion\n",
    "BATCH_SIZE = 32\n",
    "GLOBAL_DIM = 2048\n",
    "LOCAL_DIM = 2048\n",
    "MOTION_DIM = 512\n",
    "FEATURE_DIM = 4608    # global + local + motion\n",
    "\n",
    "FUSION_HIDDEN = 512\n",
    "NUM_HEADS = 8\n",
    "\n",
    "ENC_HIDDEN = 512\n",
    "ENC_LAYERS = 1\n",
    "ENC_BIDIRECTIONAL = True\n",
    "\n",
    "EMBED_SIZE = 300\n",
    "DEC_HIDDEN = 512\n",
    "\n",
    "LR = 1e-4\n",
    "EPOCHS = 15\n",
    "CLIP = 5.0\n",
    "TEACHER_FORCING = 0.75\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "SAMPLE_FRAMES = 16\n",
    "MAX_CAPTION_LEN = 30\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:55:26.681190Z",
     "iopub.status.busy": "2025-11-22T12:55:26.680866Z",
     "iopub.status.idle": "2025-11-22T12:55:26.807147Z",
     "shell.execute_reply": "2025-11-22T12:55:26.806511Z",
     "shell.execute_reply.started": "2025-11-22T12:55:26.681169Z"
    }
   },
   "outputs": [],
   "source": [
    "#initiation of encoder,decoder and fusion module\n",
    "fusion_module = FusionAttention(\n",
    "    dim_g=2048,     \n",
    "    dim_l=2048,     \n",
    "    dim_m=512,   \n",
    "    fused_dim=512   \n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "enc = EncoderRNN(\n",
    "    feat_size=512, \n",
    "    hidden_size=ENC_HIDDEN, \n",
    "    bidirectional=True\n",
    ").to(DEVICE)\n",
    "\n",
    "dec = DecoderWithAttention(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    enc_dim=enc.output_size,\n",
    "    dec_hidden=DEC_HIDDEN,\n",
    "    vocab_size=len(vocab.word2idx)\n",
    ").to(DEVICE)\n",
    "\n",
    "def train_one_epoch(train_loader, fusion_module, enc, dec, optimizer, criterion, device, clip=5.0):\n",
    "    enc.train(); dec.train(); fusion_module.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for global_f, local_f, motion_f, caps, cap_lens in tqdm(train_loader):\n",
    "        global_f = global_f.to(device)   # (B, T, 2048)\n",
    "        local_f  = local_f.to(device)    # (B, T, 2048)\n",
    "        motion_f = motion_f.to(device)   # (B, T, 512)\n",
    "        caps     = caps.to(device)       # (B, L)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "       \n",
    "        fused_feats = fusion_module(global_f, local_f, motion_f)   # (B, T, F)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_outs, _ = enc(fused_feats)\n",
    "\n",
    "        # Decoder\n",
    "        outputs, _ = dec(encoder_outs, caps, teacher_forcing_ratio=0.75)\n",
    "\n",
    "      \n",
    "        outputs = outputs[:, 1:, :].contiguous()\n",
    "        targets = caps[:, 1:].contiguous()\n",
    "\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(list(enc.parameters()) + \n",
    "                                       list(dec.parameters()) + \n",
    "                                       list(fusion_module.parameters()), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "    \n",
    "params = list(fusion_module.parameters()) + list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=vocab.word2idx[vocab.pad_token],label_smoothing=LABEL_SMOOTHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:56:23.748095Z",
     "iopub.status.busy": "2025-11-22T12:56:23.747736Z",
     "iopub.status.idle": "2025-11-22T12:56:23.760584Z",
     "shell.execute_reply": "2025-11-22T12:56:23.759951Z",
     "shell.execute_reply.started": "2025-11-22T12:56:23.748067Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "def evaluate_with_metrics(loader, fusion_module, enc, dec, vocab, device, DEC_HIDDEN=512):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    fusion_module.eval()\n",
    "\n",
    "    all_refs, all_preds = [], []\n",
    "\n",
    "    rouge = Rouge()\n",
    "    cider_scorer = Cider()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for global_f, local_f, motion_f, caps, cap_lens in tqdm(loader, desc=\"Evaluating\"):\n",
    "            \n",
    "            global_f = global_f.to(device)    # (B, T, 2048)\n",
    "            local_f  = local_f.to(device)     # (B, T, 2048)\n",
    "            motion_f = motion_f.to(device)    # (B, T, 512)\n",
    "            caps     = caps.to(device)        # (B, L)\n",
    "\n",
    "            B = global_f.size(0)\n",
    "\n",
    "           # Apply multimodal fusion\n",
    "          \n",
    "            fused_feats = fusion_module(global_f, local_f, motion_f)  # (B, T, F)\n",
    "\n",
    "         \n",
    "            # Encoder Forward\n",
    "            encoder_outs, _ = enc(fused_feats)\n",
    "\n",
    "            # Initialize hidden & cell\n",
    "            hidden = torch.zeros(B, DEC_HIDDEN, device=device)\n",
    "            cell   = torch.zeros(B, DEC_HIDDEN, device=device)\n",
    "\n",
    "            # Start token <BOS>\n",
    "            input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]] * B).to(device)\n",
    "\n",
    "            preds = [[] for _ in range(B)]\n",
    "            max_len = caps.size(1)\n",
    "\n",
    "           \n",
    "            # Greedy Decoding\n",
    "            for t in range(1, max_len):\n",
    "                out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                    input_word, hidden, cell, encoder_outs\n",
    "                )\n",
    "                top1 = out.argmax(1)\n",
    "                input_word = top1\n",
    "\n",
    "                for i in range(B):\n",
    "                    preds[i].append(top1[i].item())\n",
    "\n",
    "   \n",
    "            # Convert Predictions to Words\n",
    "          \n",
    "            for i in range(B):\n",
    "                pred_tokens = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    pred_tokens.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(pred_tokens)\n",
    "\n",
    "                # Reference caption\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token], vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "    refs_str = [' '.join(ref[0]) for ref in all_refs]\n",
    "    preds_str = [' '.join(pred) for pred in all_preds]\n",
    "\n",
    "    rouge_scores = rouge.get_scores(preds_str, refs_str, avg=True)\n",
    "\n",
    "    cider_score, _ = cider_scorer.compute_score(\n",
    "        {i: [refs_str[i]] for i in range(len(refs_str))},\n",
    "        {i: [preds_str[i]] for i in range(len(preds_str))}\n",
    "    )\n",
    "\n",
    "    return bleu4, rouge_scores, cider_score, all_refs, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T06:48:49.138505Z",
     "iopub.status.busy": "2025-11-15T06:48:49.138003Z",
     "iopub.status.idle": "2025-11-15T07:33:42.159565Z",
     "shell.execute_reply": "2025-11-15T07:33:42.158803Z",
     "shell.execute_reply.started": "2025-11-15T06:48:49.138479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:31<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 6.3394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:07<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0083\n",
      "  ROUGE-L (F1) = 0.2546\n",
      "  CIDEr = 0.1327\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 5.8232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0191\n",
      "  ROUGE-L (F1) = 0.2472\n",
      "  CIDEr = 0.1494\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 5.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0378\n",
      "  ROUGE-L (F1) = 0.2603\n",
      "  CIDEr = 0.2944\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 5.5453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0420\n",
      "  ROUGE-L (F1) = 0.2754\n",
      "  CIDEr = 0.3604\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 5.4196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0407\n",
      "  ROUGE-L (F1) = 0.2779\n",
      "  CIDEr = 0.3811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 5.3037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0418\n",
      "  ROUGE-L (F1) = 0.2762\n",
      "  CIDEr = 0.3729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 5.2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0415\n",
      "  ROUGE-L (F1) = 0.2825\n",
      "  CIDEr = 0.4011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 5.1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0404\n",
      "  ROUGE-L (F1) = 0.2838\n",
      "  CIDEr = 0.3952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 5.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0442\n",
      "  ROUGE-L (F1) = 0.2837\n",
      "  CIDEr = 0.4052\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 4.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0435\n",
      "  ROUGE-L (F1) = 0.2766\n",
      "  CIDEr = 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 4.8847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Metrics:\n",
      "  BLEU-4 = 0.0464\n",
      "  ROUGE-L (F1) = 0.2843\n",
      "  CIDEr = 0.4227\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 4.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Metrics:\n",
      "  BLEU-4 = 0.0452\n",
      "  ROUGE-L (F1) = 0.2864\n",
      "  CIDEr = 0.4217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 4.7715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Validation Metrics:\n",
      "  BLEU-4 = 0.0442\n",
      "  ROUGE-L (F1) = 0.2815\n",
      "  CIDEr = 0.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:22<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 4.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Validation Metrics:\n",
      "  BLEU-4 = 0.0447\n",
      "  ROUGE-L (F1) = 0.2820\n",
      "  CIDEr = 0.4205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 4.6506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Validation Metrics:\n",
      "  BLEU-4 = 0.0474\n",
      "  ROUGE-L (F1) = 0.2827\n",
      "  CIDEr = 0.4400\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:22<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 4.6044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Validation Metrics:\n",
      "  BLEU-4 = 0.0445\n",
      "  ROUGE-L (F1) = 0.2854\n",
      "  CIDEr = 0.4098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 4.5527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Validation Metrics:\n",
      "  BLEU-4 = 0.0479\n",
      "  ROUGE-L (F1) = 0.2847\n",
      "  CIDEr = 0.4545\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 4.5132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Validation Metrics:\n",
      "  BLEU-4 = 0.0420\n",
      "  ROUGE-L (F1) = 0.2740\n",
      "  CIDEr = 0.4017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 4.4646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Validation Metrics:\n",
      "  BLEU-4 = 0.0473\n",
      "  ROUGE-L (F1) = 0.2816\n",
      "  CIDEr = 0.4287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 4.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Validation Metrics:\n",
      "  BLEU-4 = 0.0451\n",
      "  ROUGE-L (F1) = 0.2811\n",
      "  CIDEr = 0.4197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 4.3896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Validation Metrics:\n",
      "  BLEU-4 = 0.0466\n",
      "  ROUGE-L (F1) = 0.2823\n",
      "  CIDEr = 0.4232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 4.3379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Validation Metrics:\n",
      "  BLEU-4 = 0.0457\n",
      "  ROUGE-L (F1) = 0.2811\n",
      "  CIDEr = 0.4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 4.3195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Validation Metrics:\n",
      "  BLEU-4 = 0.0432\n",
      "  ROUGE-L (F1) = 0.2743\n",
      "  CIDEr = 0.4028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 4.2839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Validation Metrics:\n",
      "  BLEU-4 = 0.0438\n",
      "  ROUGE-L (F1) = 0.2784\n",
      "  CIDEr = 0.4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 4.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Validation Metrics:\n",
      "  BLEU-4 = 0.0465\n",
      "  ROUGE-L (F1) = 0.2800\n",
      "  CIDEr = 0.4258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train Loss: 4.2144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Validation Metrics:\n",
      "  BLEU-4 = 0.0448\n",
      "  ROUGE-L (F1) = 0.2776\n",
      "  CIDEr = 0.4073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss: 4.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Validation Metrics:\n",
      "  BLEU-4 = 0.0458\n",
      "  ROUGE-L (F1) = 0.2821\n",
      "  CIDEr = 0.4234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Train Loss: 4.1546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Validation Metrics:\n",
      "  BLEU-4 = 0.0475\n",
      "  ROUGE-L (F1) = 0.2783\n",
      "  CIDEr = 0.4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:22<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Train Loss: 4.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Validation Metrics:\n",
      "  BLEU-4 = 0.0444\n",
      "  ROUGE-L (F1) = 0.2770\n",
      "  CIDEr = 0.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:23<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss: 4.0922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Validation Metrics:\n",
      "  BLEU-4 = 0.0483\n",
      "  ROUGE-L (F1) = 0.2827\n",
      "  CIDEr = 0.4304\n",
      " Saved new best checkpoint.\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0483\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, fusion_module, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(\n",
    "        val_loader,\n",
    "        fusion_module,\n",
    "        enc,\n",
    "        dec,\n",
    "        vocab,\n",
    "        DEVICE,\n",
    "        DEC_HIDDEN\n",
    "    )\n",
    "    \n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    #Save best model \n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'fusion_state': fusion_module.state_dict(),\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_multifeature_attentionfusion.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:55:34.218856Z",
     "iopub.status.busy": "2025-11-15T07:55:34.218519Z",
     "iopub.status.idle": "2025-11-15T07:55:34.226022Z",
     "shell.execute_reply": "2025-11-15T07:55:34.225267Z",
     "shell.execute_reply.started": "2025-11-15T07:55:34.218830Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_caption_for_video(global_f, local_f, motion_f, fusion_module, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    fusion_module.eval()\n",
    "\n",
    "    # Move inputs to device\n",
    "    global_f = global_f.to(device)\n",
    "    local_f = local_f.to(device)\n",
    "    motion_f = motion_f.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        fused_feats = fusion_module(global_f, local_f, motion_f)  # (1, T, fused_dim)\n",
    "\n",
    "        # Encoder forward\n",
    "        encoder_outs, _ = enc(fused_feats)\n",
    "\n",
    "        # Initialize hidden and cell states as during training\n",
    "        hidden = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word, hidden.squeeze(0), cell.squeeze(0), encoder_outs\n",
    "            )\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            cell = cell.unsqueeze(0)\n",
    "\n",
    "            next_word = out.argmax(1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:54:45.076572Z",
     "iopub.status.busy": "2025-11-15T07:54:45.076284Z",
     "iopub.status.idle": "2025-11-15T07:54:53.794138Z",
     "shell.execute_reply": "2025-11-15T07:54:53.793322Z",
     "shell.execute_reply.started": "2025-11-15T07:54:45.076549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fusion, encoder, decoder states from checkpoint.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:07<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0484\n",
      "ROUGE-L = 0.2733\n",
      "CIDEr   = 0.4251\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load checkpoint with fusion, enc, dec states\n",
    "\n",
    "checkpoint_path = \"/kaggle/working/best_checkpoint_multifeature_attentionfusion.pth\"\n",
    "ckpt = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "fusion_module.load_state_dict(ckpt['fusion_state'])\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "vocab.word2idx = ckpt.get('vocab', vocab.word2idx)\n",
    "\n",
    "print(\"Loaded fusion, encoder, decoder states from checkpoint.\")\n",
    "\n",
    "# Set models to evaluation mode\n",
    "fusion_module.eval()\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "\n",
    "test_bleu, test_rouge, test_cider, _, _ = evaluate_with_metrics(\n",
    "    test_loader, fusion_module, enc, dec, vocab, DEVICE, DEC_HIDDEN\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge['rouge-l']['f']:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:09:20.043247Z",
     "iopub.status.busy": "2025-11-15T08:09:20.042962Z",
     "iopub.status.idle": "2025-11-15T08:09:20.280324Z",
     "shell.execute_reply": "2025-11-15T08:09:20.279610Z",
     "shell.execute_reply.started": "2025-11-15T08:09:20.043226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded fusion, encoder, decoder from checkpoint successfully.\n",
      "\n",
      "============================================================\n",
      " Generating Captions for Sample Test Videos\n",
      "============================================================\n",
      "\n",
      " Video 1: video864.mp4\n",
      " Generated Caption: a person is a car\n",
      " Reference Captions:\n",
      "   Ref 1: a 1997 acura integra for sale\n",
      "   Ref 2: a 1997 acura is displayed for sale\n",
      "   Ref 3: a 1997 acura is parked and for sale at midtown motors\n",
      "\n",
      " Video 2: video440.mp4\n",
      " Generated Caption: a woman is talking about her hair\n",
      " Reference Captions:\n",
      "   Ref 1: a brown haired girl is twisting her hair and talking about keeping it tight\n",
      "   Ref 2: a woman with black hair is combing her hair\n",
      "   Ref 3: there is a woman is combing her hair beautifully\n",
      "\n",
      " Video 3: video206.mp4\n",
      " Generated Caption: a man is talking about a and talking\n",
      " Reference Captions:\n",
      "   Ref 1: a guy playing a guitar\n",
      "   Ref 2: a guy singing a song in a airport setting\n",
      "   Ref 3: a male vocalist performs a ballad\n",
      "\n",
      " Video 4: video810.mp4\n",
      " Generated Caption: a man is swimming in a pool\n",
      " Reference Captions:\n",
      "   Ref 1: a diver swims in the ocean with fish and a manta ray while filming the ray\n",
      "   Ref 2: a manta ray is filmed by a photographer underwater as other fish swim by\n",
      "   Ref 3: a person is scuba diving with different species of sharks and fish\n",
      "\n",
      " Video 5: video114.mp4\n",
      " Generated Caption: two girls are walking in a pool\n",
      " Reference Captions:\n",
      "   Ref 1: a girl is demonstrating a dance move with a group of people on a beach\n",
      "   Ref 2: a girl teaches people how to dance on a beach\n",
      "   Ref 3: a group of men and women sings and dances at sea shore\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = \"/kaggle/working/best_checkpoint_multifeature_attentionfusion.pth\"\n",
    "ckpt = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "fusion_module.load_state_dict(ckpt['fusion_state'])\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "\n",
    "print(\" Loaded fusion, encoder, decoder from checkpoint successfully.\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "fusion_module.eval()\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "\n",
    "def load_global_feature(vid, sample_frames=16):\n",
    "\n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "    global_feat = np.load(os.path.join(FEATURES_GLOBAL_DIR, vid_id))  # (T, 2048)\n",
    "    \n",
    "    if global_feat.ndim == 1:\n",
    "        global_feat = global_feat[None, :]\n",
    "    \n",
    "    # Pad or truncate to sample_frames\n",
    "    if global_feat.shape[0] < sample_frames:\n",
    "        pad = np.zeros((sample_frames - global_feat.shape[0], global_feat.shape[1]), dtype=np.float32)\n",
    "        global_feat = np.concatenate([global_feat, pad], axis=0)\n",
    "    else:\n",
    "        global_feat = global_feat[:sample_frames]\n",
    "    \n",
    "    return global_feat.astype(np.float32)\n",
    "\n",
    "def load_local_feature(vid, sample_frames=16):\n",
    "  \n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "    local_feat = np.load(os.path.join(FEATURES_LOCAL_DIR, vid_id))  # (T, 49, 2048) or (T, 2048)\n",
    "    \n",
    "    # Handle different shapes\n",
    "    if local_feat.ndim == 3:\n",
    "        local_feat = local_feat.mean(axis=1)  # (T, 2048)\n",
    "    elif local_feat.ndim == 1:\n",
    "        local_feat = local_feat[None, :]\n",
    "    \n",
    "    # Pad or truncate to sample_frames\n",
    "    if local_feat.shape[0] < sample_frames:\n",
    "        pad = np.zeros((sample_frames - local_feat.shape[0], local_feat.shape[1]), dtype=np.float32)\n",
    "        local_feat = np.concatenate([local_feat, pad], axis=0)\n",
    "    else:\n",
    "        local_feat = local_feat[:sample_frames]\n",
    "    \n",
    "    return local_feat.astype(np.float32)\n",
    "\n",
    "def load_motion_feature(vid, sample_frames=16):\n",
    "\n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "    motion_feat = np.load(os.path.join(FEATURES_MOTION_DIR, vid_id))  # (512,)\n",
    "    \n",
    "    if motion_feat.ndim == 1:\n",
    "        # Repeat motion feature across time dimension\n",
    "        motion_feat = np.repeat(motion_feat[None, :], sample_frames, axis=0)  # (T, 512)\n",
    "    \n",
    "    # Pad or truncate to sample_frames\n",
    "    if motion_feat.shape[0] < sample_frames:\n",
    "        pad = np.zeros((sample_frames - motion_feat.shape[0], motion_feat.shape[1]), dtype=np.float32)\n",
    "        motion_feat = np.concatenate([motion_feat, pad], axis=0)\n",
    "    else:\n",
    "        motion_feat = motion_feat[:sample_frames]\n",
    "    \n",
    "    return motion_feat.astype(np.float32)\n",
    "\n",
    "\n",
    "# Caption generation function with fusion\n",
    "def generate_caption_for_video(global_f, local_f, motion_f, fusion_module, enc, dec, vocab, device, max_len=20):\n",
    "  \n",
    "    fusion_module.eval()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # Move to device\n",
    "    global_f = global_f.to(device)\n",
    "    local_f = local_f.to(device)\n",
    "    motion_f = motion_f.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Fuse features\n",
    "        fused_feats = fusion_module(global_f, local_f, motion_f)  # (1, T, 512)\n",
    "\n",
    "        #  Encode fused features\n",
    "        encoder_outs, _ = enc(fused_feats)  # (1, T, enc_output_dim)\n",
    "\n",
    "        #  Initialize decoder hidden and cell states\n",
    "        B = 1\n",
    "        hidden = torch.zeros(B, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(B, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        # Start with <BOS> token\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        #  Generate caption token by token\n",
    "        for _ in range(max_len):\n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word, hidden, cell, encoder_outs\n",
    "            )\n",
    "            \n",
    "            next_word = out.argmax(1).item()\n",
    "\n",
    "            # Stop if <EOS> token generated\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "\n",
    "# Generate captions for sample test videos\n",
    "random.seed(9)\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" Generating Captions for Sample Test Videos\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    global_f = load_global_feature(vid, sample_frames=16)   # (16, 2048)\n",
    "    local_f = load_local_feature(vid, sample_frames=16)     # (16, 2048)\n",
    "    motion_f = load_motion_feature(vid, sample_frames=16)   # (16, 512)\n",
    "\n",
    "    # Convert to tensors with batch dimension\n",
    "    global_f = torch.FloatTensor(global_f).unsqueeze(0)  # (1, 16, 2048)\n",
    "    local_f = torch.FloatTensor(local_f).unsqueeze(0)    # (1, 16, 2048)\n",
    "    motion_f = torch.FloatTensor(motion_f).unsqueeze(0)  # (1, 16, 512)\n",
    "\n",
    "    # Generate caption using fusion module\n",
    "    generated_caption = generate_caption_for_video(\n",
    "        global_f, local_f, motion_f, fusion_module, enc, dec, vocab, DEVICE\n",
    "    )\n",
    "    \n",
    "  \n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {generated_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"   Ref {j+1}: {ref}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:12:48.980434Z",
     "iopub.status.busy": "2025-11-15T08:12:48.980140Z",
     "iopub.status.idle": "2025-11-15T08:12:49.057414Z",
     "shell.execute_reply": "2025-11-15T08:12:49.056780Z",
     "shell.execute_reply.started": "2025-11-15T08:12:48.980412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " Generating Captions for Sample Test Videos\n",
      "============================================================\n",
      "\n",
      " Video 1: video600.mp4\n",
      " Generated Caption: two men are wrestling in a competition\n",
      " Reference Captions:\n",
      "   Ref 1: a home recording of a wrestling match\n",
      "   Ref 2: a homemade recording of a school wrestling match\n",
      "   Ref 3: a scene from a wrestling match is shown\n",
      "\n",
      " Video 2: video466.mp4\n",
      " Generated Caption: a person is showing a car\n",
      " Reference Captions:\n",
      "   Ref 1: a man describes the back seat of a car\n",
      "   Ref 2: a man describing a new car\n",
      "   Ref 3: a man describing features of a car\n",
      "\n",
      " Video 3: video1161.mp4\n",
      " Generated Caption: a girl is talking on a stage\n",
      " Reference Captions:\n",
      "   Ref 1: a boy is performing a song on the voice\n",
      "   Ref 2: a boy is singing a song in a stage in front of others and playing guitor\n",
      "   Ref 3: a boy is singing and playing guitar on a stage\n",
      "\n",
      " Video 4: video1252.mp4\n",
      " Generated Caption: a man is playing a basketball\n",
      " Reference Captions:\n",
      "   Ref 1: a baskeball player runs across the court and makes a basket\n",
      "   Ref 2: a basketball player is shooting the ball\n",
      "   Ref 3: a basketball player shot a basket\n",
      "\n",
      " Video 5: video146.mp4\n",
      " Generated Caption: a child is playing with a toy\n",
      " Reference Captions:\n",
      "   Ref 1: a lego character is bending over while another lego character is moving\n",
      "   Ref 2: a lego clip showing a man bent over and another man behind him\n",
      "   Ref 3: a lego man is humping\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# UNIFIED FEATURE LOADING & PREPROCESSING\n",
    "# ==========================================\n",
    "def load_and_preprocess_features(vid, sample_frames=16, \n",
    "                                  global_dir=FEATURES_GLOBAL_DIR,\n",
    "                                  local_dir=FEATURES_LOCAL_DIR,\n",
    "                                  motion_dir=FEATURES_MOTION_DIR):\n",
    " \n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "    \n",
    "    # Load global features\n",
    "    global_feat = np.load(os.path.join(global_dir, vid_id))  # (T, 2048)\n",
    "    if global_feat.ndim == 1:\n",
    "        global_feat = global_feat[None, :]\n",
    "    \n",
    "    # Load local features\n",
    "    local_feat = np.load(os.path.join(local_dir, vid_id))  # (T, 49, 2048) or (T, 2048)\n",
    "    if local_feat.ndim == 3:\n",
    "        local_feat = local_feat.mean(axis=1)  # (T, 2048)\n",
    "    elif local_feat.ndim == 1:\n",
    "        local_feat = local_feat[None, :]\n",
    "    \n",
    "    # Load motion features\n",
    "    motion_feat = np.load(os.path.join(motion_dir, vid_id))  # (512,)\n",
    "    if motion_feat.ndim == 1:\n",
    "        motion_feat = np.repeat(motion_feat[None, :], max(global_feat.shape[0], local_feat.shape[0]), axis=0)\n",
    "    \n",
    "    # Ensure all features have same temporal length\n",
    "    T_max = max(global_feat.shape[0], local_feat.shape[0], motion_feat.shape[0])\n",
    "    \n",
    "    # Pad or truncate all to sample_frames\n",
    "    def pad_to_T(x, T):\n",
    "        if x.shape[0] < T:\n",
    "            pad = np.zeros((T - x.shape[0], x.shape[1]), dtype=np.float32)\n",
    "            return np.concatenate([x, pad], axis=0)\n",
    "        else:\n",
    "            return x[:T].astype(np.float32)\n",
    "    \n",
    "    global_feat = pad_to_T(global_feat, sample_frames)  # (sample_frames, 2048)\n",
    "    local_feat = pad_to_T(local_feat, sample_frames)    # (sample_frames, 2048)\n",
    "    motion_feat = pad_to_T(motion_feat, sample_frames)  # (sample_frames, 512)\n",
    "    \n",
    "    return global_feat, local_feat, motion_feat\n",
    "\n",
    "\n",
    "# UNIFIED CAPTION GENERATION\n",
    "\n",
    "def generate_caption_for_video(vid, fusion_module, enc, dec, vocab, device, \n",
    "                               sample_frames=16, max_len=20):\n",
    "\n",
    "    fusion_module.eval()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # Load and preprocess features using unified function\n",
    "    global_f, local_f, motion_f = load_and_preprocess_features(vid, sample_frames=sample_frames)\n",
    "    \n",
    "    # Convert to tensors with batch dimension\n",
    "    global_f = torch.FloatTensor(global_f).unsqueeze(0).to(device)  # (1, T, 2048)\n",
    "    local_f = torch.FloatTensor(local_f).unsqueeze(0).to(device)    # (1, T, 2048)\n",
    "    motion_f = torch.FloatTensor(motion_f).unsqueeze(0).to(device)  # (1, T, 512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Fuse features\n",
    "        fused_feats = fusion_module(global_f, local_f, motion_f)  # (1, T, 512)\n",
    "\n",
    "        # Encode\n",
    "        encoder_outs, _ = enc(fused_feats)\n",
    "\n",
    "        # Initialize decoder\n",
    "        B = 1\n",
    "        hidden = torch.zeros(B, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(B, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        # Decode\n",
    "        for _ in range(max_len):\n",
    "            out, hidden, cell, _ = dec.forward_step(input_word, hidden, cell, encoder_outs)\n",
    "            next_word = out.argmax(1).item()\n",
    "\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "\n",
    "# SAMPLE GENERATION\n",
    "\n",
    "random.seed(60)\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" Generating Captions for Sample Test Videos\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    generated_caption = generate_caption_for_video(\n",
    "        vid, fusion_module, enc, dec, vocab, DEVICE, sample_frames=16, max_len=20\n",
    "    )\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {generated_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"   Ref {j+1}: {ref}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:16:17.316472Z",
     "iopub.status.busy": "2025-11-15T08:16:17.316198Z",
     "iopub.status.idle": "2025-11-15T08:16:17.387708Z",
     "shell.execute_reply": "2025-11-15T08:16:17.387068Z",
     "shell.execute_reply.started": "2025-11-15T08:16:17.316450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " Generating Captions for Sample Test Videos\n",
      "============================================================\n",
      "\n",
      " Video 1: video1259.mp4\n",
      " Generated Caption: a man is talking about a\n",
      " Reference Captions:\n",
      "   Ref 1: a group of men discuss action movies\n",
      "   Ref 2: a group of men discuss point break\n",
      "   Ref 3: a group of men talking about who the best action character is\n",
      "\n",
      " Video 2: video1116.mp4\n",
      " Generated Caption: a person is showing how to make a pet\n",
      " Reference Captions:\n",
      "   Ref 1: a battery cover is removed off a toy\n",
      "   Ref 2: a child puts back together one of his toys\n",
      "   Ref 3: a girl is dressing up a doll\n",
      "\n",
      " Video 3: video740.mp4\n",
      " Generated Caption: a group of people are playing\n",
      " Reference Captions:\n",
      "   Ref 1: a big cat is in a play\n",
      "   Ref 2: a fox sniffs around\n",
      "   Ref 3: a group of people are performing on stage\n",
      "\n",
      " Video 4: video1282.mp4\n",
      " Generated Caption: two men are in a room\n",
      " Reference Captions:\n",
      "   Ref 1: a commercial for an omnibot toy\n",
      "   Ref 2: a man talks about omnibot\n",
      "   Ref 3: a robot bringing food to people\n",
      "\n",
      " Video 5: video546.mp4\n",
      " Generated Caption: a person is cooking a dish in a bowl\n",
      " Reference Captions:\n",
      "   Ref 1: a cook adding different ingredients to a dish\n",
      "   Ref 2: a cook prepares food in a hot pan\n",
      "   Ref 3: a hand sprinkles spices onto food\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "random.seed(56)\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" Generating Captions for Sample Test Videos\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    generated_caption = generate_caption_for_video(\n",
    "        vid, fusion_module, enc, dec, vocab, DEVICE, max_len=20\n",
    "    )\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {generated_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"   Ref {j+1}: {ref}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:57:31.607299Z",
     "iopub.status.busy": "2025-11-22T12:57:31.606626Z",
     "iopub.status.idle": "2025-11-22T13:13:43.602437Z",
     "shell.execute_reply": "2025-11-22T13:13:43.601489Z",
     "shell.execute_reply.started": "2025-11-22T12:57:31.607267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.7284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0781\n",
      "  ROUGE-L (F1) = 0.2723\n",
      "  CIDEr = 0.2425\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:57<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 5.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0804\n",
      "  ROUGE-L (F1) = 0.2901\n",
      "  CIDEr = 0.2560\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:57<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 5.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0707\n",
      "  ROUGE-L (F1) = 0.2735\n",
      "  CIDEr = 0.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 4.9229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0787\n",
      "  ROUGE-L (F1) = 0.2976\n",
      "  CIDEr = 0.2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 4.8161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0787\n",
      "  ROUGE-L (F1) = 0.2999\n",
      "  CIDEr = 0.3358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 4.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0830\n",
      "  ROUGE-L (F1) = 0.3064\n",
      "  CIDEr = 0.3682\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:57<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 4.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0786\n",
      "  ROUGE-L (F1) = 0.3069\n",
      "  CIDEr = 0.3674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:57<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 4.5635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0834\n",
      "  ROUGE-L (F1) = 0.3097\n",
      "  CIDEr = 0.3844\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 4.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0806\n",
      "  ROUGE-L (F1) = 0.3096\n",
      "  CIDEr = 0.4023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:57<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 4.4324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0818\n",
      "  ROUGE-L (F1) = 0.3079\n",
      "  CIDEr = 0.3885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 4.3681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Metrics:\n",
      "  BLEU-4 = 0.0782\n",
      "  ROUGE-L (F1) = 0.3101\n",
      "  CIDEr = 0.4006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:57<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 4.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Metrics:\n",
      "  BLEU-4 = 0.0745\n",
      "  ROUGE-L (F1) = 0.3055\n",
      "  CIDEr = 0.3733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 4.3003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Validation Metrics:\n",
      "  BLEU-4 = 0.0739\n",
      "  ROUGE-L (F1) = 0.3028\n",
      "  CIDEr = 0.3810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 4.2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Validation Metrics:\n",
      "  BLEU-4 = 0.0727\n",
      "  ROUGE-L (F1) = 0.3018\n",
      "  CIDEr = 0.3976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:58<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 4.2039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Validation Metrics:\n",
      "  BLEU-4 = 0.0800\n",
      "  ROUGE-L (F1) = 0.3045\n",
      "  CIDEr = 0.4085\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0834\n"
     ]
    }
   ],
   "source": [
    "#Multimodal attention based fusion-hindi\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, fusion_module, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(\n",
    "        val_loader,\n",
    "        fusion_module,\n",
    "        enc,\n",
    "        dec,\n",
    "        vocab,\n",
    "        DEVICE,\n",
    "        DEC_HIDDEN\n",
    "    )\n",
    "    \n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    # ---- Save best model ----\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'fusion_state': fusion_module.state_dict(),\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_multifeature_attention_fusion_hindi.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:15:15.611129Z",
     "iopub.status.busy": "2025-11-22T13:15:15.610362Z",
     "iopub.status.idle": "2025-11-22T13:15:22.406679Z",
     "shell.execute_reply": "2025-11-22T13:15:22.405784Z",
     "shell.execute_reply.started": "2025-11-22T13:15:15.611103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fusion, encoder, decoder states from checkpoint.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0671\n",
      "ROUGE-L = 0.2852\n",
      "CIDEr   = 0.3246\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load checkpoint with fusion, enc, dec states\n",
    "\n",
    "checkpoint_path = \"/kaggle/working/best_checkpoint_multifeature_attention_fusion_hindi.pth\"\n",
    "ckpt = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "fusion_module.load_state_dict(ckpt['fusion_state'])\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "vocab.word2idx = ckpt.get('vocab', vocab.word2idx)\n",
    "\n",
    "print(\"Loaded fusion, encoder, decoder states from checkpoint.\")\n",
    "\n",
    "\n",
    "fusion_module.eval()\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Run evaluation on test set\n",
    "\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "\n",
    "test_bleu, test_rouge, test_cider, _, _ = evaluate_with_metrics(\n",
    "    test_loader, fusion_module, enc, dec, vocab, DEVICE, DEC_HIDDEN\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge['rouge-l']['f']:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:19:54.372964Z",
     "iopub.status.busy": "2025-11-22T13:19:54.372657Z",
     "iopub.status.idle": "2025-11-22T13:19:54.523138Z",
     "shell.execute_reply": "2025-11-22T13:19:54.522347Z",
     "shell.execute_reply.started": "2025-11-22T13:19:54.372939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded fusion, encoder, decoder from checkpoint successfully.\n",
      "\n",
      "============================================================\n",
      " Generating Captions for Sample Test Videos\n",
      "============================================================\n",
      "\n",
      " Video 1: video505.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§§‡•ç‡§Ø ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      " Reference Captions:\n",
      "   Ref 1: ‡§ï‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§®‡•á ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§á‡§Ç‡§ü‡§∞‡§µ‡•ç‡§Ø‡•Ç ‡§≤‡§ø‡§Ø‡§æ\n",
      "   Ref 2: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§è‡§ï ‡§â‡§§‡•ç‡§∏‡§µ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "   Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§Ç‡§¶‡•Ç‡§ï ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video393.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "   Ref 1: ‡§∏‡•Ç‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§π ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§π‡•ã ‡§≤‡§ø‡§Ø‡§æ ‡§§‡§æ‡§ï‡§ø ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•Ä ‡§π‡§§‡•ç‡§Ø‡§æ ‡§® ‡§ï‡§∞‡•á\n",
      "   Ref 2: ‡§è‡§ï ‡§ú‡§π‡§æ‡§ú‡§º ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡•à‡§†‡•á ‡§è‡§ï ‡§¨‡•Ç‡§¢‡§º‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§è‡§ï ‡§ï‡§æ‡§≤‡•á ‡§ú‡•à‡§ï‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§ñ‡§°‡§º‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "   Ref 3: ‡§¶‡•ã ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§π‡§§‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§™‡§ø‡§§‡§æ ‡§™‡§∞ ‡§Ü‡§∞‡•ã‡§™ ‡§≤‡§ó‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video65.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§§‡•ç‡§Ø ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "   Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "   Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "   Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ó‡•Ä‡§§ ‡§ó‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 4: video191.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      " Reference Captions:\n",
      "   Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§®‡•á‡§µ‡§æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "   Ref 2: ‡§∏‡§≠‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡•ç‡§ü‡§æ‡§á‡§≤ ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "   Ref 3: ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® - ‡§™‡•ã‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      " Video 5: video939.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "   Ref 1: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡§æ‡§∞ ‡§ï‡§ø‡§®‡§æ‡§∞‡•á ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§¨‡§π‡§æ‡§µ\n",
      "   Ref 2: ‡§¶‡•å‡§°‡§º ‡§ü‡•ç‡§∞‡•à‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§≠‡§ü‡§ï‡§§‡•Ä ‡§π‡•Å‡§à\n",
      "   Ref 3: ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ï‡•ã‡§®‡•á ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¨‡§π‡§æ‡§µ\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_caption(spm_text):\n",
    "   \n",
    "    # Remove SentencePiece underscore markers\n",
    "    text = spm_text.replace(\"‚ñÅ\", \" \").strip()\n",
    "\n",
    "    # Replace <unk> tokens with placeholder or remove\n",
    "    text = text.replace(\"<unk>\", \"\")  # Or use \"[UNK]\" or any custom token\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = \"/kaggle/working/best_checkpoint_multifeature_attention_fusion_hindi.pth\"\n",
    "ckpt = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "fusion_module.load_state_dict(ckpt['fusion_state'])\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "\n",
    "print(\" Loaded fusion, encoder, decoder from checkpoint successfully.\")\n",
    "\n",
    "\n",
    "fusion_module.eval()\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "\n",
    "def load_global_feature(vid, sample_frames=16):\n",
    " \n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "    global_feat = np.load(os.path.join(FEATURES_GLOBAL_DIR, vid_id))  # (T, 2048)\n",
    "    \n",
    "    if global_feat.ndim == 1:\n",
    "        global_feat = global_feat[None, :]\n",
    "    \n",
    "    # Pad or truncate to sample_frames\n",
    "    if global_feat.shape[0] < sample_frames:\n",
    "        pad = np.zeros((sample_frames - global_feat.shape[0], global_feat.shape[1]), dtype=np.float32)\n",
    "        global_feat = np.concatenate([global_feat, pad], axis=0)\n",
    "    else:\n",
    "        global_feat = global_feat[:sample_frames]\n",
    "    \n",
    "    return global_feat.astype(np.float32)\n",
    "\n",
    "def load_local_feature(vid, sample_frames=16):\n",
    " \n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "    local_feat = np.load(os.path.join(FEATURES_LOCAL_DIR, vid_id))  # (T, 49, 2048) or (T, 2048)\n",
    "    \n",
    "    # Handle different shapes\n",
    "    if local_feat.ndim == 3:\n",
    "        local_feat = local_feat.mean(axis=1)  # (T, 2048)\n",
    "    elif local_feat.ndim == 1:\n",
    "        local_feat = local_feat[None, :]\n",
    "    \n",
    "    # Pad or truncate to sample_frames\n",
    "    if local_feat.shape[0] < sample_frames:\n",
    "        pad = np.zeros((sample_frames - local_feat.shape[0], local_feat.shape[1]), dtype=np.float32)\n",
    "        local_feat = np.concatenate([local_feat, pad], axis=0)\n",
    "    else:\n",
    "        local_feat = local_feat[:sample_frames]\n",
    "    \n",
    "    return local_feat.astype(np.float32)\n",
    "\n",
    "def load_motion_feature(vid, sample_frames=16):\n",
    " \n",
    "    vid_id = vid.replace(\".mp4\", \".npy\")\n",
    "    motion_feat = np.load(os.path.join(FEATURES_MOTION_DIR, vid_id))  # (512,)\n",
    "    \n",
    "    if motion_feat.ndim == 1:\n",
    "        # Repeat motion feature across time dimension\n",
    "        motion_feat = np.repeat(motion_feat[None, :], sample_frames, axis=0)  # (T, 512)\n",
    "    \n",
    "    # Pad or truncate to sample_frames\n",
    "    if motion_feat.shape[0] < sample_frames:\n",
    "        pad = np.zeros((sample_frames - motion_feat.shape[0], motion_feat.shape[1]), dtype=np.float32)\n",
    "        motion_feat = np.concatenate([motion_feat, pad], axis=0)\n",
    "    else:\n",
    "        motion_feat = motion_feat[:sample_frames]\n",
    "    \n",
    "    return motion_feat.astype(np.float32)\n",
    "\n",
    "\n",
    "def generate_caption_for_video(global_f, local_f, motion_f, fusion_module, enc, dec, vocab, device, max_len=20):\n",
    "\n",
    "    fusion_module.eval()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # Move to device\n",
    "    global_f = global_f.to(device)\n",
    "    local_f = local_f.to(device)\n",
    "    motion_f = motion_f.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Step 1: Fuse features\n",
    "        fused_feats = fusion_module(global_f, local_f, motion_f)  # (1, T, 512)\n",
    "\n",
    "        # Step 2: Encode fused features\n",
    "        encoder_outs, _ = enc(fused_feats)  # (1, T, enc_output_dim)\n",
    "\n",
    "        # Step 3: Initialize decoder hidden and cell states\n",
    "        B = 1\n",
    "        hidden = torch.zeros(B, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(B, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        # Start with <BOS> token\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        # Step 4: Generate caption token by token\n",
    "        for _ in range(max_len):\n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word, hidden, cell, encoder_outs\n",
    "            )\n",
    "            \n",
    "            next_word = out.argmax(1).item()\n",
    "\n",
    "            # Stop if <EOS> token generated\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "# Generate captions for sample test videos\n",
    "\n",
    "random.seed(70)\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" Generating Captions for Sample Test Videos\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    # Load individual features for this video\n",
    "    global_f = load_global_feature(vid, sample_frames=16)   # (16, 2048)\n",
    "    local_f = load_local_feature(vid, sample_frames=16)     # (16, 2048)\n",
    "    motion_f = load_motion_feature(vid, sample_frames=16)   # (16, 512)\n",
    "\n",
    "    # Convert to tensors with batch dimension\n",
    "    global_f = torch.FloatTensor(global_f).unsqueeze(0)  # (1, 16, 2048)\n",
    "    local_f = torch.FloatTensor(local_f).unsqueeze(0)    # (1, 16, 2048)\n",
    "    motion_f = torch.FloatTensor(motion_f).unsqueeze(0)  # (1, 16, 512)\n",
    "\n",
    "    # Generate caption using fusion module\n",
    "    generated_caption = generate_caption_for_video(\n",
    "        global_f, local_f, motion_f, fusion_module, enc, dec, vocab, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Get reference captions\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {clean_caption(generated_caption)}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"   Ref {j+1}: {ref}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments using different Models for HINDI CAPTION GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T06:02:16.883723Z",
     "iopub.status.busy": "2025-11-16T06:02:16.883011Z",
     "iopub.status.idle": "2025-11-16T06:02:21.826139Z",
     "shell.execute_reply": "2025-11-16T06:02:21.825366Z",
     "shell.execute_reply.started": "2025-11-16T06:02:16.883693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0rc1\n",
      "  Using cached googletrans-4.0.0rc1-py3-none-any.whl\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0rc1)\n",
      "  Using cached httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2025.10.5)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
      "  Using cached hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.3.1)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.5.0)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
      "  Using cached httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1)\n",
      "  Using cached h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1)\n",
      "  Using cached h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (5.2.0)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1)\n",
      "  Using cached hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Using cached h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Using cached hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Using cached hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Installing collected packages: hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: hpack\n",
      "    Found existing installation: hpack 4.1.0\n",
      "    Uninstalling hpack-4.1.0:\n",
      "      Successfully uninstalled hpack-4.1.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 5.2.0\n",
      "    Uninstalling chardet-5.2.0:\n",
      "      Successfully uninstalled chardet-5.2.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.11\n",
      "    Uninstalling idna-3.11:\n",
      "      Successfully uninstalled idna-3.11\n",
      "  Attempting uninstall: h2\n",
      "    Found existing installation: h2 4.3.0\n",
      "    Uninstalling h2-4.3.0:\n",
      "      Successfully uninstalled h2-4.3.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 4.0.2\n",
      "    Uninstalling googletrans-4.0.2:\n",
      "      Successfully uninstalled googletrans-4.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires httpx>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "google-genai 1.48.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
      "mcp 1.20.0 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\n",
      "openai 2.7.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "litellm 1.76.3 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "gradio-client 1.11.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
      "langsmith 0.4.8 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 idna-2.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T06:10:45.055814Z",
     "iopub.status.busy": "2025-11-16T06:10:45.055254Z",
     "iopub.status.idle": "2025-11-16T06:14:32.814286Z",
     "shell.execute_reply": "2025-11-16T06:14:32.813265Z",
     "shell.execute_reply.started": "2025-11-16T06:10:45.055791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of video keys in JSON: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/10000 [03:47<45:05:47, 16.26s/it]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/2084589213.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Run the asyncio function with await (only inside Jupyter or IPython)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mtranslate_all_captions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_48/2084589213.py\u001b[0m in \u001b[0;36mtranslate_all_captions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mhi_caps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_caps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtranslate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mhi_caps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/2084589213.py\u001b[0m in \u001b[0;36mtranslate_caption\u001b[0;34m(translator, cap)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mpassed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mBasic\u001b[0m \u001b[0musage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_translate_legacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;31m#dummy default value here as it is not used by api client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'webapp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTracebackType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     ) -> None:\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpires_at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             ):\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mconnections_to_close\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_from_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclose_connection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_keepalive_sweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             )\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_http11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             self.connection = AsyncHTTP11Connection(\n\u001b[1;32m    105\u001b[0m                 \u001b[0msocket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssl_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/http2.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     async def wait_for_event(\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     ) -> h2.events.Event:\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/http2.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    148\u001b[0m         ]\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initiate_connection=%r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_flow_control_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/http2.py\u001b[0m in \u001b[0;36m_receive_response\u001b[0;34m(self, request, stream_id)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mreason_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reason_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         stream = AsyncByteStream(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0maiterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maclose_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/http2.py\u001b[0m in \u001b[0;36m_receive_stream_event\u001b[0;34m(self, request, stream_id)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     async def receive_response(\n\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/http2.py\u001b[0m in \u001b[0;36m_receive_events\u001b[0;34m(self, request, stream_id)\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0mamount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_controlled_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                 await self.connection.acknowledge_received_data(\n\u001b[0;32m--> 364\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 )\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/http2.py\u001b[0m in \u001b[0;36m_read_incoming_data\u001b[0;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sslobject_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36m_call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1261\u001b[0m             ):\n\u001b[1;32m   1262\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/locks.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36m__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# This tells Task to wait for completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"await wasn't used with future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def translate_caption(translator, cap):\n",
    "    translation = await translator.translate(cap, src='en', dest='hi')\n",
    "    return translation.text\n",
    "\n",
    "async def translate_all_captions():\n",
    "    with open('/kaggle/input/msr-vtt-1289-hindi-english/captions.json', 'r', encoding='utf-8') as f:\n",
    "        captions_en = json.load(f)\n",
    "\n",
    "    translator = Translator()\n",
    "    captions_hi = {}\n",
    "\n",
    "    video_keys = list(captions_en.keys())\n",
    "    print(f\"Number of video keys in JSON: {len(video_keys)}\")\n",
    "\n",
    "    for vid_key in tqdm(video_keys):\n",
    "        en_caps = captions_en[vid_key]\n",
    "        hi_caps = []\n",
    "        for cap in en_caps:\n",
    "            text = await translate_caption(translator, cap)\n",
    "            hi_caps.append(text)\n",
    "\n",
    "        captions_hi[vid_key] = hi_caps\n",
    "\n",
    "    with open('captions_hindi.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(captions_hi, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Finished Hindi caption translation and saved.\")\n",
    "\n",
    "# Run the asyncio function with await (only inside Jupyter or IPython)\n",
    "await translate_all_captions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T06:32:15.894507Z",
     "iopub.status.busy": "2025-11-16T06:32:15.894024Z",
     "iopub.status.idle": "2025-11-16T06:43:14.336085Z",
     "shell.execute_reply": "2025-11-16T06:43:14.335199Z",
     "shell.execute_reply.started": "2025-11-16T06:32:15.894483Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1290/1290 [10:56<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Hindi caption translation and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load tokenizer and model for English->Hindi translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-hi'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load English captions JSON\n",
    "with open('/kaggle/input/msr-vtt-1289-hindi-english/captions.json', 'r', encoding='utf-8') as f:\n",
    "    captions_en = json.load(f)\n",
    "\n",
    "captions_hi = {}\n",
    "\n",
    "num_videos = 1290  # total number of videos expected\n",
    "\n",
    "for vid_idx in tqdm(range(num_videos)):\n",
    "    vid_key = f\"video{vid_idx}.mp4\"\n",
    "    en_caps = captions_en.get(vid_key)\n",
    "    if not en_caps:\n",
    "        print(f\"Warning: captions for {vid_key} not found in JSON, skipping\")\n",
    "        continue\n",
    "    hi_caps = []\n",
    "    \n",
    "    # Process captions in small batches for efficiency\n",
    "    batch_size = 8\n",
    "    for i in range(0, len(en_caps), batch_size):\n",
    "        batch = en_caps[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "        translated = model.generate(**inputs)\n",
    "        hi_batch = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        hi_caps.extend(hi_batch)\n",
    "    \n",
    "    captions_hi[vid_key] = hi_caps\n",
    "\n",
    "# Save Hindi captions\n",
    "with open('captions_hindi.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(captions_hi, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Finished Hindi caption translation and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:13:53.951224Z",
     "iopub.status.busy": "2025-11-24T18:13:53.950606Z",
     "iopub.status.idle": "2025-11-24T18:13:54.572937Z",
     "shell.execute_reply": "2025-11-24T18:13:54.572122Z",
     "shell.execute_reply.started": "2025-11-24T18:13:53.951201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: hindi_spm_captions.txt\n",
      "  input_format: \n",
      "  model_prefix: hindi_spm\n",
      "  model_type: WORD\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <PAD>\n",
      "  user_defined_symbols: <BOS>\n",
      "  user_defined_symbols: <EOS>\n",
      "  user_defined_symbols: <UNK>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: hindi_spm_captions.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 25800 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <BOS>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <EOS>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <UNK>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1244200\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.952% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=98\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99952\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 25800 sentences.\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: hindi_spm.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: hindi_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (including special tokens): 8000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "\n",
    "class VocabSPM:\n",
    "    def __init__(self, captions_dict=None, freq_threshold=1, max_size=None,\n",
    "                 model_prefix='hindi_spm', vocab_size=8000, character_coverage=0.9995, model_type='word'):\n",
    "        #character_coverage=0.98\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.special_tokens = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n",
    "\n",
    "        # Build SentencePiece model on captions if captions_dict given\n",
    "        if captions_dict:\n",
    "            captions_file = model_prefix + '_captions.txt'\n",
    "            with open(captions_file, 'w', encoding='utf-8') as f:\n",
    "                for caps in captions_dict.values():\n",
    "                    for c in caps:\n",
    "                        f.write(c.strip() + '\\n')\n",
    "\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=captions_file,\n",
    "                model_prefix=model_prefix,\n",
    "                vocab_size=vocab_size,\n",
    "                character_coverage=character_coverage,\n",
    "                model_type=model_type,\n",
    "                user_defined_symbols=self.special_tokens\n",
    "            )\n",
    "\n",
    "        # Load trained SentencePiece model\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(model_prefix + '.model')\n",
    "\n",
    "        # Build word2idx and idx2word with special token offsets\n",
    "        self.word2idx = {tok: idx for idx, tok in enumerate(self.special_tokens)}\n",
    "        offset = len(self.special_tokens)\n",
    "        for i in range(self.sp.get_piece_size()):\n",
    "            piece = self.sp.id_to_piece(i)\n",
    "            self.word2idx[piece] = i + offset\n",
    "        self.idx2word = {idx: tok for tok, idx in self.word2idx.items()}\n",
    "\n",
    "    def build_vocab(self, captions_dict):\n",
    "        # Optional: build frequency stats across captions for filtering or stats\n",
    "        counter = Counter()\n",
    "        for caps in captions_dict.values():\n",
    "            for c in caps:\n",
    "                pieces = self.sp.encode(c, out_type=str)\n",
    "                counter.update(pieces)\n",
    "        # Filtering by freq_threshold and max_size if needed (optional)\n",
    "        # Not strictly needed for SentencePiece vocab as vocab fixed by spm\n",
    "        self.freq_counter = counter\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        # Encode using SentencePiece adding special BOS/EOS tokens\n",
    "        spm_pieces = self.sp.encode(text, out_type=int)  # ids from spm\n",
    "        offset_pieces = [i + len(self.special_tokens) for i in spm_pieces]  # offset by specials\n",
    "        return [self.word2idx[self.bos_token]] + offset_pieces + [self.word2idx[self.eos_token]]\n",
    "\n",
    "    def decode(self, idx_list):\n",
    "        # Decode token ids back to sentence, ignoring special tokens\n",
    "        offset = len(self.special_tokens)\n",
    "        spm_ids = [i - offset for i in idx_list if i >= offset]\n",
    "        return self.sp.decode(spm_ids)\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "CAPTIONS_FILE = '/kaggle/input/captions-hindi-1290/captions_hindi_1290.json'\n",
    "with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "    captions_hindi = json.load(f)\n",
    "\n",
    "vocab = VocabSPM(captions_dict=captions_hindi, vocab_size=8000)\n",
    "vocab.build_vocab(captions_hindi)\n",
    "print(\"Vocabulary size (including special tokens):\", len(vocab.word2idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T07:27:23.732455Z",
     "iopub.status.busy": "2025-11-16T07:27:23.732183Z",
     "iopub.status.idle": "2025-11-16T07:45:25.534514Z",
     "shell.execute_reply": "2025-11-16T07:45:25.533715Z",
     "shell.execute_reply.started": "2025-11-16T07:27:23.732436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.7554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0755\n",
      "  ROUGE-L (F1) = 0.3008\n",
      "  CIDEr = 0.3569\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.6547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 13.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0777\n",
      "  ROUGE-L (F1) = 0.3054\n",
      "  CIDEr = 0.3725\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:03<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 4.5663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0709\n",
      "  ROUGE-L (F1) = 0.2972\n",
      "  CIDEr = 0.3629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:03<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 4.5067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0742\n",
      "  ROUGE-L (F1) = 0.3062\n",
      "  CIDEr = 0.3664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 4.4429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0805\n",
      "  ROUGE-L (F1) = 0.3070\n",
      "  CIDEr = 0.3978\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 4.3846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0811\n",
      "  ROUGE-L (F1) = 0.3121\n",
      "  CIDEr = 0.4227\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 4.3239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0762\n",
      "  ROUGE-L (F1) = 0.3072\n",
      "  CIDEr = 0.4198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 4.2835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0779\n",
      "  ROUGE-L (F1) = 0.3077\n",
      "  CIDEr = 0.4173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 4.2517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0810\n",
      "  ROUGE-L (F1) = 0.3132\n",
      "  CIDEr = 0.4379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:11<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 4.2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0812\n",
      "  ROUGE-L (F1) = 0.3079\n",
      "  CIDEr = 0.4259\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 4.1586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Metrics:\n",
      "  BLEU-4 = 0.0713\n",
      "  ROUGE-L (F1) = 0.3060\n",
      "  CIDEr = 0.3954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 4.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Metrics:\n",
      "  BLEU-4 = 0.0753\n",
      "  ROUGE-L (F1) = 0.3061\n",
      "  CIDEr = 0.3962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 4.0939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Validation Metrics:\n",
      "  BLEU-4 = 0.0791\n",
      "  ROUGE-L (F1) = 0.3152\n",
      "  CIDEr = 0.4478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 4.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Validation Metrics:\n",
      "  BLEU-4 = 0.0725\n",
      "  ROUGE-L (F1) = 0.3058\n",
      "  CIDEr = 0.4136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:04<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 4.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Validation Metrics:\n",
      "  BLEU-4 = 0.0769\n",
      "  ROUGE-L (F1) = 0.3109\n",
      "  CIDEr = 0.4522\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0812\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Cell 7: Training Loop\n",
    "# -------------------------\n",
    "best_bleu = 0.0\n",
    "EPOCHS=15\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE, DEC_HIDDEN)\n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']  # Extract F1-score for ROUGE-L\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    # ---- Save best model ----\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_1290_hinndi.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T08:50:54.227237Z",
     "iopub.status.busy": "2025-11-16T08:50:54.226542Z",
     "iopub.status.idle": "2025-11-16T08:51:01.629000Z",
     "shell.execute_reply": "2025-11-16T08:51:01.628089Z",
     "shell.execute_reply.started": "2025-11-16T08:50:54.227193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:06<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0699\n",
      "ROUGE-L = 0.2894\n",
      "CIDEr   = 0.3943\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the best saved checkpoint\n",
    "\n",
    "ckpt = torch.load(\"/kaggle/working/best_checkpoint_1290_hinndi.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "# If you used attention, also load it here:\n",
    "# attn_refiner.load_state_dict(ckpt['attn_state'])\n",
    "\n",
    "# =====================================\n",
    "# üîπ Evaluate on test set\n",
    "# =====================================\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "# Extract the ROUGE-L F1 score\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # Avoid warning about tensor creation\n",
    "    if isinstance(video_feat, torch.Tensor):\n",
    "        feat_tensor = video_feat.clone().detach().float().to(device)\n",
    "    else:\n",
    "        feat_tensor = torch.tensor(video_feat, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Ensure correct shape (B, T, D)\n",
    "    if feat_tensor.dim() == 2:\n",
    "        feat_tensor = feat_tensor.unsqueeze(0)   # (1, T, D)\n",
    "    elif feat_tensor.dim() == 4:\n",
    "        feat_tensor = feat_tensor.squeeze(0)     # (B, T, D)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outs, _ = enc(feat_tensor)\n",
    "\n",
    "        # hidden, cell initialized same as training\n",
    "        hidden = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            #  match expected shape for attention\n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word,\n",
    "                hidden.squeeze(0),  # (B, hidden_size)\n",
    "                cell.squeeze(0),    # (B, hidden_size)\n",
    "                encoder_outs\n",
    "            )\n",
    "\n",
    "            # restore shape for next timestep\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            cell = cell.unsqueeze(0)\n",
    "\n",
    "            next_word = out.argmax(1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "def clean_caption(spm_text):\n",
    "   \n",
    "    # Remove SentencePiece underscore markers (replace with space)\n",
    "    text = spm_text.replace(\"‚ñÅ\", \" \").strip()\n",
    "\n",
    "    # Replace <unk> tokens with placeholder or remove\n",
    "    text = text.replace(\"<unk>\", \"\")  # Or use \"[UNK]\" or any custom token\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T08:52:14.012395Z",
     "iopub.status.busy": "2025-11-16T08:52:14.012136Z",
     "iopub.status.idle": "2025-11-16T08:52:14.100342Z",
     "shell.execute_reply": "2025-11-16T08:52:14.099625Z",
     "shell.execute_reply.started": "2025-11-16T08:52:14.012378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video505.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§ï‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§®‡•á ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§á‡§Ç‡§ü‡§∞‡§µ‡•ç‡§Ø‡•Ç ‡§≤‡§ø‡§Ø‡§æ\n",
      "  Ref 2: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§è‡§ï ‡§â‡§§‡•ç‡§∏‡§µ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§Ç‡§¶‡•Ç‡§ï ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video393.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§∏‡•Ç‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§π ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§π‡•ã ‡§≤‡§ø‡§Ø‡§æ ‡§§‡§æ‡§ï‡§ø ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•Ä ‡§π‡§§‡•ç‡§Ø‡§æ ‡§® ‡§ï‡§∞‡•á\n",
      "  Ref 2: ‡§è‡§ï ‡§ú‡§π‡§æ‡§ú‡§º ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡•à‡§†‡•á ‡§è‡§ï ‡§¨‡•Ç‡§¢‡§º‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§è‡§ï ‡§ï‡§æ‡§≤‡•á ‡§ú‡•à‡§ï‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§ñ‡§°‡§º‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "  Ref 3: ‡§¶‡•ã ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§π‡§§‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§™‡§ø‡§§‡§æ ‡§™‡§∞ ‡§Ü‡§∞‡•ã‡§™ ‡§≤‡§ó‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video65.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ó‡§æ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ó‡•Ä‡§§ ‡§ó‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 4: video191.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§ï‡•ã ‡§∏‡§Æ ‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§®‡•á‡§µ‡§æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§∏‡§≠‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡•ç‡§ü‡§æ‡§á‡§≤ ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® - ‡§™‡•ã‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      " Video 5: video939.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡§æ‡§∞ ‡§ï‡§ø‡§®‡§æ‡§∞‡•á ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§¨‡§π‡§æ‡§µ\n",
      "  Ref 2: ‡§¶‡•å‡§°‡§º ‡§ü‡•ç‡§∞‡•à‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§≠‡§ü‡§ï‡§§‡•Ä ‡§π‡•Å‡§à\n",
      "  Ref 3: ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ï‡•ã‡§®‡•á ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¨‡§π‡§æ‡§µ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(70)\n",
    "\n",
    "# select random 5 videos from test_items\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    # Load combined feature for this video\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)  # (1, T, D)\n",
    "\n",
    "    # Generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "\n",
    "\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "\n",
    "    references = test_items[vid]  # list of ground-truth captions\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T16:57:34.194640Z",
     "iopub.status.busy": "2025-11-20T16:57:34.193882Z",
     "iopub.status.idle": "2025-11-20T17:17:25.546847Z",
     "shell.execute_reply": "2025-11-20T17:17:25.545974Z",
     "shell.execute_reply.started": "2025-11-20T16:57:34.194605Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0799\n",
      "  ROUGE-L (F1) = 0.3072\n",
      "  CIDEr = 0.4595\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0736\n",
      "  ROUGE-L (F1) = 0.3062\n",
      "  CIDEr = 0.4286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 4.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0781\n",
      "  ROUGE-L (F1) = 0.3084\n",
      "  CIDEr = 0.4595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 3.9929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0698\n",
      "  ROUGE-L (F1) = 0.3032\n",
      "  CIDEr = 0.4383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 3.9558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0746\n",
      "  ROUGE-L (F1) = 0.3042\n",
      "  CIDEr = 0.4458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 3.9462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0721\n",
      "  ROUGE-L (F1) = 0.3027\n",
      "  CIDEr = 0.4132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 3.8976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0796\n",
      "  ROUGE-L (F1) = 0.3074\n",
      "  CIDEr = 0.4125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 3.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0727\n",
      "  ROUGE-L (F1) = 0.3046\n",
      "  CIDEr = 0.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 3.8556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0690\n",
      "  ROUGE-L (F1) = 0.3001\n",
      "  CIDEr = 0.4141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 3.8379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0720\n",
      "  ROUGE-L (F1) = 0.3034\n",
      "  CIDEr = 0.4269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 3.8030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Metrics:\n",
      "  BLEU-4 = 0.0688\n",
      "  ROUGE-L (F1) = 0.3014\n",
      "  CIDEr = 0.4238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 3.7959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Metrics:\n",
      "  BLEU-4 = 0.0682\n",
      "  ROUGE-L (F1) = 0.3037\n",
      "  CIDEr = 0.4422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 3.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Validation Metrics:\n",
      "  BLEU-4 = 0.0792\n",
      "  ROUGE-L (F1) = 0.3112\n",
      "  CIDEr = 0.4547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 3.7288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Validation Metrics:\n",
      "  BLEU-4 = 0.0726\n",
      "  ROUGE-L (F1) = 0.3050\n",
      "  CIDEr = 0.4191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [01:12<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 3.7049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Validation Metrics:\n",
      "  BLEU-4 = 0.0684\n",
      "  ROUGE-L (F1) = 0.3000\n",
      "  CIDEr = 0.4109\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0799\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Cell 7: Training Loop\n",
    "# -------------------------\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE, DEC_HIDDEN)\n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']  # Extract F1-score for ROUGE-L\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    # ---- Save best model ----\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"best_checkpoint_1290_3f_hindi_lstm-2.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:19:57.829108Z",
     "iopub.status.busy": "2025-11-20T17:19:57.828795Z",
     "iopub.status.idle": "2025-11-20T17:20:04.865306Z",
     "shell.execute_reply": "2025-11-20T17:20:04.864411Z",
     "shell.execute_reply.started": "2025-11-20T17:19:57.829083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0683\n",
      "ROUGE-L = 0.2917\n",
      "CIDEr   = 0.4176\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================================\n",
    "# üîπ Load the best saved checkpoint\n",
    "# =====================================\n",
    "ckpt = torch.load(\"/kaggle/working/best_checkpoint_1290_3f_hindi_lstm-2.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "# If you used attention, also load it here:\n",
    "# attn_refiner.load_state_dict(ckpt['attn_state'])\n",
    "\n",
    "# =====================================\n",
    "# üîπ Evaluate on test set\n",
    "# =====================================\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "# Extract the ROUGE-L F1 score\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # Avoid warning about tensor creation\n",
    "    if isinstance(video_feat, torch.Tensor):\n",
    "        feat_tensor = video_feat.clone().detach().float().to(device)\n",
    "    else:\n",
    "        feat_tensor = torch.tensor(video_feat, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Ensure correct shape (B, T, D)\n",
    "    if feat_tensor.dim() == 2:\n",
    "        feat_tensor = feat_tensor.unsqueeze(0)   # (1, T, D)\n",
    "    elif feat_tensor.dim() == 4:\n",
    "        feat_tensor = feat_tensor.squeeze(0)     # (B, T, D)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outs, _ = enc(feat_tensor)\n",
    "\n",
    "        # hidden, cell initialized same as training\n",
    "        hidden = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            #  match expected shape for attention\n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word,\n",
    "                hidden.squeeze(0),  # (B, hidden_size)\n",
    "                cell.squeeze(0),    # (B, hidden_size)\n",
    "                encoder_outs\n",
    "            )\n",
    "\n",
    "            # restore shape for next timestep\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            cell = cell.unsqueeze(0)\n",
    "\n",
    "            next_word = out.argmax(1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "def clean_caption(spm_text):\n",
    "    \"\"\"\n",
    "    Cleans SentencePiece text output by:\n",
    "    - Removing underscores used as word boundary markers.\n",
    "    - Replacing <unk> tokens with a placeholder or removing them.\n",
    "    - Stripping extra spaces.\n",
    "    \"\"\"\n",
    "    # Remove SentencePiece underscore markers (replace with space)\n",
    "    text = spm_text.replace(\"‚ñÅ\", \" \").strip()\n",
    "\n",
    "    # Replace <unk> tokens with placeholder or remove\n",
    "    text = text.replace(\"<unk>\", \"\")  # Or use \"[UNK]\" or any custom token\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:20:22.519204Z",
     "iopub.status.busy": "2025-11-20T17:20:22.518894Z",
     "iopub.status.idle": "2025-11-20T17:20:22.592222Z",
     "shell.execute_reply": "2025-11-20T17:20:22.591560Z",
     "shell.execute_reply.started": "2025-11-20T17:20:22.519175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video505.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§ï‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§®‡•á ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§á‡§Ç‡§ü‡§∞‡§µ‡•ç‡§Ø‡•Ç ‡§≤‡§ø‡§Ø‡§æ\n",
      "  Ref 2: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§è‡§ï ‡§â‡§§‡•ç‡§∏‡§µ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§Ç‡§¶‡•Ç‡§ï ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video393.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§∏‡•Ç‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§π ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§π‡•ã ‡§≤‡§ø‡§Ø‡§æ ‡§§‡§æ‡§ï‡§ø ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•Ä ‡§π‡§§‡•ç‡§Ø‡§æ ‡§® ‡§ï‡§∞‡•á\n",
      "  Ref 2: ‡§è‡§ï ‡§ú‡§π‡§æ‡§ú‡§º ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡•à‡§†‡•á ‡§è‡§ï ‡§¨‡•Ç‡§¢‡§º‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§è‡§ï ‡§ï‡§æ‡§≤‡•á ‡§ú‡•à‡§ï‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§ñ‡§°‡§º‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "  Ref 3: ‡§¶‡•ã ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§π‡§§‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§™‡§ø‡§§‡§æ ‡§™‡§∞ ‡§Ü‡§∞‡•ã‡§™ ‡§≤‡§ó‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video65.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ó‡§æ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ó‡•Ä‡§§ ‡§ó‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 4: video191.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§®‡•á‡§µ‡§æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§∏‡§≠‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡•ç‡§ü‡§æ‡§á‡§≤ ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® - ‡§™‡•ã‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      " Video 5: video939.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§∏‡§°‡§º‡§ï ‡§™‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡§æ‡§∞ ‡§ï‡§ø‡§®‡§æ‡§∞‡•á ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§¨‡§π‡§æ‡§µ\n",
      "  Ref 2: ‡§¶‡•å‡§°‡§º ‡§ü‡•ç‡§∞‡•à‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§≠‡§ü‡§ï‡§§‡•Ä ‡§π‡•Å‡§à\n",
      "  Ref 3: ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ï‡•ã‡§®‡•á ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¨‡§π‡§æ‡§µ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(70)\n",
    "\n",
    "# select random 5 videos from test_items\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    # Load combined feature for this video\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)  # (1, T, D)\n",
    "\n",
    "    # Generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "\n",
    "\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "\n",
    "    references = test_items[vid]  # list of ground-truth captions\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:50:21.172090Z",
     "iopub.status.busy": "2025-11-20T17:50:21.171560Z",
     "iopub.status.idle": "2025-11-20T17:50:21.983219Z",
     "shell.execute_reply": "2025-11-20T17:50:21.982039Z",
     "shell.execute_reply.started": "2025-11-20T17:50:21.172066Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/645 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/3317632918.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# ---- Training ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} | Train Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/135869985.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_loader, enc, dec, optimizer, criterion, device, clip)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mglobal_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_lens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Move tensors to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mglobal_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# (B, 28, 2048)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Cell 7: Training Loop\n",
    "# -------------------------\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE, DEC_HIDDEN)\n",
    "    rouge_l_f = rouge_scores['rouge-l']['f']  # Extract F1-score for ROUGE-L\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    # ---- Save best model ----\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"checkpoint_1290_3f_hindi_lstm-2-no_bi.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:42:01.999375Z",
     "iopub.status.busy": "2025-11-20T17:42:01.998815Z",
     "iopub.status.idle": "2025-11-20T17:42:08.890937Z",
     "shell.execute_reply": "2025-11-20T17:42:08.889900Z",
     "shell.execute_reply.started": "2025-11-20T17:42:01.999345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0707\n",
      "ROUGE-L = 0.2805\n",
      "CIDEr   = 0.2995\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================================\n",
    "# üîπ Load the best saved checkpoint\n",
    "# =====================================\n",
    "ckpt = torch.load(\"/kaggle/working/checkpoint_1290_3f_hindi_lstm-2-no_bi.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "# If you used attention, also load it here:\n",
    "# attn_refiner.load_state_dict(ckpt['attn_state'])\n",
    "\n",
    "# =====================================\n",
    "# üîπ Evaluate on test set\n",
    "# =====================================\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "# Extract the ROUGE-L F1 score\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=20):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # Avoid warning about tensor creation\n",
    "    if isinstance(video_feat, torch.Tensor):\n",
    "        feat_tensor = video_feat.clone().detach().float().to(device)\n",
    "    else:\n",
    "        feat_tensor = torch.tensor(video_feat, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Ensure correct shape (B, T, D)\n",
    "    if feat_tensor.dim() == 2:\n",
    "        feat_tensor = feat_tensor.unsqueeze(0)   # (1, T, D)\n",
    "    elif feat_tensor.dim() == 4:\n",
    "        feat_tensor = feat_tensor.squeeze(0)     # (B, T, D)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outs, _ = enc(feat_tensor)\n",
    "\n",
    "        # hidden, cell initialized same as training\n",
    "        hidden = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, dec.lstm.hidden_size, device=device)\n",
    "\n",
    "        input_word = torch.LongTensor([vocab.word2idx[vocab.bos_token]]).to(device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            #  match expected shape for attention\n",
    "            out, hidden, cell, attn_weights = dec.forward_step(\n",
    "                input_word,\n",
    "                hidden.squeeze(0),  # (B, hidden_size)\n",
    "                cell.squeeze(0),    # (B, hidden_size)\n",
    "                encoder_outs\n",
    "            )\n",
    "\n",
    "            # restore shape for next timestep\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            cell = cell.unsqueeze(0)\n",
    "\n",
    "            next_word = out.argmax(1).item()\n",
    "            if next_word == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(vocab.idx2word.get(next_word, vocab.unk_token))\n",
    "            input_word = torch.LongTensor([next_word]).to(device)\n",
    "\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "def clean_caption(spm_text):\n",
    "    \"\"\"\n",
    "    Cleans SentencePiece text output by:\n",
    "    - Removing underscores used as word boundary markers.\n",
    "    - Replacing <unk> tokens with a placeholder or removing them.\n",
    "    - Stripping extra spaces.\n",
    "    \"\"\"\n",
    "    # Remove SentencePiece underscore markers (replace with space)\n",
    "    text = spm_text.replace(\"‚ñÅ\", \" \").strip()\n",
    "\n",
    "    # Replace <unk> tokens with placeholder or remove\n",
    "    text = text.replace(\"<unk>\", \"\")  # Or use \"[UNK]\" or any custom token\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:42:38.557546Z",
     "iopub.status.busy": "2025-11-20T17:42:38.557205Z",
     "iopub.status.idle": "2025-11-20T17:42:38.645245Z",
     "shell.execute_reply": "2025-11-20T17:42:38.644605Z",
     "shell.execute_reply.started": "2025-11-20T17:42:38.557514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video505.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§ï‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§®‡•á ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§á‡§Ç‡§ü‡§∞‡§µ‡•ç‡§Ø‡•Ç ‡§≤‡§ø‡§Ø‡§æ\n",
      "  Ref 2: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§è‡§ï ‡§â‡§§‡•ç‡§∏‡§µ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§Ç‡§¶‡•Ç‡§ï ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video393.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§∏‡•Ç‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§π ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§π‡•ã ‡§≤‡§ø‡§Ø‡§æ ‡§§‡§æ‡§ï‡§ø ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•Ä ‡§π‡§§‡•ç‡§Ø‡§æ ‡§® ‡§ï‡§∞‡•á\n",
      "  Ref 2: ‡§è‡§ï ‡§ú‡§π‡§æ‡§ú‡§º ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡•à‡§†‡•á ‡§è‡§ï ‡§¨‡•Ç‡§¢‡§º‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§è‡§ï ‡§ï‡§æ‡§≤‡•á ‡§ú‡•à‡§ï‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§ñ‡§°‡§º‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "  Ref 3: ‡§¶‡•ã ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§π‡§§‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§™‡§ø‡§§‡§æ ‡§™‡§∞ ‡§Ü‡§∞‡•ã‡§™ ‡§≤‡§ó‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video65.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ó‡•Ä‡§§ ‡§ó‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 4: video191.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§®‡•á‡§µ‡§æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§∏‡§≠‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡•ç‡§ü‡§æ‡§á‡§≤ ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® - ‡§™‡•ã‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      " Video 5: video939.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡§æ‡§∞ ‡§ï‡§ø‡§®‡§æ‡§∞‡•á ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§¨‡§π‡§æ‡§µ\n",
      "  Ref 2: ‡§¶‡•å‡§°‡§º ‡§ü‡•ç‡§∞‡•à‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§≠‡§ü‡§ï‡§§‡•Ä ‡§π‡•Å‡§à\n",
      "  Ref 3: ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ï‡•ã‡§®‡•á ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¨‡§π‡§æ‡§µ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(70)\n",
    "\n",
    "# select random 5 videos from test_items\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "    # Load combined feature for this video\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)  # (1, T, D)\n",
    "\n",
    "    # Generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "\n",
    "\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "\n",
    "    references = test_items[vid]  # list of ground-truth captions\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm+transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T10:19:24.830233Z",
     "iopub.status.busy": "2025-11-21T10:19:24.829434Z",
     "iopub.status.idle": "2025-11-21T10:19:24.843602Z",
     "shell.execute_reply": "2025-11-21T10:19:24.843022Z",
     "shell.execute_reply.started": "2025-11-21T10:19:24.830203Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "def evaluate_metrics(loader, enc, dec, vocab, device):\n",
    "    enc.eval(); dec.eval()\n",
    "    \n",
    "    all_refs, all_preds = [], []\n",
    "    pred_sentences, ref_sentences = {}, {}\n",
    "    idx_counter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (global_feats, motion_feats, caps, cap_lens) in tqdm(loader):\n",
    "            global_feats, motion_feats, caps = global_feats.to(device), motion_feats.to(device), caps.to(device)\n",
    "            encoder_outs = enc(global_feats, motion_feats)\n",
    "            batch_size = caps.size(0)\n",
    "\n",
    "            # Greedy decoding\n",
    "            input_seq = torch.full((batch_size, 1),\n",
    "                                   vocab.word2idx[vocab.bos_token],\n",
    "                                   dtype=torch.long, device=device)\n",
    "            preds = [[] for _ in range(batch_size)]\n",
    "            \n",
    "            for _ in range(caps.size(1)):\n",
    "                outputs = dec(encoder_outs, input_seq)\n",
    "                next_word = outputs[:, -1, :].argmax(1)\n",
    "                input_seq = torch.cat([input_seq, next_word.unsqueeze(1)], dim=1)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    preds[i].append(next_word[i].item())\n",
    "            \n",
    "            # Convert tokens to words\n",
    "            for i in range(batch_size):\n",
    "                pred_tokens = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.bos_token], vocab.word2idx[vocab.pad_token]):\n",
    "                        continue\n",
    "                    pred_tokens.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(pred_tokens)\n",
    "                \n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    if tok in (vocab.word2idx[vocab.bos_token], vocab.word2idx[vocab.pad_token]):\n",
    "                        continue\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "                \n",
    "                # For pycocoevalcap\n",
    "                pred_sentences[idx_counter] = [' '.join(pred_tokens)]\n",
    "                ref_sentences[idx_counter] = [' '.join(ref_tokens)]\n",
    "                idx_counter += 1\n",
    "\n",
    "    # ----- Compute BLEU-4 -----\n",
    "    bleu4 = corpus_bleu(all_refs, all_preds, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)\n",
    "\n",
    "    # ----- Compute CIDEr -----\n",
    "    cider_scorer = Cider()\n",
    "    cider_score, _ = cider_scorer.compute_score(ref_sentences, pred_sentences)\n",
    "\n",
    "    # ----- Compute ROUGE-L -----\n",
    "    rouge_scorer = Rouge()\n",
    "    rouge_score, _ = rouge_scorer.compute_score(ref_sentences, pred_sentences)\n",
    "\n",
    "    # ----- Compute METEOR -----\n",
    "    meteor_scorer = Meteor()\n",
    "    meteor_score, _ = meteor_scorer.compute_score(ref_sentences, pred_sentences)\n",
    "\n",
    "    print(f\"BLEU-4: {bleu4:.4f} | CIDEr: {cider_score:.4f} | ROUGE-L: {rouge_score:.4f} | METEOR: {meteor_score:.4f}\")\n",
    "    \n",
    "    return bleu4, cider_score, rouge_score, meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:03:47.045302Z",
     "iopub.status.busy": "2025-11-24T18:03:47.044512Z",
     "iopub.status.idle": "2025-11-24T18:03:47.063518Z",
     "shell.execute_reply": "2025-11-24T18:03:47.062942Z",
     "shell.execute_reply.started": "2025-11-24T18:03:47.045276Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def evaluate_with_metrics(loader, enc, dec, vocab, device):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    all_refs, all_preds = [], []\n",
    "    rouge = Rouge()\n",
    "    cider_scorer = Cider()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, cap_lens in tqdm(loader, desc=\"Evaluating\"):\n",
    "            feats, caps = feats.to(device), caps.to(device)\n",
    "            B = feats.size(0)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Encoder forward\n",
    "            # -----------------------------\n",
    "            encoder_outs, _ = enc(feats)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Transformer: start token\n",
    "            # -----------------------------\n",
    "            input_word = torch.LongTensor(\n",
    "                [vocab.word2idx[vocab.bos_token]] * B\n",
    "            ).to(device)\n",
    "\n",
    "            preds = [[] for _ in range(B)]\n",
    "            max_len = caps.size(1)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Greedy decoding (Transformer)\n",
    "            # -----------------------------\n",
    "            for t in range(1, max_len):\n",
    "\n",
    "                if t == 1:\n",
    "                    inp_seq = input_word.unsqueeze(1)   # (B, 1)\n",
    "                else:\n",
    "                    inp_seq = torch.cat([inp_seq, input_word.unsqueeze(1)], dim=1)\n",
    "\n",
    "                # ---- Transformer forward ----\n",
    "                out = dec(encoder_outs, inp_seq)        # (B, seq_len, vocab)\n",
    "\n",
    "                logits = out[:, -1, :]\n",
    "                top1 = logits.argmax(1)\n",
    "\n",
    "                input_word = top1\n",
    "                for i in range(B):\n",
    "                    preds[i].append(top1[i].item())\n",
    "\n",
    "            # -----------------------------\n",
    "            # Convert predictions ‚Üí words\n",
    "            # -----------------------------\n",
    "            for i in range(B):\n",
    "                pred_tokens = []\n",
    "                for tok in preds[i]:\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token],\n",
    "                               vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    pred_tokens.append(vocab.idx2word.get(tok, vocab.unk_token))\n",
    "                all_preds.append(pred_tokens)\n",
    "\n",
    "                # Reference tokens\n",
    "                ref_tokens = []\n",
    "                for tok in caps[i].cpu().numpy():\n",
    "                    if tok in (vocab.word2idx[vocab.pad_token],\n",
    "                               vocab.word2idx[vocab.bos_token]):\n",
    "                        continue\n",
    "                    if tok == vocab.word2idx[vocab.eos_token]:\n",
    "                        break\n",
    "                    ref_tokens.append(vocab.idx2word.get(int(tok), vocab.unk_token))\n",
    "                all_refs.append([ref_tokens])\n",
    "\n",
    "    # --------------------------------\n",
    "    # Compute BLEU, ROUGE, CIDEr\n",
    "    # --------------------------------\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4 = corpus_bleu(\n",
    "        all_refs,\n",
    "        all_preds,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "\n",
    "    refs_str = [' '.join(ref[0]) for ref in all_refs]\n",
    "    preds_str = [' '.join(pred) for pred in all_preds]\n",
    "\n",
    "    rouge_scores = rouge.get_scores(preds_str, refs_str, avg=True)\n",
    "\n",
    "    if isinstance(rouge_scores, list):\n",
    "        try:\n",
    "            rouge_scores = rouge_scores[0]\n",
    "        except:\n",
    "            rouge_scores = None\n",
    "\n",
    "    if not isinstance(rouge_scores, dict):\n",
    "        rouge_scores = {\n",
    "            \"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0},\n",
    "            \"rouge-2\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0},\n",
    "            \"rouge-l\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0},\n",
    "        }\n",
    "\n",
    "    # CIDEr\n",
    "    cider_score, _ = cider_scorer.compute_score(\n",
    "        {i: [refs_str[i]] for i in range(len(refs_str))},\n",
    "        {i: [preds_str[i]] for i in range(len(preds_str))}\n",
    "    )\n",
    "\n",
    "    return bleu4, rouge_scores, cider_score, all_refs, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T17:44:00.732004Z",
     "iopub.status.busy": "2025-11-21T17:44:00.731554Z",
     "iopub.status.idle": "2025-11-21T18:04:08.655120Z",
     "shell.execute_reply": "2025-11-21T18:04:08.653992Z",
     "shell.execute_reply.started": "2025-11-21T17:44:00.731973Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0783\n",
      "  ROUGE-L (F1) = 0.2967\n",
      "  CIDEr = 0.3798\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 3.8072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0824\n",
      "  ROUGE-L (F1) = 0.3087\n",
      "  CIDEr = 0.4099\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 3.6242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:24<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0702\n",
      "  ROUGE-L (F1) = 0.3024\n",
      "  CIDEr = 0.3622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 3.4794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:24<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0684\n",
      "  ROUGE-L (F1) = 0.2898\n",
      "  CIDEr = 0.3474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 3.3554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0569\n",
      "  ROUGE-L (F1) = 0.2850\n",
      "  CIDEr = 0.3444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 3.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0625\n",
      "  ROUGE-L (F1) = 0.2864\n",
      "  CIDEr = 0.3545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 3.1493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0609\n",
      "  ROUGE-L (F1) = 0.2858\n",
      "  CIDEr = 0.3439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:54<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 3.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0741\n",
      "  ROUGE-L (F1) = 0.2908\n",
      "  CIDEr = 0.3623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:54<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 2.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0532\n",
      "  ROUGE-L (F1) = 0.2675\n",
      "  CIDEr = 0.3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:54<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 2.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0537\n",
      "  ROUGE-L (F1) = 0.2721\n",
      "  CIDEr = 0.2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:54<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 2.8226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Metrics:\n",
      "  BLEU-4 = 0.0611\n",
      "  ROUGE-L (F1) = 0.2767\n",
      "  CIDEr = 0.3162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:54<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 2.7502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Metrics:\n",
      "  BLEU-4 = 0.0606\n",
      "  ROUGE-L (F1) = 0.2688\n",
      "  CIDEr = 0.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:54<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 2.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:23<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Validation Metrics:\n",
      "  BLEU-4 = 0.0513\n",
      "  ROUGE-L (F1) = 0.2735\n",
      "  CIDEr = 0.3426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:55<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 2.6177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:24<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Validation Metrics:\n",
      "  BLEU-4 = 0.0558\n",
      "  ROUGE-L (F1) = 0.2742\n",
      "  CIDEr = 0.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:56<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 2.5532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:24<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Validation Metrics:\n",
      "  BLEU-4 = 0.0513\n",
      "  ROUGE-L (F1) = 0.2639\n",
      "  CIDEr = 0.2936\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0824\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Cell 7: Training Loop\n",
    "# -------------------------\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # SAFE ROUGE EXTRACTION\n",
    "    # -----------------------------------------\n",
    "    try:\n",
    "        rouge_l_f = float(rouge_scores.get(\"rouge-l\", {}).get(\"f\", 0.0))\n",
    "    except Exception:\n",
    "        rouge_l_f = 0.0\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    # ---- Save best model ----\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"checkpoint_1290_3f_hindi_lstm+transformer-5.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:04:34.336196Z",
     "iopub.status.busy": "2025-11-21T18:04:34.335268Z",
     "iopub.status.idle": "2025-11-21T18:04:34.342790Z",
     "shell.execute_reply": "2025-11-21T18:04:34.341814Z",
     "shell.execute_reply.started": "2025-11-21T18:04:34.336164Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=30):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        video_feat = video_feat.unsqueeze(0).to(device)\n",
    "        encoder_outs, _ = enc(video_feat)\n",
    "\n",
    "        generated = torch.LongTensor([[vocab.word2idx[vocab.bos_token]]]).to(device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # ---- FIX: remove tgt_mask argument ----\n",
    "            logits = dec(encoder_outs, generated)\n",
    "            \n",
    "            next_tok = logits[:, -1, :].argmax(-1)\n",
    "\n",
    "            if next_tok.item() == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            generated = torch.cat([generated, next_tok.unsqueeze(0)], dim=1)\n",
    "\n",
    "        # Convert token IDs to words\n",
    "        words = [\n",
    "            vocab.idx2word.get(tok.item(), vocab.unk_token)\n",
    "            for tok in generated[0]\n",
    "            if tok not in (vocab.word2idx[vocab.bos_token], vocab.word2idx[vocab.eos_token])\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:04:47.746321Z",
     "iopub.status.busy": "2025-11-21T18:04:47.745677Z",
     "iopub.status.idle": "2025-11-21T18:05:13.721737Z",
     "shell.execute_reply": "2025-11-21T18:05:13.720834Z",
     "shell.execute_reply.started": "2025-11-21T18:04:47.746298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:24<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0695\n",
      "ROUGE-L = 0.2881\n",
      "CIDEr   = 0.3753\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================================\n",
    "# üîπ Load the best saved checkpoint\n",
    "# =====================================\n",
    "ckpt = torch.load(\"/kaggle/working/checkpoint_1290_3f_hindi_lstm+transformer-5.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "# If you used attention, also load it here:\n",
    "# attn_refiner.load_state_dict(ckpt['attn_state'])\n",
    "\n",
    "# =====================================\n",
    "# üîπ Evaluate on test set\n",
    "# =====================================\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "# Extract the ROUGE-L F1 score\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_caption(spm_text):\n",
    "    \"\"\"\n",
    "    Cleans SentencePiece text output by:\n",
    "    - Removing underscores used as word boundary markers.\n",
    "    - Replacing <unk> tokens with a placeholder or removing them.\n",
    "    - Stripping extra spaces.\n",
    "    \"\"\"\n",
    "    # Remove SentencePiece underscore markers (replace with space)\n",
    "    text = spm_text.replace(\"‚ñÅ\", \" \").strip()\n",
    "\n",
    "    # Replace <unk> tokens with placeholder or remove\n",
    "    text = text.replace(\"<unk>\", \"\")  # Or use \"[UNK]\" or any custom token\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results for Transformer (3L) +bilstm enc+3f**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:43:11.322843Z",
     "iopub.status.busy": "2025-11-21T11:43:11.322272Z",
     "iopub.status.idle": "2025-11-21T11:43:11.669019Z",
     "shell.execute_reply": "2025-11-21T11:43:11.668234Z",
     "shell.execute_reply.started": "2025-11-21T11:43:11.322820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video505.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§ï‡•ã ‡•ç‡§Ø‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§ï‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§®‡•á ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§á‡§Ç‡§ü‡§∞‡§µ‡•ç‡§Ø‡•Ç ‡§≤‡§ø‡§Ø‡§æ\n",
      "  Ref 2: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§è‡§ï ‡§â‡§§‡•ç‡§∏‡§µ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§Ç‡§¶‡•Ç‡§ï ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video393.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§∏‡•Ç‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§π ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§π‡•ã ‡§≤‡§ø‡§Ø‡§æ ‡§§‡§æ‡§ï‡§ø ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•Ä ‡§π‡§§‡•ç‡§Ø‡§æ ‡§® ‡§ï‡§∞‡•á\n",
      "  Ref 2: ‡§è‡§ï ‡§ú‡§π‡§æ‡§ú‡§º ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡•à‡§†‡•á ‡§è‡§ï ‡§¨‡•Ç‡§¢‡§º‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§è‡§ï ‡§ï‡§æ‡§≤‡•á ‡§ú‡•à‡§ï‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§ñ‡§°‡§º‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "  Ref 3: ‡§¶‡•ã ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§π‡§§‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§™‡§ø‡§§‡§æ ‡§™‡§∞ ‡§Ü‡§∞‡•ã‡§™ ‡§≤‡§ó‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video65.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ó‡§æ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ó‡•Ä‡§§ ‡§ó‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 4: video191.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§®‡•á‡§µ‡§æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§∏‡§≠‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡•ç‡§ü‡§æ‡§á‡§≤ ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® - ‡§™‡•ã‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      " Video 5: video939.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡§°‡§º‡§ï ‡§ï‡•á ‡§®‡•Ä‡§ö‡•á ‡§ö‡§≤‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡§æ‡§∞ ‡§ï‡§ø‡§®‡§æ‡§∞‡•á ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§¨‡§π‡§æ‡§µ\n",
      "  Ref 2: ‡§¶‡•å‡§°‡§º ‡§ü‡•ç‡§∞‡•à‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§≠‡§ü‡§ï‡§§‡•Ä ‡§π‡•Å‡§à\n",
      "  Ref 3: ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ï‡•ã‡§®‡•á ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¨‡§π‡§æ‡§µ\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(70)\n",
    "\n",
    "# pick 5 random videos\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=25):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ------- encode --------\n",
    "        if video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(0)        # (1, T, D)\n",
    "        video_feat = video_feat.to(device)\n",
    "\n",
    "        encoder_outs, _ = enc(video_feat)\n",
    "\n",
    "        # ------- start with <bos> -------\n",
    "        input_word = torch.LongTensor(\n",
    "            [vocab.word2idx[vocab.bos_token]]\n",
    "        ).to(device)                                   # (1,)\n",
    "        \n",
    "        # sequence grows autoregressively\n",
    "        inp_seq = input_word.unsqueeze(0)              # (1, 1)\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # decoder forward exactly like evaluation\n",
    "            out = dec(encoder_outs, inp_seq)           # (1, seq_len, vocab)\n",
    "\n",
    "            # take last timestep\n",
    "            logits = out[:, -1, :]\n",
    "            next_tok = logits.argmax(-1).item()\n",
    "\n",
    "            if next_tok == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            # collect token\n",
    "            if next_tok != vocab.word2idx[vocab.bos_token]:\n",
    "                tokens.append(next_tok)\n",
    "\n",
    "            # append next token to growing sequence\n",
    "            next_tok_tensor = torch.LongTensor([[next_tok]]).to(device)   # shape (1,1)\n",
    "            inp_seq = torch.cat([inp_seq, next_tok_tensor], dim=1)\n",
    "\n",
    "        # convert ids ‚Üí words\n",
    "        words = [vocab.idx2word.get(tok, vocab.unk_token) for tok in tokens]\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Print sample predictions for 5 random videos\n",
    "# ----------------------------------------------------------\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    # load feature\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "    # ground truth (list of captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:04:56.275956Z",
     "iopub.status.busy": "2025-11-21T12:04:56.275367Z",
     "iopub.status.idle": "2025-11-21T12:04:56.777907Z",
     "shell.execute_reply": "2025-11-21T12:04:56.777141Z",
     "shell.execute_reply.started": "2025-11-21T12:04:56.275932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video777.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§è‡§ï ‡§™‡§∞ ‡§ö‡§≤ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§®‡§ø‡§∑‡•ç‡§™‡§ï‡•ç‡§∑ ‡§Ö‡§Ç‡§¶‡§æ‡§ú‡§º‡§æ ‡§≤‡§ó‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•â‡§°‡§≤\n",
      "  Ref 2: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§Æ‡•á‡§¢‡§º‡•á‡§Ç ‡§™‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§µ‡§π‡§æ‡§Å ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§® ‡§ï‡•á ‡§∏‡§æ‡§• ‡§â‡§∏ ‡§™‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 2: video1210.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§Æ‡§∂‡•Ä‡§® ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏ ‡§î‡§∞ ‡§≤‡§ï‡§°‡§º‡•Ä ‡§ï‡•á ‡§ü‡•Å‡§ï‡§°‡§º‡•á\n",
      "  Ref 2: ‡§µ‡§π‡§æ‡§Å ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§â‡§™‡§ï‡§∞‡§£ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§≤‡§ï‡§°‡§º‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§æ‡§® ‡§¨‡§®‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ú‡•ã ‡§≤‡§ï‡§°‡§º‡•Ä ‡§∏‡•á ‡§¨‡§®‡§æ ‡§ï‡•Å‡§õ ‡§á‡§ï‡§ü‡•ç‡§†‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§¶‡§Æ ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video285.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ú‡•ã‡§°‡§º‡§æ ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§§‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§∏‡§®‡•ç‡§¶‡•Ç‡§ï‡•Ä ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã\n",
      "  Ref 3: ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§î‡§∞‡§§ ‡§è‡§ï ‡§∏‡§æ‡§• ‡§®‡•É‡§§‡•ç‡§Ø\n",
      "\n",
      " Video 4: video1130.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§Ç‡§ú‡§® ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§∏‡§´‡•á‡§¶ ‡§Æ‡•á‡§ú‡§º ‡§™‡§∞ ‡§µ‡§ø‡§µ‡§ø‡§ß ‡§∏‡§¨‡•ç‡§ú‡§º‡§ø‡§Ø‡§æ‡§Å\n",
      "  Ref 2: ‡§Ö‡§≤‡§ó - ‡§Ö‡§≤‡§ó ‡§§‡§∞‡§π ‡§ï‡•á ‡§ï‡§™‡§°‡§º‡•á ‡§™‡§π‡§®‡•á ‡§π‡•Å‡§è ‡§î‡§∞ ‡§¨‡•á‡§™‡§∞‡•ã‡§Ç ‡§ï‡•Ä ‡§Ö‡§≤‡§ó - ‡§Ö‡§≤‡§ó ‡§ï‡§ø‡§∏‡•ç‡§Æ ‡§ï‡•Ä ‡§ö‡•Ä‡§ú‡§º‡•á‡§Ç ‡§∞‡§ñ‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç\n",
      "  Ref 3: ‡§Ö‡§ó‡§∞ ‡§π‡§Æ ‡§ê‡§∏‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§§‡•ã ‡§π‡§Æ ‡§Ø‡§π‡•ã‡§µ‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§™‡§®‡•á ‡§∞‡§ø‡§∂‡•ç‚Äç‡§§‡•á ‡§ï‡•ã ‡§Æ‡§ú‡§º‡§¨‡•Ç‡§§ ‡§ï‡§∞ ‡§™‡§æ‡§è‡§Å‡§ó‡•á ‡•§\n",
      "\n",
      " Video 5: video198.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§Ø‡§π ‡§ï‡•Å‡§õ ‡§Ø‡•Å‡§µ‡§æ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§è‡§ï ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§π‡•à... ... ‡§î‡§∞ ‡§è‡§ï ‡§∏‡§ú‡•ç‡§ú‡§® ‡§è‡§ï ‡§∏‡•ç‡§µ‡§ö‡§æ‡§≤‡§ø‡§§ ‡§∞‡§ö‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§∏‡§ú‡•ç‡§ú‡§®\n",
      "  Ref 2: ‡§ó‡§æ‡§°‡§º‡•Ä ‡§ï‡•á ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•â‡§®‡§ø‡§ï ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§™‡§∞ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§∏‡§Æ‡•Ç‡§π\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§á‡§Ç‡§ú‡•Ä‡§®‡§ø‡§Ø‡§∞‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡•Ä ‡§∏‡§µ‡§æ‡§∞‡•Ä ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§¨‡§ø‡§∏‡•ç‡§§‡§∞ ‡§™‡§∞\n",
      "\n",
      " Video 6: video191.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§®‡•á‡§µ‡§æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§∏‡§≠‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡•ç‡§ü‡§æ‡§á‡§≤ ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® - ‡§™‡•ã‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      " Video 7: video600.mp4\n",
      " Generated Caption: ‡§¶‡•ã ‡§≤‡•ã‡§ó ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§∂‡•ç‡§§‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§Ø‡•Å. ‡§™‡•Ç.\n",
      "  Ref 2: ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ï‡•Ä ‡§è‡§ï ‡§ï‡§ø‡§§‡§æ‡§¨, ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§≤‡§ø‡§ñ‡§æ ‡§π‡•à ‡§ï‡§ø ‡§π‡§Æ ‡§è‡§ï - ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§§‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 3: ‡§ï‡•Å‡§∂‡•ç‚Äç‡§§‡•Ä ‡§ï‡§æ ‡§è‡§ï ‡§¶‡•É‡§∂‡•ç‚Äç‡§Ø ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à\n",
      "\n",
      " Video 8: video1252.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§æ‡§∏‡•ç‡§ï‡•á‡§ü ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§®‡§Ç‡§¶‡§Æ‡§Ø ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§Ö‡§¶‡§æ‡§≤‡§§ ‡§ï‡•á ‡§™‡§æ‡§∞ ‡§¶‡•å‡§°‡§º‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§è‡§ï ‡§ü‡•ã‡§ï‡§∞‡•Ä ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§ï‡§Æ‡•ç‡§™‡§≤‡•Ä ‡§π‡•Å‡§à ‡§ó‡•á‡§Ç‡§¶ ‡§ï‡•ã ‡§∂‡•Ç‡§ü ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§æ‡§∏‡•ç‡§ï‡•á‡§ü ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§®‡•á ‡§ü‡•ã‡§ï‡§∞‡•Ä ‡§ï‡•ã ‡§ó‡•ã‡§≤‡•Ä ‡§Æ‡§æ‡§∞ ‡§¶‡•ÄName\n",
      "\n",
      " Video 9: video501.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¶‡§ø‡§ñ‡§æ ‡§à ‡§ú‡§æ‡§§ ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§®‡•á ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡§æ ‡§™‡§§‡§æ ‡§≤‡§ó‡§æ‡§Ø‡§æ\n",
      "  Ref 3: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§∏‡§°‡§º‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 10: video1274.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§ö‡•à‡§®‡§≤ ‡§™‡§∞ ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞‡§§ ‡§¶‡§∞‡•ç‡§∂‡§ï‡•ã‡§Ç ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‚Äç‡§® ‡§ï‡§ø‡§è ‡§ú‡§æ ‡§∞‡§π‡•á ‡§è‡§ï ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡•Å‡§§‡§ø ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 2: ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞‡§§\n",
      "  Ref 3: ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ñ‡§°‡§º‡•á ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§è‡§ï ‡§î‡§∞‡§§ ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(90)\n",
    "#34\n",
    "\n",
    "# pick 5 random videos\n",
    "sample_videos = random.sample(list(test_items.keys()), min(10, len(test_items)))\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=25):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ------- encode --------\n",
    "        if video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(0)        # (1, T, D)\n",
    "        video_feat = video_feat.to(device)\n",
    "\n",
    "        encoder_outs, _ = enc(video_feat)\n",
    "\n",
    "        # ------- start with <bos> -------\n",
    "        input_word = torch.LongTensor(\n",
    "            [vocab.word2idx[vocab.bos_token]]\n",
    "        ).to(device)                                   # (1,)\n",
    "        \n",
    "        # sequence grows autoregressively\n",
    "        inp_seq = input_word.unsqueeze(0)              # (1, 1)\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # decoder forward exactly like evaluation\n",
    "            out = dec(encoder_outs, inp_seq)           # (1, seq_len, vocab)\n",
    "\n",
    "            # take last timestep\n",
    "            logits = out[:, -1, :]\n",
    "            next_tok = logits.argmax(-1).item()\n",
    "\n",
    "            if next_tok == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            # collect token\n",
    "            if next_tok != vocab.word2idx[vocab.bos_token]:\n",
    "                tokens.append(next_tok)\n",
    "\n",
    "            # append next token to growing sequence\n",
    "            next_tok_tensor = torch.LongTensor([[next_tok]]).to(device)   # shape (1,1)\n",
    "            inp_seq = torch.cat([inp_seq, next_tok_tensor], dim=1)\n",
    "\n",
    "        # convert ids ‚Üí words\n",
    "        words = [vocab.idx2word.get(tok, vocab.unk_token) for tok in tokens]\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Print sample predictions for 5 random videos\n",
    "# ----------------------------------------------------------\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    # load feature\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "    # ground truth (list of captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:07:43.434561Z",
     "iopub.status.busy": "2025-11-21T12:07:43.434002Z",
     "iopub.status.idle": "2025-11-21T12:07:43.995046Z",
     "shell.execute_reply": "2025-11-21T12:07:43.994382Z",
     "shell.execute_reply.started": "2025-11-21T12:07:43.434535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video696.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ü‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§¶‡•Å‡§ï‡§æ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞\n",
      "  Ref 2: ‡§ï‡§æ‡§∞‡§ñ‡§æ‡§®‡•á ‡§ï‡•á ‡§¶‡•ç‡§∞‡§µ‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞\n",
      "  Ref 3: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video1282.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø ‡§ï‡•á ‡§≤‡§ø‡§è... / ‡§Æ‡•à‡§Ç ... meboboboty ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "  Ref 2: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä Momibot ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 3: ‡§è‡§ï ‡§∞‡•ã‡§¨‡•ã‡§ü ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≠‡•ã‡§ú‡§® ‡§≤‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video958.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•ã ‡§ï‡•Å‡§õ ‡§∏‡§Æ ‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§µ‡§æ‡§™‡§∏ ‡§¨‡•à‡§ó ‡§ï‡•á ‡§µ‡§ú‡§® ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§è‡§ï ‡§î‡§∞‡§§ ‡§´‡§∞‡•ç‡§∂ ‡§™‡§∞ ‡§µ‡§æ‡§™‡§∏ ‡§¨‡•à‡§ó ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 3: ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§∏‡•ç‡§§‡•ç‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§´‡§∞‡•ç‡§∂ ‡§™‡§∞ ‡§¨‡•à‡§†‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§™‡•Å‡§∞‡•Å‡§∑‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§è‡§ï ‡§¨‡•à‡§ó ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      "\n",
      " Video 4: video1258.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§î‡§∞ ‡§ï‡•Å‡§õ ‡§•‡•ã‡§°‡•á ‡§∏‡•á ‡§™‡§ø‡§õ‡§≤‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•á ‡§Æ‡•ã‡§§‡•Ä\n",
      "  Ref 2: ‡§è‡§ï ‡§ñ‡•á‡§≤ ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Æ‡•á‡§ú‡§¨‡§æ‡§® ‡§π‡§Ç‡§∏‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§¨‡•Å‡§∞‡•Ä ‡§§‡§∞‡§π ‡§∏‡•á ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§∏‡§Æ‡•Ç‡§π ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ï‡•Å‡§õ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      "\n",
      " Video 5: video457.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§è‡§ï ‡•Ç‡§∏‡§∞ ‡§™‡§∏‡•Ä‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§¨‡•à ‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à ‡§î‡§∞ ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§ï‡•ã ‡§è‡§ï ‡§ö ‡§º ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡§æ ‡§¶‡•É‡§∂‡•ç‚Äç‡§Ø ‡§ú‡•ã ‡§π‡§∞‡•á - ‡§≠‡§∞‡•á ‡§∏‡•Ç‡§ü ‡§™‡§π‡§®‡•á ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡•ã ‡§∏‡§ø‡§ó‡§∞‡•á‡§ü ‡§™‡•Ä‡§®‡•á ‡§ï‡§æ ‡§¨‡§¢‡§º‡§æ‡§µ‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§ï‡§æ ‡§¶‡•É‡§∂‡•ç‚Äç‡§Ø ‡§ú‡•ã ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡•ã ‡§π‡§∞‡•á - ‡§≠‡§∞‡•á ‡§ï‡§™‡§°‡§º‡•á ‡§Æ‡•á‡§Ç ‡§ß‡•Ç‡§Æ‡•ç‡§∞‡§™‡§æ‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§¨‡§æ‡§°‡§º ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§¨‡•à‡§†‡§ï‡§∞ ‡§∏‡§ø‡§ó‡§∞‡•á‡§ü ‡§™‡•Ä ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 6: video1206.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§ï‡§æ‡§ó‡§ú‡§º ‡§™‡§∞ ‡§è‡§ï ‡§π‡§æ‡§• ‡§∞‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•à‡§∏‡•á ‡§°‡•ç‡§∞‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§è\n",
      "  Ref 2: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ö‡§ø‡§§‡•ç‡§∞ ‡§ñ‡•Ä‡§Ç‡§ö‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ö‡§ø‡§§‡•ç‡§∞ ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡§æ‡§ó‡§ú ‡§ï‡•á ‡§ü‡•Å‡§ï‡§°‡§º‡•á ‡§™‡§∞ ‡§Ü‡§∞‡•á‡§ñ‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 7: video740.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ï‡•Å‡§õ ‡§∏‡§Æ ‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§¨‡§°‡§º‡§æ ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§è‡§ï ‡§ñ‡•á‡§≤ ‡§Æ‡•á‡§Ç ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§ó‡§°‡§º‡§ï ‡§∏‡§ø‡§∞ ‡§¶‡§∞‡•ç‡§¶ ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§â‡§°‡§º‡§æ‡§§‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§∏‡§Æ‡•Ç‡§π ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 8: video209.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•ã ‡§ï‡•Å‡§õ ‡§∏‡§Æ ‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ö‡§™‡§®‡•á ‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Ç‡§Ü‡§§ ‡§∏‡•ç‡§µ‡§Ç‡§∂ ‡§ú‡•Ç‡§§‡•á ‡§¨‡§®‡§æ‡§®‡•á ‡§™‡§∞ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§á‡§≤‡§æ‡§ú ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§™‡§®‡•á ‡§ï‡§ü‡•å‡§§‡•Ä ‡§ú‡•Ç‡§§‡•á ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡§æ ‡§°‡•á‡§Æ‡•ã ‡§¶‡•á‡§§‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä‡§®‡•Ä ‡§ú‡•Ç‡§§‡•á ‡§ß‡§æ‡§∞‡§£ ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 9: video778.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§´‡•Å‡§ü‡§¨ ‡§≤ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§è‡§ï ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§´‡•Å‡§ü‡§¨‡•â‡§≤ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§î‡§∞ ‡§è‡§ï ‡§ï‡•ã‡§≤ ‡§π‡§æ‡§• ‡§Æ‡§ø‡§≤‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä\n",
      "  Ref 2: ‡§è‡§ï ‡§´‡•Å‡§ü‡§¨‡•â‡§≤ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§ó‡•á‡§Ç‡§¶ ‡§´‡•á‡§Ç‡§ï ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§ó‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 10: video1286.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞‡•ç‡§ü‡•Ç‡§® ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡§æ ‡§î‡§∞ ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ö‡§≤‡§®‡§æ\n",
      "  Ref 2: ‡§ï‡•â‡§≤‡•á‡§ú ‡§≤‡§°‡§º‡§ï‡•Ä ‡§™‡•á‡§Ç‡§ü‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§ï‡§æ‡§ó‡§ú‡•ã‡§Ç ‡§ï‡•ã ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞‡§§ ‡§è‡§ï ‡§¨‡§ø‡§∏‡•ç‡§§‡§∞ ‡§∏‡•á ‡§®‡•Ä‡§ö‡•á ‡§ö‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "#34\n",
    "\n",
    "# pick 5 random videos\n",
    "sample_videos = random.sample(list(test_items.keys()), min(10, len(test_items)))\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    # load feature\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "    # ground truth (list of captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result of Transformer(5L)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(70)\n",
    "\n",
    "# pick 5 random videos\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=25):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ------- encode --------\n",
    "        if video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(0)        # (1, T, D)\n",
    "        video_feat = video_feat.to(device)\n",
    "\n",
    "        encoder_outs, _ = enc(video_feat)\n",
    "\n",
    "        # ------- start with <bos> -------\n",
    "        input_word = torch.LongTensor(\n",
    "            [vocab.word2idx[vocab.bos_token]]\n",
    "        ).to(device)                                   # (1,)\n",
    "        \n",
    "        # sequence grows autoregressively\n",
    "        inp_seq = input_word.unsqueeze(0)              # (1, 1)\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # decoder forward exactly like evaluation\n",
    "            out = dec(encoder_outs, inp_seq)           # (1, seq_len, vocab)\n",
    "\n",
    "            # take last timestep\n",
    "            logits = out[:, -1, :]\n",
    "            next_tok = logits.argmax(-1).item()\n",
    "\n",
    "            if next_tok == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            # collect token\n",
    "            if next_tok != vocab.word2idx[vocab.bos_token]:\n",
    "                tokens.append(next_tok)\n",
    "\n",
    "            # append next token to growing sequence\n",
    "            next_tok_tensor = torch.LongTensor([[next_tok]]).to(device)   # shape (1,1)\n",
    "            inp_seq = torch.cat([inp_seq, next_tok_tensor], dim=1)\n",
    "\n",
    "        # convert ids ‚Üí words\n",
    "        words = [vocab.idx2word.get(tok, vocab.unk_token) for tok in tokens]\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Print sample predictions for 5 random videos\n",
    "# ----------------------------------------------------------\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    # load feature\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "    # ground truth (list of captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:27:34.072597Z",
     "iopub.status.busy": "2025-11-21T18:27:34.071897Z",
     "iopub.status.idle": "2025-11-21T18:27:34.772421Z",
     "shell.execute_reply": "2025-11-21T18:27:34.771651Z",
     "shell.execute_reply.started": "2025-11-21T18:27:34.072572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video741.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡•ã‡§°‡§º‡•á ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§¨‡§æ‡§¶‡§≤ ‡§î‡§∞ ‡§™‡§æ‡§®‡•Ä ‡§¨‡§°‡§º‡•Ä ‡§§‡•á‡§ú‡§º‡•Ä ‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 2: ‡§ï‡•Å‡§õ ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡•á ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§¶‡•É‡§∂‡•ç‚Äç‡§Ø ‡§ï‡§æ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§¶‡§ø‡§ñ‡§æ‡§§‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 3: ‡§µ‡§π‡§æ‡§Å ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§Ü‡§ó ‡§Æ‡•á‡§Ç ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video563.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ï‡•ã ‡§è‡§ï ‡§æ‡§∞ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ü‡•ã‡§≤‡•Ä ‡§ó‡•Ä‡§§ ‡§ï‡•ã ‡§Ö‡§≤‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡§æ ‡§ö‡•Ä‡§ú‡§º‡•á‡§Ç ‡§≤‡•á‡§ï‡§∞ ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡§æ ‡§è‡§ï ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ñ‡•á‡§≤‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video435.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§µ‡§π‡§æ‡§Å ‡§è‡§ï ‡§ü‡•Ä‡§≤‡§æ ‡§≤‡§°‡§º‡§ï‡§æ ‡§ï‡§Æ‡§∞‡•á ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§õ‡•ã‡§ü‡§æ ‡§≤‡§°‡§º‡§ï‡§æ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§™‡§∞ ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§¨‡•à‡§†‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§ï‡§∞‡•Ä‡§¨ ‡§õ‡§É ‡§∏‡§æ‡§≤ ‡§ï‡§æ ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡§æ ‡§ï‡•à‡§Æ‡§∞‡§æ ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§®‡§æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ö‡§™‡§®‡•Ä ‡§Æ‡§æ‡§Å ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•Å‡§®‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 4: video326.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§è‡§ï ‡§ó‡•Ä‡§§ ‡§ó‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§¶‡§≤ ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§¶‡§≤ ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§¶‡§≤ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 5: video327.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§∞ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§∂‡•ã ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•àName\n",
      "  Ref 2: ‡§è‡§ï ‡§∂‡§π‡§∞ ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§î‡§∞ ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§≠‡•Ä‡§°‡§º ‡§ï‡•Ä ‡§¢‡•á‡§∞ ‡§∏‡§æ‡§∞‡§æ ‡§∂‡•ç‡§∞‡•á‡§Ø ‡§ï‡•á ‡§∏‡§æ‡§•\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ò‡•ã‡§°‡§º‡•á ‡§ï‡•Ä ‡§Æ‡§æ‡§∏‡•ç‡§ï ‡§ñ‡§°‡§º‡§æ ‡§™‡§π‡§®‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä\n",
      "\n",
      " Video 6: video1103.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§´‡•Å‡§ü‡§¨ ‡§≤ ‡§ñ‡•á‡§≤ ‡§Æ‡•á‡§Ç ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§ñ‡§æ‡§®‡§æ ‡§™‡§ï‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§Ü‡§¶‡§Æ‡•Ä\n",
      "  Ref 2: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§´‡•Å‡§ü‡§¨‡•â‡§≤ ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§Ö‡§™‡§®‡•á ‡§ü‡•Ä‡§Æ ‡§∏‡§æ‡§•‡•Ä ‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§∏‡§æ‡§• ‡§´‡•Å‡§ü‡§¨‡•â‡§≤ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§ñ‡•ã ‡§¨‡•à‡§†‡§æ\n",
      "\n",
      " Video 7: video130.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§ñ‡•á‡§≤ ‡§ñ‡•á‡§≤‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§ñ‡•á‡§≤ ‡§Æ‡•á‡§Ç ‡§¨‡§∞‡•ç‡§´ ‡§™‡§∞ ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§ö‡§≤‡§§‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§∞‡•ç‡§´ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§ñ‡•á‡§≤ ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 8: video1133.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ó‡§æ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§ó‡§æ‡§Ø‡§® ‡§ï‡§æ ‡§ï‡•ç‡§≤‡§ø‡§™\n",
      "  Ref 2: ‡§ó‡•Ä‡§§ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 9: video758.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: 3 ‡§≤‡•ã‡§ó ‡§ï‡•Å‡§õ ‡§™‡§∞ ‡§ó‡•Å‡§∏‡•ç‡§∏‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 2: ‡§Æ‡•á‡§ú‡§º ‡§™‡§∞ ‡§Æ‡§®‡•Å‡§∑‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§∏‡§Æ‡•Ç‡§π ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§Ç‡§ú‡§® ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§™‡•ç‡§≤‡•á‡§ü ‡§∏‡•á ‡§ï‡§µ‡§∞ ‡§π‡•à\n",
      "\n",
      " Video 10: video1209.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä ‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ù‡•Ä‡§≤ ‡§ï‡•á ‡§ï‡§ø‡§®‡§æ‡§∞‡•á ‡§ñ‡§°‡§º‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§™‡§æ‡§∏ ‡§è‡§ï ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§î‡§∞‡§§ ‡§¨‡§æ‡§π‡§∞ ‡§ï‡•Å‡§õ ‡§¨‡§æ‡§§ ‡§™‡§∞ ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 3: ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ú‡•ã ‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(450)\n",
    "\n",
    "# pick 5 random videos\n",
    "sample_videos = random.sample(list(test_items.keys()), min(10, len(test_items)))\n",
    "\n",
    "\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    # load feature\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "    # ground truth (list of captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:04:54.427087Z",
     "iopub.status.busy": "2025-11-24T18:04:54.426426Z",
     "iopub.status.idle": "2025-11-24T18:04:54.433716Z",
     "shell.execute_reply": "2025-11-24T18:04:54.432760Z",
     "shell.execute_reply.started": "2025-11-24T18:04:54.427054Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, encoder, decoder, optimizer, criterion, device, clip=5.0):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for feats, caps, cap_lens in tqdm(train_loader):\n",
    "        feats = feats.to(device)          # (B, T_enc, D)\n",
    "        caps = caps.to(device)            # (B, T_dec)\n",
    "\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ------------------------\n",
    "        # 1. ENCODER\n",
    "        # ------------------------\n",
    "        enc_out, _ = encoder(feats)       # (B, T_enc, enc_dim)\n",
    "\n",
    "        # ------------------------\n",
    "        # 2. MASKS FOR TRANSFORMER\n",
    "        # ------------------------\n",
    "        T_dec = caps.size(1)\n",
    "        tgt_mask = decoder._generate_square_subsequent_mask(T_dec, device)\n",
    "\n",
    "\n",
    "        # if no padding in encoder features:\n",
    "        memory_key_padding_mask = None\n",
    "\n",
    "        # ------------------------\n",
    "        # 3. DECODER (CROSS-ATTN)\n",
    "        # ------------------------\n",
    "        outputs = decoder(\n",
    "            enc_out,\n",
    "            caps,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )                                  # (B, T_dec, V)\n",
    "\n",
    "        # ------------------------\n",
    "        # 4. SHIFT OUTPUTS / TARGETS\n",
    "        # ------------------------\n",
    "        outputs = outputs[:, :-1, :].contiguous()  # predict next token\n",
    "        targets = caps[:, 1:].contiguous()\n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.size(-1)),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        # ------------------------\n",
    "        # 5. BACKPROP\n",
    "        # ------------------------\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:18:09.734665Z",
     "iopub.status.busy": "2025-11-22T12:18:09.733859Z",
     "iopub.status.idle": "2025-11-22T12:36:31.794311Z",
     "shell.execute_reply": "2025-11-22T12:36:31.793342Z",
     "shell.execute_reply.started": "2025-11-22T12:18:09.734639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.7243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Metrics:\n",
      "  BLEU-4 = 0.0646\n",
      "  ROUGE-L (F1) = 0.2854\n",
      "  CIDEr = 0.2691\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Metrics:\n",
      "  BLEU-4 = 0.0729\n",
      "  ROUGE-L (F1) = 0.2867\n",
      "  CIDEr = 0.3277\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 3.7683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Metrics:\n",
      "  BLEU-4 = 0.0800\n",
      "  ROUGE-L (F1) = 0.2991\n",
      "  CIDEr = 0.4264\n",
      " Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 3.5865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Metrics:\n",
      "  BLEU-4 = 0.0773\n",
      "  ROUGE-L (F1) = 0.2943\n",
      "  CIDEr = 0.3704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 3.4431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Metrics:\n",
      "  BLEU-4 = 0.0750\n",
      "  ROUGE-L (F1) = 0.3018\n",
      "  CIDEr = 0.4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:49<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 3.3197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Metrics:\n",
      "  BLEU-4 = 0.0739\n",
      "  ROUGE-L (F1) = 0.2986\n",
      "  CIDEr = 0.3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 3.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Metrics:\n",
      "  BLEU-4 = 0.0694\n",
      "  ROUGE-L (F1) = 0.2881\n",
      "  CIDEr = 0.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:49<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 3.1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Metrics:\n",
      "  BLEU-4 = 0.0790\n",
      "  ROUGE-L (F1) = 0.2991\n",
      "  CIDEr = 0.3964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 3.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Metrics:\n",
      "  BLEU-4 = 0.0615\n",
      "  ROUGE-L (F1) = 0.2760\n",
      "  CIDEr = 0.3724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 2.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Metrics:\n",
      "  BLEU-4 = 0.0670\n",
      "  ROUGE-L (F1) = 0.2850\n",
      "  CIDEr = 0.3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 2.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Metrics:\n",
      "  BLEU-4 = 0.0638\n",
      "  ROUGE-L (F1) = 0.2796\n",
      "  CIDEr = 0.3645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 2.7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Metrics:\n",
      "  BLEU-4 = 0.0711\n",
      "  ROUGE-L (F1) = 0.2930\n",
      "  CIDEr = 0.3984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 2.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Validation Metrics:\n",
      "  BLEU-4 = 0.0617\n",
      "  ROUGE-L (F1) = 0.2768\n",
      "  CIDEr = 0.3383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 2.5970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Validation Metrics:\n",
      "  BLEU-4 = 0.0661\n",
      "  ROUGE-L (F1) = 0.2852\n",
      "  CIDEr = 0.3513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645/645 [00:50<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 2.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Validation Metrics:\n",
      "  BLEU-4 = 0.0553\n",
      "  ROUGE-L (F1) = 0.2744\n",
      "  CIDEr = 0.3505\n",
      "\n",
      "Training complete. Best BLEU-4 achieved: 0.0800\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Cell 7: Training Loop\n",
    "# -------------------------\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    # ---- Training ----\n",
    "    train_loss = train_one_epoch(train_loader, enc, dec, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    bleu, rouge_scores, cider, _, _ = evaluate_with_metrics(val_loader, enc, dec, vocab, DEVICE)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # SAFE ROUGE EXTRACTION\n",
    "    # -----------------------------------------\n",
    "    try:\n",
    "        rouge_l_f = float(rouge_scores.get(\"rouge-l\", {}).get(\"f\", 0.0))\n",
    "    except Exception:\n",
    "        rouge_l_f = 0.0\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Validation Metrics:\")\n",
    "    print(f\"  BLEU-4 = {bleu:.4f}\")\n",
    "    print(f\"  ROUGE-L (F1) = {rouge_l_f:.4f}\")\n",
    "    print(f\"  CIDEr = {cider:.4f}\")\n",
    "    \n",
    "    # ---- Save best model ----\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({\n",
    "            'enc_state': enc.state_dict(),\n",
    "            'dec_state': dec.state_dict(),\n",
    "            'vocab': vocab.word2idx\n",
    "        }, \"checkpoint_1290_3f_hindi_lstm+transformer+attention_4L.pth\")\n",
    "        print(\" Saved new best checkpoint.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best BLEU-4 achieved: {best_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:36:54.986266Z",
     "iopub.status.busy": "2025-11-22T12:36:54.985941Z",
     "iopub.status.idle": "2025-11-22T12:37:18.189405Z",
     "shell.execute_reply": "2025-11-22T12:37:18.188494Z",
     "shell.execute_reply.started": "2025-11-22T12:36:54.986231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint successfully.\n",
      "Running test evaluation (BLEU, ROUGE, CIDEr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics:\n",
      "BLEU-4  = 0.0587\n",
      "ROUGE-L = 0.2761\n",
      "CIDEr   = 0.3701\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================================\n",
    "# üîπ Load the best saved checkpoint\n",
    "# =====================================\n",
    "ckpt = torch.load(\"/kaggle/working/checkpoint_1290_3f_hindi_lstm+transformer+attention_4L.pth\", map_location=DEVICE)\n",
    "enc.load_state_dict(ckpt['enc_state'])\n",
    "dec.load_state_dict(ckpt['dec_state'])\n",
    "print(\" Loaded checkpoint successfully.\")\n",
    "\n",
    "# If you used attention, also load it here:\n",
    "# attn_refiner.load_state_dict(ckpt['attn_state'])\n",
    "\n",
    "# =====================================\n",
    "# üîπ Evaluate on test set\n",
    "# =====================================\n",
    "print(\"Running test evaluation (BLEU, ROUGE, CIDEr)...\")\n",
    "test_bleu, test_rouge, test_cider ,_,_= evaluate_with_metrics(test_loader, enc, dec, vocab, DEVICE)\n",
    "# Extract the ROUGE-L F1 score\n",
    "test_rouge_l = test_rouge['rouge-l']['f']\n",
    "\n",
    "print(f\"\\nüìä Test Metrics:\")\n",
    "print(f\"BLEU-4  = {test_bleu:.4f}\")\n",
    "print(f\"ROUGE-L = {test_rouge_l:.4f}\")\n",
    "print(f\"CIDEr   = {test_cider:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_caption(spm_text):\n",
    "   \n",
    "    # Remove SentencePiece underscore markers (replace with space)\n",
    "    text = spm_text.replace(\"‚ñÅ\", \" \").strip()\n",
    "\n",
    "    # Replace <unk> tokens with placeholder or remove\n",
    "    text = text.replace(\"<unk>\", \"\")  # Or use \"[UNK]\" or any custom token\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:37:18.772952Z",
     "iopub.status.busy": "2025-11-22T12:37:18.772710Z",
     "iopub.status.idle": "2025-11-22T12:37:19.067552Z",
     "shell.execute_reply": "2025-11-22T12:37:19.066876Z",
     "shell.execute_reply.started": "2025-11-22T12:37:18.772927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video505.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡•Ç‡§∏‡§∞ ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§ï‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§®‡•á ‡§è‡§ï ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§á‡§Ç‡§ü‡§∞‡§µ‡•ç‡§Ø‡•Ç ‡§≤‡§ø‡§Ø‡§æ\n",
      "  Ref 2: ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‚Äç‡§§‡§ø ‡§è‡§ï ‡§â‡§§‡•ç‡§∏‡§µ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¨‡§Ç‡§¶‡•Ç‡§ï ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video393.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§∏‡•Ç‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§π ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§π‡•ã ‡§≤‡§ø‡§Ø‡§æ ‡§§‡§æ‡§ï‡§ø ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•Ä ‡§π‡§§‡•ç‡§Ø‡§æ ‡§® ‡§ï‡§∞‡•á\n",
      "  Ref 2: ‡§è‡§ï ‡§ú‡§π‡§æ‡§ú‡§º ‡§∏‡•Ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡•à‡§†‡•á ‡§è‡§ï ‡§¨‡•Ç‡§¢‡§º‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§è‡§ï ‡§ï‡§æ‡§≤‡•á ‡§ú‡•à‡§ï‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§ñ‡§°‡§º‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "  Ref 3: ‡§¶‡•ã ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§π‡§§‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§™‡§ø‡§§‡§æ ‡§™‡§∞ ‡§Ü‡§∞‡•ã‡§™ ‡§≤‡§ó‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à\n",
      "\n",
      " Video 3: video65.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§¶‡§≤ ‡§Æ‡§Ç‡§ö ‡§™‡§∞ ‡§ó‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ó‡•Ä‡§§ ‡§ó‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 4: video191.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π ‡•Ä ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§®‡•á‡§µ‡§æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§≤ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§∏‡§≠‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡•ç‡§ü‡§æ‡§á‡§≤ ‡§è‡§ï ‡§´‡•à‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§æ‡§≤‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® - ‡§™‡•ã‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§∞‡§π ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      " Video 5: video939.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ö‡§≤‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§ï‡§æ‡§∞ ‡§ï‡§ø‡§®‡§æ‡§∞‡•á ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§¨‡§π‡§æ‡§µ\n",
      "  Ref 2: ‡§¶‡•å‡§°‡§º ‡§ü‡•ç‡§∞‡•à‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§≠‡§ü‡§ï‡§§‡•Ä ‡§π‡•Å‡§à\n",
      "  Ref 3: ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ï‡•ã‡§®‡•á ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§æ‡§∞ ‡§¨‡§π‡§æ‡§µ\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(70)\n",
    "\n",
    "# pick 5 random videos\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=25):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ------- encode --------\n",
    "        if video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(0)        # (1, T, D)\n",
    "        video_feat = video_feat.to(device)\n",
    "\n",
    "        encoder_outs, _ = enc(video_feat)\n",
    "\n",
    "        # ------- start with <bos> -------\n",
    "        input_word = torch.LongTensor(\n",
    "            [vocab.word2idx[vocab.bos_token]]\n",
    "        ).to(device)                                   # (1,)\n",
    "        \n",
    "        # sequence grows autoregressively\n",
    "        inp_seq = input_word.unsqueeze(0)              # (1, 1)\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # decoder forward exactly like evaluation\n",
    "            out = dec(encoder_outs, inp_seq)           # (1, seq_len, vocab)\n",
    "\n",
    "            # take last timestep\n",
    "            logits = out[:, -1, :]\n",
    "            next_tok = logits.argmax(-1).item()\n",
    "\n",
    "            if next_tok == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            # collect token\n",
    "            if next_tok != vocab.word2idx[vocab.bos_token]:\n",
    "                tokens.append(next_tok)\n",
    "\n",
    "            # append next token to growing sequence\n",
    "            next_tok_tensor = torch.LongTensor([[next_tok]]).to(device)   # shape (1,1)\n",
    "            inp_seq = torch.cat([inp_seq, next_tok_tensor], dim=1)\n",
    "\n",
    "        # convert ids ‚Üí words\n",
    "        words = [vocab.idx2word.get(tok, vocab.unk_token) for tok in tokens]\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Print sample predictions for 5 random videos\n",
    "# ----------------------------------------------------------\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    # load feature\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "    # ground truth (list of captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T12:46:02.086401Z",
     "iopub.status.busy": "2025-11-22T12:46:02.085731Z",
     "iopub.status.idle": "2025-11-22T12:46:02.398202Z",
     "shell.execute_reply": "2025-11-22T12:46:02.397625Z",
     "shell.execute_reply.started": "2025-11-22T12:46:02.086375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video 1: video778.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§´‡•Å‡§ü‡§¨ ‡§≤ ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§´‡•Å‡§ü‡§¨‡•â‡§≤ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§î‡§∞ ‡§è‡§ï ‡§ï‡•ã‡§≤ ‡§π‡§æ‡§• ‡§Æ‡§ø‡§≤‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä\n",
      "  Ref 2: ‡§è‡§ï ‡§´‡•Å‡§ü‡§¨‡•â‡§≤ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§ó‡•á‡§Ç‡§¶ ‡§´‡•á‡§Ç‡§ï ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§ó‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 2: video1133.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§î‡§∞‡§§ ‡§ï‡•á ‡§¨‡§æ‡§∞ ‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§î‡§∞‡§§ ‡§ó‡§æ‡§Ø‡§® ‡§ï‡§æ ‡§ï‡•ç‡§≤‡§ø‡§™\n",
      "  Ref 2: ‡§ó‡•Ä‡§§ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä\n",
      "  Ref 3: ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§∞ ‡§ó‡•Ä‡§§ ‡§∏‡•Å‡§® ‡§∞‡§π‡•Ä ‡§π‡•à\n",
      "\n",
      " Video 3: video146.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§õ‡•ã‡§ü ‡§æ ‡§∏‡§æ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§è‡§ï ‡§õ‡•ã‡§ü ‡•Ä ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§™‡•à‡§∞‡§æ ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡§¨‡§ï‡§ø ‡§è‡§ï ‡§î‡§∞ ‡§™‡•à‡§∞‡•ã ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§è‡§ï ‡§™‡•à‡§∞‡§ø‡§Ø‡§æ‡§∞‡•Ä ‡§®‡•á ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•ã ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡§ú‡§¨‡•Ç‡§∞ ‡§ï‡§ø‡§Ø‡§æ ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§è‡§ï ‡§î‡§∞ ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•ã ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ\n",
      "  Ref 3: ‡§è‡§ï ‡§™‡•à‡§∞‡•ã ‡§Ü‡§¶‡§Æ‡•Ä ‡§§‡§Ç‡§ó ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "\n",
      " Video 4: video865.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡•ã‡§°‡§º‡•á ‡§™‡§∞ ‡§∏‡§µ‡§æ‡§∞ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§è‡§ï ‡§≠‡•Ä‡§°‡§º ‡§ó‡§æ‡§§‡§æ ‡§π‡•à\n",
      "  Ref 2: ‡§´‡•ç‡§∞‡•á‡§ö ‡§ï‡•ç‡§∞‡§æ‡§Ç‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§∏‡§Æ‡•Ç‡§π ‡§ó‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 3: ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•Ä ‡§è‡§ï ‡§¨‡§°‡§º‡•Ä ‡§≠‡•Ä‡§°‡§º ‡§∂‡•ã‡§∞ ‡§ó‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§π‡§µ‡§æ ‡§Æ‡•á‡§Ç ‡§≤‡§π‡§∞ ‡§ù‡§Ç‡§°‡•á\n",
      "\n",
      " Video 5: video1283.mp4\n",
      " Generated Caption: ‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞ ‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à\n",
      " Reference Captions:\n",
      "  Ref 1: ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§î‡§∞ ‡§™‡§ï‡•ç‡§∑‡•Ä ‡§≤‡§°‡§º ‡§∞‡§π‡•á ‡§π‡•à‡§Ç\n",
      "  Ref 2: ‡§è‡§ï ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ú‡§º‡§Æ‡•Ä‡§® ‡§™‡§∞ ‡§Æ‡•Å‡§∞‡•ç‡§ó‡•á ‡§ï‡•á ‡§∏‡§æ‡§• ‡§≤‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à\n",
      "  Ref 3: ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§¨‡•à‡§†‡§æ ‡§π‡•à\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(34)\n",
    "\n",
    "# pick 5 random videos\n",
    "sample_videos = random.sample(list(test_items.keys()), min(5, len(test_items)))\n",
    "\n",
    "\n",
    "def generate_caption_for_video(video_feat, enc, dec, vocab, device, max_len=25):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ------- encode --------\n",
    "        if video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(0)        # (1, T, D)\n",
    "        video_feat = video_feat.to(device)\n",
    "\n",
    "        encoder_outs, _ = enc(video_feat)\n",
    "\n",
    "        # ------- start with <bos> -------\n",
    "        input_word = torch.LongTensor(\n",
    "            [vocab.word2idx[vocab.bos_token]]\n",
    "        ).to(device)                                   # (1,)\n",
    "        \n",
    "        # sequence grows autoregressively\n",
    "        inp_seq = input_word.unsqueeze(0)              # (1, 1)\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # decoder forward exactly like evaluation\n",
    "            out = dec(encoder_outs, inp_seq)           # (1, seq_len, vocab)\n",
    "\n",
    "            # take last timestep\n",
    "            logits = out[:, -1, :]\n",
    "            next_tok = logits.argmax(-1).item()\n",
    "\n",
    "            if next_tok == vocab.word2idx[vocab.eos_token]:\n",
    "                break\n",
    "\n",
    "            # collect token\n",
    "            if next_tok != vocab.word2idx[vocab.bos_token]:\n",
    "                tokens.append(next_tok)\n",
    "\n",
    "            # append next token to growing sequence\n",
    "            next_tok_tensor = torch.LongTensor([[next_tok]]).to(device)   # shape (1,1)\n",
    "            inp_seq = torch.cat([inp_seq, next_tok_tensor], dim=1)\n",
    "\n",
    "        # convert ids ‚Üí words\n",
    "        words = [vocab.idx2word.get(tok, vocab.unk_token) for tok in tokens]\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Print sample predictions for 5 random videos\n",
    "# ----------------------------------------------------------\n",
    "for i, vid in enumerate(sample_videos):\n",
    "\n",
    "    # load feature\n",
    "    sample_feat = load_combined_features(vid, sample_frames=16)\n",
    "    sample_feat_tensor = torch.FloatTensor(sample_feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # generate caption\n",
    "    generated_caption = generate_caption_for_video(sample_feat_tensor, enc, dec, vocab, DEVICE)\n",
    "    cleaned_caption = clean_caption(generated_caption)\n",
    "\n",
    "    # ground truth (list of captions)\n",
    "    references = test_items[vid]\n",
    "\n",
    "    print(f\"\\n Video {i+1}: {vid}\")\n",
    "    print(f\" Generated Caption: {cleaned_caption}\")\n",
    "    print(\" Reference Captions:\")\n",
    "    for j, ref in enumerate(references[:3]):\n",
    "        print(f\"  Ref {j+1}: {ref}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8719373,
     "sourceId": 13706572,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8749726,
     "sourceId": 13750798,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
